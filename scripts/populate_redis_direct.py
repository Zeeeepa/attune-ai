#!/usr/bin/env python3
"""Directly populate Redis with test data for dashboard testing.

This script bypasses the telemetry API and writes directly to Redis,
useful for testing the dashboard without initializing the full framework.
"""
import json
import random
import time
import uuid
from datetime import datetime, timedelta

import redis


def generate_test_data():
    """Generate test data directly in Redis."""
    # Connect to Redis
    r = redis.Redis(host='localhost', port=6379, decode_responses=False)

    print("=" * 60)
    print("POPULATING REDIS DIRECTLY (NO API)")
    print("=" * 60)
    print()

    # Agent Types: Workflow-based + Role-based
    WORKFLOW_AGENTS = [
        ("code-review", "Analyzing code quality"),
        ("test-generation", "Generating unit tests"),
        ("security-audit", "Scanning for vulnerabilities"),
        ("refactoring", "Refactoring legacy code"),
        ("bug-predict", "Predicting potential bugs"),
    ]

    ROLE_AGENTS = [
        ("orchestrator", "Coordinating workflow pipeline"),
        ("validator", "Validating outputs"),
        ("monitor", "Monitoring system health"),
    ]

    # Combine all agents for heartbeats
    ALL_AGENTS = WORKFLOW_AGENTS + ROLE_AGENTS

    # Pattern 1: Agent Heartbeats
    print("ðŸ“Š Pattern 1: Creating agent heartbeats...")
    agent_ids = []  # Track for use in signals

    for agent_name, default_task in ALL_AGENTS:
        # Create unique agent ID with run suffix
        agent_id = f"{agent_name}-{uuid.uuid4().hex[:6]}"
        agent_ids.append(agent_id)

        heartbeat_data = {
            "agent_id": agent_id,
            "status": random.choice(["running", "idle", "running", "running"]),
            "progress": random.random(),
            "current_task": default_task,
            "timestamp": datetime.utcnow().isoformat(),
            "metadata": {
                "demo": True,
                "type": "workflow" if agent_name in [a[0] for a in WORKFLOW_AGENTS] else "role"
            }
        }

        key = f"heartbeat:{agent_id}".encode()
        value = json.dumps(heartbeat_data).encode()
        r.setex(key, 60, value)  # 60 second TTL
        print(f"  âœ“ {agent_id}: {heartbeat_data['status']} ({heartbeat_data['progress']*100:.0f}%)")

    print()

    # Pattern 2: Coordination Signals
    print("ðŸ“¡ Pattern 2: Creating coordination signals...")
    signal_types_with_context = [
        ("task_complete", "Task completed successfully"),
        ("request_help", "Requesting assistance"),
        ("ready", "Ready for next stage"),
        ("status_update", "Status update"),
        ("error", "Error encountered"),
        ("checkpoint", "Checkpoint reached"),
    ]

    for i in range(10):
        source = random.choice(agent_ids)
        target = random.choice(agent_ids)
        signal_type, message = random.choice(signal_types_with_context)

        signal_data = {
            "signal_id": f"signal_{uuid.uuid4().hex[:8]}",
            "signal_type": signal_type,
            "source_agent": source,
            "target_agent": target,
            "timestamp": datetime.utcnow().isoformat(),
            "ttl_seconds": 300,
            "payload": {"message": message, "demo": True}
        }

        # Key format matches CoordinationSignals API: empathy:signal:{target}:{type}:{id}
        key = f"empathy:signal:{target}:{signal_type}:{signal_data['signal_id']}".encode()
        value = json.dumps(signal_data).encode()
        r.setex(key, 300, value)  # 5 minute TTL

        # Show short names for readability
        source_short = source.split('-')[0]
        target_short = target.split('-')[0]
        print(f"  âœ“ Signal: {source_short} â†’ {target_short} ({signal_type})")
        time.sleep(0.01)  # Small delay for unique timestamps

    print()

    # Pattern 4: Event Streaming
    print("ðŸ“¤ Pattern 4: Creating events...")
    for i in range(15):
        event_data = {
            "event_id": f"event-{int(time.time() * 1000)}-{i}",
            "event_type": random.choice(["workflow_progress", "agent_heartbeat", "coordination_signal"]),
            "timestamp": datetime.utcnow().isoformat(),
            "data": {
                "workflow": random.choice(["code-review", "test-generation", "refactoring"]),
                "stage": random.choice(["analysis", "generation", "validation"]),
                "progress": random.random(),
                "demo": True
            },
            "source": f"agent-{random.randint(1, 5)}"
        }

        # Add to stream
        stream_key = f"stream:{event_data['event_type']}".encode()
        r.xadd(stream_key, {b"data": json.dumps(event_data).encode()}, maxlen=1000)
        print(f"  âœ“ Event {i+1} added to stream")
        time.sleep(0.01)

    print()

    # Pattern 5: Approval Requests
    print("âœ‹ Pattern 5: Creating approval requests...")
    for i in range(2):
        request_id = f"approval-{int(time.time() * 1000)}-{i}"
        approval_data = {
            "request_id": request_id,
            "approval_type": random.choice(["deploy_to_staging", "delete_old_data", "refactor_module"]),
            "agent_id": "demo-workflow",
            "context": {"version": "1.0.0", "demo": True},
            "timestamp": datetime.utcnow().isoformat(),
            "timeout_seconds": 300.0
        }

        key = f"approval:pending:{request_id}".encode()
        value = json.dumps(approval_data).encode()
        r.setex(key, 300, value)  # 5 minute TTL
        print(f"  âœ“ Approval request {i+1}: {approval_data['approval_type']}")
        time.sleep(0.1)

    print()

    # Pattern 6: Quality Feedback
    print("ðŸ“Š Pattern 6: Creating quality feedback...")
    workflows = ["code-review", "test-generation", "refactoring"]
    stages = ["analysis", "generation", "validation"]
    tiers = ["cheap", "capable", "premium"]

    feedback_count = 0
    for workflow in workflows:
        for stage in stages:
            for tier in tiers:
                # Generate 10-15 samples per combination
                num_samples = random.randint(10, 15)

                for j in range(num_samples):
                    # Vary quality by tier
                    if tier == "cheap":
                        base_quality = 0.65
                    elif tier == "capable":
                        base_quality = 0.80
                    else:  # premium
                        base_quality = 0.90

                    # Add some randomness
                    quality = base_quality + (random.random() * 0.15 - 0.075)
                    quality = max(0.0, min(1.0, quality))  # Clamp to 0-1

                    feedback_id = f"fb-{int(time.time() * 1000)}-{feedback_count}"
                    feedback_data = {
                        "workflow_name": workflow,
                        "stage_name": stage,
                        "tier": tier,
                        "quality_score": quality,
                        "timestamp": datetime.utcnow().isoformat(),
                        "metadata": {"demo": True, "tokens": random.randint(50, 300)}
                    }

                    key = f"feedback:{workflow}:{stage}:{tier}:{feedback_id}".encode()
                    value = json.dumps(feedback_data).encode()
                    r.setex(key, 604800, value)  # 7 day TTL
                    feedback_count += 1

                print(f"  âœ“ {workflow}/{stage}/{tier}: {num_samples} samples")

    print()
    print("=" * 60)
    print("âœ… REDIS POPULATED SUCCESSFULLY")
    print("=" * 60)
    print()
    print(f"Total keys in Redis: {r.dbsize()}")
    print()
    print("ðŸš€ Now you can start the dashboard:")
    print("   python examples/dashboard_demo.py")
    print()
    print("   Or directly:")
    print("   python -m src.attune.dashboard.simple_server")


if __name__ == "__main__":
    generate_test_data()
