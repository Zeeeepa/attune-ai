---
description: Quick Start: Anthropic Optimizations: **üéâ Implementation Complete!** All three optimization tracks are now active.
---

# Quick Start: Anthropic Optimizations

**üéâ Implementation Complete!** All three optimization tracks are now active.

---

## ‚úÖ What's New

| Feature | Cost Savings | Status |
|---------|--------------|--------|
| **Batch API** | 50% on batch tasks | ‚úÖ Ready |
| **Prompt Caching** | 20-30% on repeated context | ‚úÖ Enabled by default |
| **Precise Token Counting** | <1% tracking error | ‚úÖ Available |

**Expected Overall Savings: 30-50%**

---

## üöÄ Quick Start Guide

### 1. Verify Installation

```bash
python scripts/verify_anthropic_optimizations.py
```

Expected output: `3/3 tests passed` ‚úÖ

---

### 2. Use Precise Token Counting

```python
from empathy_llm_toolkit.utils.tokens import count_tokens, estimate_cost

# Count tokens accurately
text = "Your prompt here"
tokens = count_tokens(text, model="claude-sonnet-4-5")
print(f"Tokens: {tokens}")

# Estimate cost before API call
cost = estimate_cost(input_tokens=1000, output_tokens=500, model="claude-sonnet-4-5")
print(f"Estimated cost: ${cost:.4f}")
```

**Benefits:**
- Billing-accurate token counts (uses Anthropic SDK)
- Pre-request cost validation
- Budget management

---

### 3. Monitor Cache Performance

```python
from empathy_os.telemetry.usage_tracker import UsageTracker

# Get cache statistics
tracker = UsageTracker.get_instance()
stats = tracker.get_cache_stats(days=7)

print(f"Cache Hit Rate: {stats['hit_rate']:.1%}")
print(f"Savings: ${stats['savings']:.2f}")
print(f"Reads: {stats['total_reads']:,} tokens")
```

**Cache is ALREADY ENABLED by default!** You're saving 20-30% on repeated context automatically.

**Tips to maximize cache hits:**
- Group similar requests together (5-min cache window)
- Structure prompts with static content first
- Reuse system prompts across requests

---

### 4. Use Batch API for Non-Urgent Tasks (50% Savings!)

```python
import asyncio
from src.empathy_os.workflows.batch_processing import (
    BatchProcessingWorkflow,
    BatchRequest
)

async def run_batch():
    # Create batch requests
    requests = [
        BatchRequest(
            task_id="log_1",
            task_type="analyze_logs",
            input_data={"logs": "ERROR: Connection failed..."},
            model_tier="capable"
        ),
        BatchRequest(
            task_id="report_1",
            task_type="generate_report",
            input_data={"data": "..."},
            model_tier="capable"
        )
    ]

    # Submit and wait for results
    workflow = BatchProcessingWorkflow()
    results = await workflow.execute_batch(
        requests,
        poll_interval=300,  # Check every 5 minutes
        timeout=86400       # 24-hour max
    )

    # Check results
    for result in results:
        if result.success:
            print(f"‚úì {result.task_id}: Success")
        else:
            print(f"‚úó {result.task_id}: {result.error}")

# Run
asyncio.run(run_batch())
```

**Batch-Eligible Tasks:**
- Log analysis (`analyze_logs`)
- Bulk classification (`classify_bulk`)
- Report generation (`generate_report`)
- Documentation generation (`generate_docs`)
- Test generation (`generate_tests`)
- And 17 more... (see [tasks.py](src/empathy_os/models/tasks.py))

**NOT for Batch:**
- Interactive chat
- Real-time debugging
- User queries
- Critical fixes

---

## üìä Real-World Examples

### Example 1: Analyze 100 Log Files (Batch)

**Before:** $20.00 (real-time API)
**After:** $10.00 (batch API)
**Savings:** 50% = $10.00

```python
# Create batch requests for 100 log files
import asyncio
from src.empathy_os.workflows.batch_processing import BatchProcessingWorkflow, BatchRequest

requests = [
    BatchRequest(
        task_id=f"log_{i}",
        task_type="analyze_logs",
        input_data={"logs": open(f"logs/app_{i}.log").read()},
        model_tier="capable"
    )
    for i in range(100)
]

workflow = BatchProcessingWorkflow()
results = await workflow.execute_batch(requests)
print(f"{sum(r.success for r in results)}/100 logs analyzed")
```

### Example 2: Code Review with Caching

**Before:** $15.00 (no caching)
**After:** $10.50 (with caching)
**Savings:** 30% = $4.50

```python
# Same system prompt reused ‚Üí cache hit!
from empathy_llm_toolkit.providers import AnthropicProvider

provider = AnthropicProvider(use_prompt_caching=True)  # Already default

system_prompt = """You are a code reviewer. Check for:
- Security vulnerabilities
- Performance issues
- Best practices
"""  # This gets cached!

for file in files:
    response = provider.complete(
        messages=[{"role": "user", "content": f"Review:\n{file}"}],
        system_prompt=system_prompt,  # Cache hit after first call
        model="claude-sonnet-4-5"
    )
```

---

## üéØ Expected Impact

### By Workload Type

| Workload | Before | After | Savings |
|----------|--------|-------|---------|
| **Batch log analysis** (100 files) | $20.00 | $10.00 | 50% ($10) |
| **Code review** (10 files, cached prompts) | $15.00 | $10.50 | 30% ($4.50) |
| **Documentation generation** (batch) | $30.00 | $15.00 | 50% ($15) |
| **Interactive debugging** (no optimization) | $10.00 | $10.00 | 0% (N/A) |

**Total Monthly Savings** (typical use): **$200-500/month ‚Üí $130-325/month** = **30-35% reduction**

---

## üìà Monitoring & Optimization

### View Cache Performance

```python
from empathy_os.telemetry.usage_tracker import UsageTracker

stats = UsageTracker.get_instance().get_cache_stats(days=7)
print(f"Hit Rate: {stats['hit_rate']:.1%}")

# If hit rate < 30%, optimize by:
# 1. Grouping similar requests
# 2. Reusing system prompts
# 3. Structuring prompts better
```

### Check Token Usage

```python
from empathy_llm_toolkit.utils.tokens import count_message_tokens

# Before making API call
messages = [{"role": "user", "content": "Your message"}]
counts = count_message_tokens(messages, system_prompt="...")

if counts['total'] > 50000:
    print("‚ö†Ô∏è  Large request - consider splitting")
```

---

## üîß Configuration

### Disable Caching (if needed)

```python
from empathy_llm_toolkit.providers import AnthropicProvider

provider = AnthropicProvider(use_prompt_caching=False)  # Disable
```

### Batch API Settings

```python
from src.empathy_os.workflows.batch_processing import BatchProcessingWorkflow

workflow = BatchProcessingWorkflow()

results = await workflow.execute_batch(
    requests,
    poll_interval=600,   # Check every 10 minutes (default: 5 min)
    timeout=43200        # 12-hour timeout (default: 24 hours)
)
```

---

## üìö Documentation

- **Full Plan:** [docs/ANTHROPIC_OPTIMIZATION_PLAN.md](docs/ANTHROPIC_OPTIMIZATION_PLAN.md) (68 pages)
- **Summary:** [ANTHROPIC_OPTIMIZATION_SUMMARY.md](ANTHROPIC_OPTIMIZATION_SUMMARY.md)
- **GitHub Issues:** [#22](https://github.com/Smart-AI-Memory/empathy-framework/issues/22), [#23](https://github.com/Smart-AI-Memory/empathy-framework/issues/23), [#24](https://github.com/Smart-AI-Memory/empathy-framework/issues/24)

---

## ‚ùì FAQ

**Q: Is prompt caching automatic?**
A: Yes! It's enabled by default. You're already saving 20-30% on repeated context.

**Q: How do I know if my task can use Batch API?**
A: Check if it's in `BATCH_ELIGIBLE_TASKS` (see [tasks.py](src/empathy_os/models/tasks.py)) or if it can wait 24 hours.

**Q: Will this break existing code?**
A: No! All changes are backward compatible. Existing code works unchanged.

**Q: How much will I really save?**
A: Typical savings: 30-50% overall. Batch API alone saves 50% on eligible tasks.

**Q: Can I track savings?**
A: Yes! Use `tracker.get_cache_stats()` and `tracker.calculate_savings()` to see real savings.

---

## üéâ You're All Set!

Start using these optimizations today:
1. ‚úÖ Token counting is ready to use
2. ‚úÖ Caching is already enabled
3. ‚úÖ Batch API is ready for non-urgent tasks

**Expected savings: 30-50% on API costs!**

---

**Need Help?**
- Run: `python scripts/verify_anthropic_optimizations.py`
- Check: [ANTHROPIC_OPTIMIZATION_SUMMARY.md](ANTHROPIC_OPTIMIZATION_SUMMARY.md)
- Issues: [GitHub Issues](https://github.com/Smart-AI-Memory/empathy-framework/issues)
