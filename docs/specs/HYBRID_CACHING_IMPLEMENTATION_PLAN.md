# Hybrid Caching Implementation Plan - v3.8.0

**Version:** 3.8.0
**Created:** 2026-01-06
**Status:** ðŸ“‹ Planning Complete, Ready for Implementation

---

## Executive Summary

Implement hybrid response caching (hash-based + semantic similarity) to reduce API costs by 70% while maintaining offline capability and zero-config user experience.

**Key Metrics:**
- **Target cost reduction:** 70% on cached workflows
- **Cache hit rate goal:** 70-75% (combined hash + semantic)
- **Performance target:** <100ms cache lookup overhead
- **Storage:** In-memory + persistent disk cache
- **User experience:** Zero-config with one-time opt-in prompt

---

## Architecture Overview

### **Three-Tier Caching Strategy**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Workflow Execution                     â”‚
â”‚                  (code-review, etc.)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cache Manager (Smart Router)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Hash Cache (Fast Path) - 1-5Î¼s              â”‚    â”‚
â”‚  â”‚    â””â”€> Exact match â†’ Return cached response     â”‚    â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚ 2. Semantic Cache (Smart Path) - 10-100ms       â”‚    â”‚
â”‚  â”‚    â””â”€> Similarity > 95% â†’ Return similar responseâ”‚   â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚ 3. LLM Call (Miss) - 1-3s                       â”‚    â”‚
â”‚  â”‚    â””â”€> Call LLM â†’ Cache response â†’ Return       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Storage Layer (Memory + Disk)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ In-Memory Cache  â”‚  â”‚ Persistent Disk Cache      â”‚   â”‚
â”‚  â”‚ (LRU, fast)      â”‚  â”‚ (~/.empathy/cache/)        â”‚   â”‚
â”‚  â”‚ Max: 100MB       â”‚  â”‚ Max: 500MB, TTL: 24h       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Specifications

### **1. Base Cache Interface** (`src/empathy_os/cache/base.py`)

**Status:** âœ… Implemented

**Purpose:** Abstract interface for all cache implementations.

**Key Classes:**
- `BaseCache` - Abstract cache interface
- `CacheEntry` - Cached response with metadata
- `CacheStats` - Hit/miss/eviction tracking

**Methods:**
```python
class BaseCache(ABC):
    def get(workflow, stage, prompt, model) -> Optional[Any]
    def put(workflow, stage, prompt, model, response, ttl) -> None
    def clear() -> None
    def get_stats() -> CacheStats
```

---

### **2. Hash-Only Cache** (`src/empathy_os/cache/hash_only.py`)

**Status:** ðŸ”„ Next to implement

**Purpose:** Fast exact-match caching using SHA256 hashing.

**Performance:** 1-5Î¼s lookup time
**Hit Rate:** ~30% (exact matches only)
**Dependencies:** None (always available)

**Cache Key Format:**
```python
key = SHA256(workflow + "|" + stage + "|" + prompt + "|" + model)
```

**Implementation Details:**
- In-memory dict: `{key: CacheEntry}`
- Disk storage: JSON file at `~/.empathy/cache/hash_cache.json`
- LRU eviction when memory limit exceeded
- TTL expiration: 24 hours default

**Example:**
```python
# First call (cache miss)
result = await code_review.execute(diff="fix bug in auth.py")
# â†’ LLM call: 2.5s, $0.45

# Second call - exact same diff (cache hit)
result = await code_review.execute(diff="fix bug in auth.py")
# â†’ Cache hit: 0.001s, $0.00
```

---

### **3. Hybrid Cache** (`src/empathy_os/cache/hybrid.py`)

**Status:** ðŸ”„ To implement

**Purpose:** Hash + semantic similarity caching for maximum hit rate.

**Performance:**
- Hash lookup: 1-5Î¼s
- Semantic lookup: 50ms (CPU) / 10ms (GPU)
- Total overhead: <100ms

**Hit Rate:** ~70-75% (hash 30% + semantic 40-45%)

**Dependencies:**
- `sentence-transformers>=2.0.0`
- `torch>=2.0.0`
- `numpy>=1.24.0`

**Semantic Similarity:**
- Model: `all-MiniLM-L6-v2` (80MB)
- Threshold: 0.95 (configurable)
- Metric: Cosine similarity

**Workflow:**
```python
def get(workflow, stage, prompt, model):
    # Step 1: Try hash cache (fast path)
    hash_key = SHA256(workflow + stage + prompt + model)
    if hash_key in hash_cache:
        return hash_cache[hash_key]  # 1Î¼s

    # Step 2: Try semantic cache (smart path)
    embedding = model.encode(prompt)  # 50ms
    for cached_embedding, response in semantic_cache:
        similarity = cosine_similarity(embedding, cached_embedding)
        if similarity > 0.95:
            # Add to hash cache for future fast lookups
            hash_cache[hash_key] = response
            return response  # Semantic hit

    # Step 3: Cache miss - call LLM
    return None
```

**Example:**
```python
# Original prompt
result1 = await code_review.execute(diff="Added auth middleware to app.py")
# â†’ Cache miss: LLM call 2.5s

# Similar prompt (92% similarity)
result2 = await code_review.execute(diff="Added logging middleware to app.py")
# â†’ Semantic hit: 50ms from cache (instead of 2.5s LLM call)
```

---

### **4. Storage Layer** (`src/empathy_os/cache/storage.py`)

**Status:** ðŸ”„ To implement

**Purpose:** Hybrid in-memory + disk persistence with TTL.

**Architecture:**
```python
class CacheStorage:
    memory_cache: dict[str, CacheEntry]  # LRU, max 100MB
    disk_cache_path: Path  # ~/.empathy/cache/responses.db

    def load() -> None:
        """Load disk cache into memory on startup."""

    def save() -> None:
        """Persist memory cache to disk."""

    def evict_expired() -> int:
        """Remove expired entries based on TTL."""
```

**Storage Format (Disk):**
```json
{
  "version": "3.8.0",
  "entries": [
    {
      "key": "a1b2c3...",
      "workflow": "code-review",
      "stage": "scan",
      "model": "claude-3-5-sonnet-20241022",
      "prompt_hash": "d4e5f6...",
      "response": {...},
      "timestamp": 1704585600.0,
      "ttl": 86400
    }
  ]
}
```

**Memory Management:**
- Max in-memory: 100MB (configurable)
- Eviction policy: LRU (least recently used)
- Write-through: Save to disk on put()
- Lazy load: Load from disk on get() if not in memory

---

### **5. Dependency Manager** (`src/empathy_os/cache/dependency_manager.py`)

**Status:** ðŸ”„ To implement

**Purpose:** Auto-detect missing dependencies, prompt user to install.

**User Experience:**
```bash
# User installs base version
pip install empathy-framework

# First workflow run
empathy workflow run code-review

# Output:
âš¡ Smart Caching Available

  Empathy Framework can reduce your API costs by 70% with hybrid caching.
  This requires installing sentence-transformers (~150MB).

  Would you like to enable smart caching now? [Y/n]: y

  â†“ Installing cache dependencies...
  âœ“ sentence-transformers 2.3.1 installed
  âœ“ torch 2.1.2 installed
  âœ“ numpy 1.24.3 installed

  âœ“ Smart caching enabled! Future runs will save 70% on costs.
```

**Implementation:**
```python
class DependencyManager:
    def is_cache_installed() -> bool:
        """Check if sentence-transformers available."""

    def should_prompt_cache_install() -> bool:
        """Check if we should prompt (first run, not declined)."""

    def prompt_cache_install() -> bool:
        """Prompt user and install if accepted."""

    def install_cache_dependencies() -> bool:
        """Run pip install sentence-transformers torch numpy."""
```

**Configuration (``~/.empathy/config.yml`):**
```yaml
cache:
  enabled: true
  install_declined: false  # User said "no" to prompt
  prompt_shown: true       # Never prompt again
  model: all-MiniLM-L6-v2
  similarity_threshold: 0.95
  max_size_mb: 500
  ttl: 86400  # 24 hours
```

---

### **6. BaseWorkflow Integration** (`src/empathy_os/workflows/base.py`)

**Status:** ðŸ”„ To implement

**Purpose:** Transparent caching for all workflows.

**Modifications:**
```python
class BaseWorkflow:
    def __init__(self, **kwargs):
        # Auto-prompt for cache on first run
        self._maybe_setup_cache()

        # Create cache (hybrid if available, hash-only otherwise)
        self.cache = self._create_cache()

    def _maybe_setup_cache(self):
        """One-time prompt to install cache deps."""
        from empathy_os.cache import auto_setup_cache
        auto_setup_cache()

    def _create_cache(self):
        """Create appropriate cache based on installed deps."""
        from empathy_os.cache import create_cache
        return create_cache()

    async def _execute_stage(self, stage_name, prompt, model):
        """Execute stage with caching."""
        # Try cache first
        cached = self.cache.get(
            workflow=self.name,
            stage=stage_name,
            prompt=prompt,
            model=model
        )

        if cached is not None:
            self.cache.stats.hits += 1
            return cached

        # Cache miss - call LLM
        self.cache.stats.misses += 1
        response = await self._call_llm(prompt, model)

        # Store in cache
        self.cache.put(
            workflow=self.name,
            stage=stage_name,
            prompt=prompt,
            model=model,
            response=response
        )

        return response
```

---

### **7. Cost Reporting Enhancement**

**Status:** ðŸ”„ To implement

**Purpose:** Show cache savings in cost reports.

**Current Cost Report:**
```python
@dataclass
class CostReport:
    total_cost: float
    per_stage_cost: dict[str, float]
    model_breakdown: dict[str, float]
```

**Enhanced Cost Report:**
```python
@dataclass
class CostReport:
    total_cost: float  # Actual cost paid
    per_stage_cost: dict[str, float]
    model_breakdown: dict[str, float]

    # NEW: Cache metrics
    cache_hits: int
    cache_misses: int
    cache_hit_rate: float  # Percentage
    estimated_cost_without_cache: float  # What it would have cost
    savings_from_cache: float  # Money saved
    savings_percent: float  # Percentage saved
```

**Example Output:**
```bash
âœ“ Code review complete

Cost Report:
  Actual cost: $0.45
  Without cache: $1.50
  Savings: $1.05 (70%)

  Cache: 3 hits / 2 misses (60% hit rate)

  Per-stage:
    classify: $0.15 (cache hit)
    scan: $0.00 (cache hit)
    architect_review: $0.30 (cache miss)
```

---

## Implementation Plan

### **Phase 1: Core Infrastructure** (Days 1-2)

**Tasks:**
1. âœ… Create base cache interface (`base.py`)
2. â³ Implement HashOnlyCache (`hash_only.py`)
3. â³ Implement storage layer (`storage.py`)
4. â³ Write unit tests for hash cache

**Deliverable:** Hash-only caching working, ~30% cost savings

---

### **Phase 2: Hybrid Caching** (Days 3-4)

**Tasks:**
1. â³ Implement HybridCache with semantic similarity
2. â³ Handle optional dependency gracefully (fallback to hash-only)
3. â³ Write unit tests for semantic matching
4. â³ Benchmark performance (hash vs semantic vs LLM)

**Deliverable:** Hybrid caching working, ~70% cost savings

---

### **Phase 3: User Experience** (Day 5)

**Tasks:**
1. â³ Implement DependencyManager with one-time prompt
2. â³ Create `~/.empathy/config.yml` configuration system
3. â³ Integrate cache into BaseWorkflow
4. â³ Test with code-review workflow

**Deliverable:** Zero-config UX, automatic setup

---

### **Phase 4: Cost Reporting** (Day 6)

**Tasks:**
1. â³ Enhance CostReport with cache metrics
2. â³ Calculate estimated cost without cache
3. â³ Log cache stats (hits/misses/savings)
4. â³ Update workflow result formatting

**Deliverable:** Cost savings visible to users

---

### **Phase 5: Testing & Documentation** (Days 7-8)

**Tasks:**
1. â³ Comprehensive unit tests (target: 100% coverage)
2. â³ Integration tests with real workflows
3. â³ Create `docs/caching.md` documentation
4. â³ Update README with caching info
5. â³ Update `pyproject.toml` for v3.8.0

**Deliverable:** Production-ready, fully tested

---

## Testing Strategy

### **Unit Tests**

**Files to create:**
- `tests/unit/cache/test_base_cache.py`
- `tests/unit/cache/test_hash_cache.py`
- `tests/unit/cache/test_hybrid_cache.py`
- `tests/unit/cache/test_storage.py`
- `tests/unit/cache/test_dependency_manager.py`

**Test Coverage:**
- âœ… Cache hit/miss scenarios
- âœ… TTL expiration
- âœ… LRU eviction
- âœ… Disk persistence and reload
- âœ… Semantic similarity matching
- âœ… Fallback to hash-only when deps missing
- âœ… Cost report calculations

---

### **Integration Tests**

**Test Scenarios:**
1. Run code-review twice with same diff â†’ cache hit
2. Run code-review with similar diff â†’ semantic hit
3. Run without sentence-transformers â†’ hash-only fallback
4. Restart framework â†’ reload cache from disk
5. Cache exceeds size limit â†’ LRU eviction
6. Entry exceeds TTL â†’ expire and miss

---

## Configuration Schema

**File:** `~/.empathy/config.yml`

```yaml
# Cache configuration
cache:
  enabled: true                      # Enable/disable caching
  type: hybrid                       # hybrid | hash-only | disabled
  install_declined: false            # User declined install prompt
  prompt_shown: false                # Has prompt been shown?

  # Hybrid cache settings
  model: all-MiniLM-L6-v2           # Sentence transformer model
  similarity_threshold: 0.95         # Semantic match threshold (0.0-1.0)
  device: cpu                        # cpu | cuda

  # Storage settings
  max_size_mb: 500                   # Max disk cache size
  max_memory_mb: 100                 # Max in-memory cache
  ttl: 86400                         # Default TTL (24 hours)
  cache_dir: ~/.empathy/cache        # Cache storage path

  # Logging
  log_hits: true                     # Log cache hits to debug
  log_misses: true                   # Log cache misses to debug
```

---

## pyproject.toml Updates

**Version bump:** 3.7.1 â†’ 3.8.0

**New optional dependencies:**
```toml
[project.optional-dependencies]
cache = [
    "sentence-transformers>=2.0.0",
    "torch>=2.0.0",
    "numpy>=1.24.0",
]

full = [
    "empathy-framework[cache,developer,healthcare]",
]
```

---

## Performance Targets

| Operation | Target | Acceptable |
|-----------|--------|------------|
| Hash cache lookup | <5Î¼s | <10Î¼s |
| Semantic cache lookup | <100ms (CPU) | <200ms |
| Disk load on startup | <500ms | <1s |
| Memory overhead | <100MB | <200MB |
| Disk usage | <500MB | <1GB |

---

## Cost Savings Projections

**Workflow:** code-review (5 stages)

| Scenario | LLM Calls | Cache Hits | Cost | Savings |
|----------|-----------|------------|------|---------|
| **No cache** | 5 | 0 | $1.50 | 0% |
| **Hash-only** | 3.5 | 1.5 (30%) | $1.05 | 30% |
| **Hybrid** | 1.5 | 3.5 (70%) | $0.45 | 70% |

**Monthly savings (at $1,000/month spend):**
- Hash-only: $300/month saved
- Hybrid: $700/month saved

---

## Risk Mitigation

### **Risk 1: Semantic model download fails**
**Mitigation:** Fallback to hash-only cache automatically, log warning.

### **Risk 2: Cache causes incorrect responses**
**Mitigation:**
- Default threshold: 0.95 (95% similarity)
- Include model in cache key (different models don't share cache)
- Users can disable caching entirely

### **Risk 3: Cache fills disk**
**Mitigation:**
- Max size limit: 500MB (configurable)
- TTL expiration: 24 hours
- LRU eviction policy

### **Risk 4: Breaking change for existing users**
**Mitigation:**
- Backwards compatible (cache is opt-in)
- Base install still works without cache deps
- No API changes to workflows

---

## Success Metrics

**v3.8.0 will be considered successful if:**

- âœ… 70% cache hit rate on code-review workflow
- âœ… <100ms cache lookup overhead
- âœ… Zero breaking changes (all existing tests pass)
- âœ… User can install with `pip install empathy-framework[cache]`
- âœ… User can add cache later with `empathy install cache`
- âœ… Cost report shows savings accurately
- âœ… Works offline after initial model download
- âœ… 100% test coverage on cache module

---

## Next Steps

1. âœ… **Planning complete** - This document
2. â³ **Implement Phase 1** - Hash-only cache + storage
3. â³ **Implement Phase 2** - Hybrid semantic caching
4. â³ **Implement Phase 3** - DependencyManager + UX
5. â³ **Implement Phase 4** - Cost reporting
6. â³ **Implement Phase 5** - Tests + docs
7. â³ **Release v3.8.0** - Ship to PyPI

---

**Status:** ðŸ“‹ Ready to implement
**Estimated completion:** 8 days
**Target release date:** v3.8.0 mid-January 2026
