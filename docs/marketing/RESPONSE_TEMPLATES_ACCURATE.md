# Response Templates (Verified Claims Only)

**Version**: v3.9.3
**Last Updated**: January 9, 2026
**Principle**: Only claim what's verifiable

---

## General Responses

### Q: "This is just prompting"

```
Project memory is different from prompting:

**Prompting** (every session):
- You repeat: "Validate file paths"
- AI generates code
- Next session: repeat again

**Project Memory** (once):
- Standards in .claude/CLAUDE.md
- Loaded once at session start
- Available on-demand (doesn't fill context)
- Persists across all sessions
- Real code examples from your codebase

Think: .editorconfig for coding standards
```

---

### Q: "Does this work with Copilot/Cursor?"

```
We've verified this with Claude Code.

The pattern SHOULD work with other tools that support project context:
- GitHub Copilot: .github/copilot-instructions.md (documented feature)
- Cursor: .cursorrules (documented feature)

Haven't tested with those tools yet - if you try it, let us know how it works.

The key isn't the tool - it's the approach:

‚ùå Abstract rules: "Always validate input"
‚úÖ Real examples: "Use _validate_file_path() from config.py:29-68"

‚ùå Generic advice: "Don't use eval()"
‚úÖ Specific patterns: "Use ast.literal_eval() - here's why, how, and test pattern"
```

---

### Q: "Can you share your standards file?"

```
Absolutely!

Coding Standards Reference (1,170 lines):
https://github.com/Smart-AI-Memory/empathy-framework/blob/main/.claude/rules/empathy/coding-standards-index.md

What's inside:
‚úÖ Vulnerable code examples (with CVE references)
‚úÖ Secure code examples (from our codebase)
‚úÖ Security test patterns
‚úÖ Common false positives
‚úÖ Attack scenarios and why each matters

Fork and adapt for your team. Works with any language.
```

---

## Technical Questions

### Q: "What are your actual metrics?"

```
Verifiable technical metrics:

**Security Testing:**
- v3.0: 14 security tests
- v3.9.0: 174 security tests
- Increase: +1,143%

**Security Status:**
- Current vulnerabilities: 0
- Modules secured: 6 (CWE-22 path traversal)
- Tests cover: path traversal, null bytes, system directories

**Code Quality (v3.9.3):**
- Health score: 100/100
- Type errors in production code: 0
- Lint errors: 0

All verifiable in: github.com/Smart-AI-Memory/empathy-framework
```

---

### Q: "How much time did this save?"

```
I can't give you a number - it's too context-dependent.

What's verifiable:
- 174 security tests protecting 6 modules
- 0 current vulnerabilities
- 1,170 lines of documented standards
- 100/100 health score

Whether this saves YOU time depends on:
- Your codebase size
- Team size
- How often you repeat standards
- What bugs you prevent

Your mileage will vary. Try it and measure for yourself.
```

---

### Q: "174 tests seems excessive"

```
It's actually the minimum for comprehensive coverage:

**Per file operation, you need 4 test types:**
1. Path traversal tests (../../etc/passwd)
2. Null byte injection tests (file\x00.ext)
3. System directory tests (/etc, /sys, /proc, /dev)
4. Valid path tests (positive cases)

**Our math:**
6 modules √ó 2-3 operations √ó 4 test types = 48-72 core tests

Add edge cases + integration tests = 174 total

**Result:** 0 vulnerabilities found in production
```

---

### Q: "What about hallucinations?"

```
Our standards reference actual tested code, not hypothetical examples:

When Claude uses `_validate_file_path()`, it's using:
- Actual function: src/empathy_os/config.py:29-68
- Covered by 174 security tests
- In production since v3.0.0
- Validated against CWE-22

It's not generating new security code - it's using proven patterns.

You can verify every claim in our repo.
```

---

## Implementation Questions

### Q: "How do I implement this for my project?"

```
Step-by-step:

**1. Identify your top 5 violations**
Look at your last 10 code reviews. What do you keep commenting on?

**2. Document with real code**
For each:
- ‚ùå Show bad example (actual vulnerable code)
- ‚úÖ Show good example (your actual solution)
- üìö Explain why (what attack/bug it prevents)
- üß™ Show test pattern (how to verify)

**3. Reference your codebase**
Don't say "validate input"
Say "use validate_input() from utils.py:42-68"

**4. Add to project context**
- Claude Code: .claude/CLAUDE.md (verified to work)
- GitHub Copilot: .github/copilot-instructions.md (should work, not tested)
- Cursor: .cursorrules (should work, not tested)

**Complete guide** (11,000 words):
https://github.com/Smart-AI-Memory/empathy-framework/blob/main/docs/guides/teaching-ai-your-standards.md
```

---

### Q: "Does this work for [language/framework]?"

```
Yes! The pattern is language-agnostic.

**Our examples:**
- Python (primary)
- TypeScript (examples in guide)
- Go (examples in guide)

**The pattern:**
1. Document your standards with real code
2. Reference actual functions in your codebase
3. Show test patterns
4. Put in AI tool's project context

Works for any language. The key: real examples, not abstract rules.
```

---

### Q: "How long does this take to set up?"

```
Initial setup:
- Identify top 5 violations: 30 minutes
- Document with examples: 2-3 hours
- Add to project context: 5 minutes
- Total: 2-4 hours

Maintenance:
- Add new patterns as you discover them
- Update when standards change
- ~15 minutes per new pattern

Our 1,170-line reference was built over several months while implementing security hardening. You don't need to start that big - start with 5 patterns.
```

---

## Skeptical Questions

### Q: "This seems overhyped"

```
Fair concern. Here's what's verifiable:

‚úÖ 174 security tests (in codebase)
‚úÖ 0 current vulnerabilities (run security scan)
‚úÖ 1,170 lines of standards (in repo)
‚úÖ 100/100 health score (verifiable)

What we DON'T claim:
‚ùå "Saves X hours" (too context-dependent)
‚ùå "Guarantees no bugs" (nothing does)
‚ùå "Works for everyone" (depends on use case)

We built something. We're sharing how. You decide if it's useful.
```

---

### Q: "Security through obscurity?"

```
No - security through defense in depth:

**Layer 1:** Input validation (_validate_file_path)
- Resolves paths
- Blocks null bytes
- Blocks system directories
- Blocks path traversal

**Layer 2:** 174 security tests
- Test each attack vector
- Verify defenses work
- Catch regressions

**Layer 3:** Code review + security scans
- Human review
- Automated scanning
- Continuous monitoring

**Layer 4:** Claude generates secure code
- Follows patterns automatically
- Uses validated functions
- Includes tests

Standards document is open source. No obscurity.
```

---

### Q: "This is just documentation"

```
It's executable documentation:

**Traditional docs:**
- "Always validate file paths"
- Generic advice
- No code examples
- Not machine-readable

**Project memory approach:**
- "Use _validate_file_path() from config.py:29-68"
- Actual function reference
- Real code examples
- Machine-readable by AI
- Shows test patterns
- Explains attack vectors

AI can reference it during code generation. That's the difference.
```

---

## Business Questions

### Q: "What's the ROI?"

```
I can't calculate ROI for you - too many variables:
- Your team size
- Cost of bugs in your domain
- Current code review time
- Value of prevented security issues

What's measurable:
- Test coverage increase: +1,143%
- Vulnerabilities: 0
- Health score: 100/100

Calculate YOUR ROI:
1. Time saved on repetitive standards comments
2. Cost of prevented security issues
3. Faster onboarding (standards in context)
4. Reduced rework from caught bugs

Your context determines ROI.
```

---

### Q: "Is this enterprise-ready?"

```
The pattern is proven. Empathy Framework implementation:
- 174 security tests
- 0 vulnerabilities
- 100/100 health score
- Used in production

For enterprise:
- Adapt our standards for your domain
- Add your security requirements
- Reference your existing code
- Integrate with your CI/CD

The guides show how. Your security team reviews and adapts.

Framework is open source (MIT). Use freely.
```

---

## Community Questions

### Q: "How can I contribute?"

```
Ways to contribute:

**1. Try the pattern**
- Implement for your project
- Share your results
- Report what works/doesn't

**2. Share your standards**
- Open source your coding standards
- Help others learn from your patterns

**3. Contribute code**
- GitHub: github.com/Smart-AI-Memory/empathy-framework
- Issues welcome
- PRs welcome

**4. Write guides**
- Document your implementation
- Share language-specific patterns
- Help others adapt

**5. Report issues**
- Found a bug? Open an issue
- Have feedback? Start a discussion
```

---

### Q: "What's next for the project?"

```
Current focus:
- Community feedback on this pattern
- More language examples (Python ‚úì, TS/Go in progress)
- Case studies from adopters
- Improved documentation based on questions

Not promising features. Listening to community first.

Join the discussion:
- GitHub Discussions: [link]
- Issues: [link]

Your feedback shapes the roadmap.
```

---

## Response to Criticism

### "You're overselling this"

```
You're right to be skeptical. Here's what's fact vs opinion:

**Facts:**
- 174 tests (count them)
- 0 vulnerabilities (run scan)
- 1,170 lines of standards (read them)

**Opinion:**
- Whether this is useful for you
- Whether it saves time
- Whether the approach is sound

I'm sharing what we built. You evaluate if it's valuable.

Fair?
```

---

### "This won't work in practice"

```
Valid concern. Here's what we can verify:

**What works (for us):**
- Claude generates code using our patterns
- Security tests all pass
- Health score 100/100
- 0 vulnerabilities found

**What we DON'T know:**
- If it works for your codebase
- If it works for your team
- If it works for your language
- If it saves YOU time

Only one way to find out: try it.

If it doesn't work for you, document why. That helps everyone.
```

---

## Quick Stats Reference

Copy-paste when asked for quick facts:

```
Empathy Framework v3.9.3:

üîí 174 security tests (+1,143% from v3.0)
üõ°Ô∏è 0 current vulnerabilities
‚úÖ 100/100 health score
üìö 1,170-line coding standards
üéØ 6 modules secured (CWE-22)
üî¨ Zero type errors in production

Verify: github.com/Smart-AI-Memory/empathy-framework
```

---

## What We Never Say

‚ùå "Saves 80 hours/month" - Can't verify for users
‚ùå "Tracked over 30 days" - No controlled study
‚ùå "-62% violations" - Not precisely measured
‚ùå "Guaranteed results" - Depends on context
‚ùå "Proven ROI" - Too context-dependent
‚ùå "Best practice" - Let community decide

‚úÖ "Here's what we built"
‚úÖ "Here's what's verifiable"
‚úÖ "Here's how you can adapt it"
‚úÖ "Try it and measure yourself"

---

## Core Principle

**When in doubt**: Share what's verifiable, let users draw conclusions.

Don't claim time savings. Don't claim ROI. Don't claim adoption metrics.

Share:
- What we built (code, tests, docs)
- What's verifiable (metrics, health score)
- How to implement (guides, examples)

Users decide value. We provide facts.

---

**Last Updated**: January 9, 2026
**Status**: All claims verifiable
**Approach**: Honest, technical, humble
