"""Behavioral tests for reports.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest

from empathy_os.workflows.progressive.core import (
    ProgressiveWorkflowResult,
    Tier,
    TierResult,
)
from empathy_os.workflows.progressive.reports import (
    _format_duration,
    generate_progression_report,
    save_progression_results,
)


# Fixtures


@pytest.fixture
def tier_result_cheap():
    """Given a cheap tier result with successful items."""
    return TierResult(
        tier=Tier.CHEAP,
        model="gpt-3.5-turbo",
        generated_items=[
            {"id": 1, "content": "test1", "validation": {"passed": True}},
            {"id": 2, "content": "test2", "validation": {"passed": True}},
        ],
        attempt=1,
        success_count=2,
        success_rate=1.0,
        quality_score=0.85,
        cost=0.05,
        duration=1.5,
        escalated=False,
        escalation_reason=None,
    )


@pytest.fixture
def tier_result_capable():
    """Given a capable tier result with mixed success."""
    return TierResult(
        tier=Tier.CAPABLE,
        model="gpt-4",
        generated_items=[
            {"id": 3, "content": "test3", "validation": {"passed": True}},
            {"id": 4, "content": "test4", "validation": {"passed": False}},
        ],
        attempt=1,
        success_count=1,
        success_rate=0.5,
        quality_score=0.65,
        cost=0.25,
        duration=3.2,
        escalated=True,
        escalation_reason="Quality below threshold",
    )


@pytest.fixture
def tier_result_premium():
    """Given a premium tier result with all successful items."""
    return TierResult(
        tier=Tier.PREMIUM,
        model="gpt-4-turbo",
        generated_items=[
            {"id": 5, "content": "test5", "validation": {"passed": True}},
            {"id": 6, "content": "test6", "validation": {"passed": True}},
            {"id": 7, "content": "test7", "validation": {"passed": True}},
        ],
        attempt=1,
        success_count=3,
        success_rate=1.0,
        quality_score=0.95,
        cost=0.75,
        duration=5.0,
        escalated=False,
        escalation_reason=None,
    )


@pytest.fixture
def progressive_workflow_result_success(
    tier_result_cheap, tier_result_capable, tier_result_premium
):
    """Given a successful progressive workflow result."""
    return ProgressiveWorkflowResult(
        workflow_name="test_workflow",
        task_id="task-123",
        tier_results=[tier_result_cheap, tier_result_capable, tier_result_premium],
        final_tier=Tier.PREMIUM,
        total_cost=1.05,
        total_duration=9.7,
        cost_savings=2.15,
        cost_savings_percent=67.2,
        metadata={"user": "test_user", "priority": "high"},
    )


@pytest.fixture
def progressive_workflow_result_minimal():
    """Given a minimal workflow result with only cheap tier."""
    tier_result = TierResult(
        tier=Tier.CHEAP,
        model="gpt-3.5-turbo",
        generated_items=[{"id": 1, "content": "test", "validation": {"passed": True}}],
        attempt=1,
        success_count=1,
        success_rate=1.0,
        quality_score=0.80,
        cost=0.02,
        duration=0.5,
        escalated=False,
        escalation_reason=None,
    )
    return ProgressiveWorkflowResult(
        workflow_name="minimal_workflow",
        task_id="task-456",
        tier_results=[tier_result],
        final_tier=Tier.CHEAP,
        total_cost=0.02,
        total_duration=0.5,
        cost_savings=0.0,
        cost_savings_percent=0.0,
        metadata={},
    )


@pytest.fixture
def progressive_workflow_result_no_savings():
    """Given a workflow result with no cost savings."""
    tier_result = TierResult(
        tier=Tier.PREMIUM,
        model="gpt-4-turbo",
        generated_items=[{"id": 1, "content": "test", "validation": {"passed": True}}],
        attempt=1,
        success_count=1,
        success_rate=1.0,
        quality_score=0.95,
        cost=1.50,
        duration=2.0,
        escalated=False,
        escalation_reason=None,
    )
    return ProgressiveWorkflowResult(
        workflow_name="premium_only",
        task_id="task-789",
        tier_results=[tier_result],
        final_tier=Tier.PREMIUM,
        total_cost=1.50,
        total_duration=2.0,
        cost_savings=0.0,
        cost_savings_percent=0.0,
        metadata={},
    )


# Tests for _format_duration


class TestFormatDuration:
    """Tests for the _format_duration private function."""

    def test_format_duration_seconds_only(self):
        """Given duration less than a minute, when formatting, then returns seconds."""
        # When
        result = _format_duration(45.5)

        # Then
        assert result == "45.5s"

    def test_format_duration_minutes_and_seconds(self):
        """Given duration with minutes, when formatting, then returns minutes and seconds."""
        # When
        result = _format_duration(125.3)

        # Then
        assert result == "2m 5s"

    def test_format_duration_exact_minute(self):
        """Given duration exactly 60 seconds, when formatting, then returns 1 minute."""
        # When
        result = _format_duration(60.0)

        # Then
        assert result == "1m 0s"

    def test_format_duration_zero(self):
        """Given zero duration, when formatting, then returns 0.0s."""
        # When
        result = _format_duration(0.0)

        # Then
        assert result == "0.0s"

    def test_format_duration_very_large(self):
        """Given very large duration, when formatting, then formats correctly."""
        # When
        result = _format_duration(3661.5)

        # Then
        assert result == "61m 1s"


# Tests for generate_progression_report


class TestGenerateProgressionReport:
    """Tests for generate_progression_report function."""

    def test_generate_report_success_with_all_tiers(
        self, progressive_workflow_result_success
    ):
        """Given complete workflow result, when generating report, then includes all sections."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        assert "PROGRESSIVE ESCALATION REPORT" in report
        assert "test_workflow" in report
        assert "task-123" in report
        assert "$1.05" in report
        assert "Cost Savings: $2.15" in report
        assert "67% vs all-Premium" in report
        assert "ðŸ’° CHEAP Tier" in report
        assert "ðŸ“Š CAPABLE Tier" in report
        assert "ðŸ’Ž PREMIUM Tier" in report
        assert "TIER BREAKDOWN" in report
        assert "FINAL RESULTS" in report

    def test_generate_report_includes_tier_details(
        self, progressive_workflow_result_success
    ):
        """Given workflow result, when generating report, then includes tier-specific details."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        assert "gpt-3.5-turbo" in report
        assert "gpt-4" in report
        assert "gpt-4-turbo" in report
        assert "Items: 2" in report
        assert "Items: 3" in report
        assert "CQS=0.9" in report or "CQS=0.8" in report
        assert "Escalated: Quality below threshold" in report

    def test_generate_report_minimal_workflow(self, progressive_workflow_result_minimal):
        """Given minimal workflow, when generating report, then shows single tier."""
        # When
        report = generate_progression_report(progressive_workflow_result_minimal)

        # Then
        assert "minimal_workflow" in report
        assert "task-456" in report
        assert "ðŸ’° CHEAP Tier" in report
        assert "ðŸ“Š CAPABLE Tier" not in report
        assert "ðŸ’Ž PREMIUM Tier" not in report
        assert "Items: 1" in report
        assert "$0.02" in report

    def test_generate_report_no_cost_savings(
        self, progressive_workflow_result_no_savings
    ):
        """Given workflow with no savings, when generating report, then omits savings section."""
        # When
        report = generate_progression_report(progressive_workflow_result_no_savings)

        # Then
        assert "Cost Savings:" not in report
        assert "vs all-Premium" not in report
        assert "premium_only" in report
        assert "$1.50" in report

    def test_generate_report_success_rates(self, progressive_workflow_result_success):
        """Given workflow result, when generating report, then includes success rates."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        assert "Success: 2/2 (100%)" in report
        assert "Success: 1/2 (50%)" in report
        assert "Success: 3/3 (100%)" in report

    def test_generate_report_duration_formatting(
        self, progressive_workflow_result_success
    ):
        """Given workflow result, when generating report, then formats durations properly."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        # Should contain formatted durations
        assert "Duration:" in report
        # Check for presence of time units
        assert "s" in report or "m" in report

    def test_generate_report_escalation_info(self, tier_result_capable):
        """Given escalated tier, when generating report, then shows escalation reason."""
        # Given
        result = ProgressiveWorkflowResult(
            workflow_name="escalation_test",
            task_id="task-esc",
            tier_results=[tier_result_capable],
            final_tier=Tier.CAPABLE,
            total_cost=0.25,
            total_duration=3.2,
            cost_savings=0.0,
            cost_savings_percent=0.0,
            metadata={},
        )

        # When
        report = generate_progression_report(result)

        # Then
        assert "Escalated: Quality below threshold" in report

    def test_generate_report_no_escalation(self, tier_result_cheap):
        """Given non-escalated tier, when generating report, then no escalation message."""
        # Given
        result = ProgressiveWorkflowResult(
            workflow_name="no_escalation",
            task_id="task-noesc",
            tier_results=[tier_result_cheap],
            final_tier=Tier.CHEAP,
            total_cost=0.05,
            total_duration=1.5,
            cost_savings=0.0,
            cost_savings_percent=0.0,
            metadata={},
        )

        # When
        report = generate_progression_report(result)

        # Then
        # Should not have "Escalated:" line for non-escalated tier
        lines = report.split("\n")
        tier_section = [
            line for line in lines if line.strip().startswith("â€¢ Escalated:")
        ]
        assert len(tier_section) == 0

    def test_generate_report_quality_scores(self, progressive_workflow_result_success):
        """Given workflow result, when generating report, then includes quality scores."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        assert "Quality: CQS=" in report
        assert "0.8" in report or "0.9" in report or "0.6" in report

    def test_generate_report_formatting_consistency(
        self, progressive_workflow_result_success
    ):
        """Given workflow result, when generating report, then uses consistent formatting."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        assert "â”" in report  # Contains separator lines
        assert "ðŸŽ¯" in report  # Contains header emoji
        lines = report.split("\n")
        assert len(lines) > 10  # Has substantial content

    def test_generate_report_metadata_not_exposed(
        self, progressive_workflow_result_success
    ):
        """Given workflow with metadata, when generating report, then metadata not in output."""
        # When
        report = generate_progression_report(progressive_workflow_result_success)

        # Then
        # Metadata should not be directly exposed in report
        assert "test_user" not in report
        assert "priority" not in report


# Tests for save_progression_results


class TestSaveProgressionResults:
    """Tests for save_progression_results function."""

    @patch("empathy_os.workflows.progressive.reports._validate_file_path")
    @patch("builtins.open", new_callable=mock_open)
    @patch("pathlib.Path.mkdir")
    def test_save_results_success(
        self, mock_mkdir, mock_file, mock_validate, progressive_workflow_result_success
    ):
        """Given valid result and path, when saving, then writes JSON file."""
        # Given
        output_path = Path("/tmp/results/progression.json")
        mock_validate.return_value = output_path

        # When
        result_path = save_progression_results(
            progressive_workflow_result_success, output_path
        )

        # Then
        assert result_path == output_path
        mock_validate.assert_called_once_with(output_path, must_exist=False)
        mock_mkdir.assert_called_once_with(parents=True, exist_ok=True)
        mock_file.assert_called_once_with(output_path, "w", encoding="utf-8")
        
        # Verify JSON was written
        handle = mock_file()
        written_content = "".join(call.args[0] for call in handle.write.call_args_list)
        assert written_content  # Something was written

    @patch("empathy_os.workflows.progressive.reports._validate_file_path")
    @patch("builtins.open", new_callable=mock_open)
    @patch("pathlib.Path.mkdir")
    def test_save_results_creates_parent_directories(
        self, mock_mkdir, mock_file, mock_validate, progressive_workflow_result_minimal
    ):
        """Given path with non-existent parents, when saving, then creates directories."""
        # Given
        output_path = Path("/tmp/deep/nested/path/results.json")
        mock_validate.return_value = output_path

        # When
        save_progression_results(progressive_workflow_result_minimal, output_path)

        # Then
        mock_mkdir.assert_called_once_with(