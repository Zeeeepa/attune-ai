"""Behavioral tests for pattern_cache.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
from unittest.mock import MagicMock, Mock, patch

import pytest

from empathy_os.pattern_cache import PatternMatchCache


class TestPatternMatchCacheInitialization:
    """Test suite for PatternMatchCache initialization."""

    def test_given_default_parameters_when_initialized_then_creates_cache_with_defaults(self):
        """Given default parameters
        When cache is initialized
        Then it creates cache with default max_size of 1000.
        """
        # Given / When
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            cache = PatternMatchCache()

        # Then
        assert cache.max_size == 1000
        assert cache._cache == {}
        assert cache._access_order == []
        mock_monitor.register_cache.assert_called_once_with(
            "pattern_match", max_size=1000
        )

    def test_given_custom_max_size_when_initialized_then_creates_cache_with_custom_size(self):
        """Given custom max_size parameter
        When cache is initialized
        Then it creates cache with specified max_size.
        """
        # Given
        custom_size = 500

        # When
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            cache = PatternMatchCache(max_size=custom_size)

        # Then
        assert cache.max_size == 500
        mock_monitor.register_cache.assert_called_once_with(
            "pattern_match", max_size=500
        )

    def test_given_already_registered_cache_when_initialized_then_handles_value_error(self):
        """Given cache already registered with monitor
        When new cache is initialized
        Then it handles ValueError gracefully.
        """
        # Given
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            mock_monitor.register_cache.side_effect = ValueError("Already registered")

            # When
            cache = PatternMatchCache()

        # Then - should not raise exception
        assert cache.max_size == 1000


class TestPatternMatchCacheMakeKey:
    """Test suite for cache key generation."""

    @pytest.fixture
    def cache(self):
        """Provide PatternMatchCache instance."""
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            return PatternMatchCache()

    def test_given_simple_context_when_make_key_then_returns_json_string(self, cache):
        """Given simple context dictionary
        When _make_key is called
        Then it returns JSON serialized string.
        """
        # Given
        context = {"domain": "testing", "language": "python"}

        # When
        key = cache._make_key(context)

        # Then
        assert isinstance(key, str)
        expected = json.dumps(context, sort_keys=True)
        assert key == expected

    def test_given_same_context_different_order_when_make_key_then_returns_same_key(
        self, cache
    ):
        """Given same context with different key order
        When _make_key is called
        Then it returns identical keys.
        """
        # Given
        context1 = {"domain": "testing", "language": "python"}
        context2 = {"language": "python", "domain": "testing"}

        # When
        key1 = cache._make_key(context1)
        key2 = cache._make_key(context2)

        # Then
        assert key1 == key2

    def test_given_nested_context_when_make_key_then_serializes_correctly(self, cache):
        """Given nested context dictionary
        When _make_key is called
        Then it serializes nested structure correctly.
        """
        # Given
        context = {
            "domain": "testing",
            "nested": {"key1": "value1", "key2": [1, 2, 3]},
        }

        # When
        key = cache._make_key(context)

        # Then
        assert isinstance(key, str)
        deserialized = json.loads(key)
        assert deserialized == context

    def test_given_empty_context_when_make_key_then_returns_empty_json(self, cache):
        """Given empty context dictionary
        When _make_key is called
        Then it returns empty JSON object string.
        """
        # Given
        context = {}

        # When
        key = cache._make_key(context)

        # Then
        assert key == "{}"

    def test_given_context_with_special_characters_when_make_key_then_escapes_correctly(
        self, cache
    ):
        """Given context with special characters
        When _make_key is called
        Then it escapes characters correctly.
        """
        # Given
        context = {"text": 'Hello "World"\nNew Line', "tab": "\t"}

        # When
        key = cache._make_key(context)

        # Then
        assert isinstance(key, str)
        deserialized = json.loads(key)
        assert deserialized == context


class TestPatternMatchCacheGet:
    """Test suite for cache retrieval operations."""

    @pytest.fixture
    def cache(self):
        """Provide PatternMatchCache instance with mocked monitor."""
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            cache_instance = PatternMatchCache()
            cache_instance._monitor = mock_monitor
            yield cache_instance

    def test_given_cached_result_when_get_then_returns_result_and_records_hit(
        self, cache
    ):
        """Given result exists in cache
        When get is called
        Then it returns cached result and records hit.
        """
        # Given
        context = {"domain": "testing"}
        expected_result = {"data": "cached_value"}
        key = cache._make_key(context)
        cache._cache[key] = expected_result
        cache._access_order.append(key)

        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor

            # When
            result = cache.get(context)

        # Then
        assert result == expected_result
        mock_monitor.record_hit.assert_called_once_with("pattern_match")
        mock_monitor.record_miss.assert_not_called()

    def test_given_cache_miss_when_get_then_returns_none_and_records_miss(self, cache):
        """Given result does not exist in cache
        When get is called
        Then it returns None and records miss.
        """
        # Given
        context = {"domain": "testing"}

        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor

            # When
            result = cache.get(context)

        # Then
        assert result is None
        mock_monitor.record_miss.assert_called_once_with("pattern_match")
        mock_monitor.record_hit.assert_not_called()

    def test_given_cached_result_when_get_then_moves_to_end_of_access_order(
        self, cache
    ):
        """Given result exists in cache
        When get is called
        Then it moves key to end of access order (LRU).
        """
        # Given
        context1 = {"id": 1}
        context2 = {"id": 2}
        key1 = cache._make_key(context1)
        key2 = cache._make_key(context2)
        cache._cache[key1] = "result1"
        cache._cache[key2] = "result2"
        cache._access_order = [key1, key2]

        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor

            # When - access first item
            cache.get(context1)

        # Then - key1 should be moved to end
        assert cache._access_order == [key2, key1]

    def test_given_multiple_cached_items_when_get_multiple_times_then_updates_lru_order(
        self, cache
    ):
        """Given multiple cached items
        When get is called multiple times
        Then it maintains correct LRU order.
        """
        # Given
        contexts = [{"id": i} for i in range(3)]
        keys = [cache._make_key(ctx) for ctx in contexts]
        for i, key in enumerate(keys):
            cache._cache[key] = f"result{i}"
            cache._access_order.append(key)

        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor

            # When - access in order: 0, 2, 1
            cache.get(contexts[0])
            cache.get(contexts[2])
            cache.get(contexts[1])

        # Then
        assert cache._access_order == [keys[0], keys[2], keys[1]]


class TestPatternMatchCacheSet:
    """Test suite for cache storage operations."""

    @pytest.fixture
    def cache(self):
        """Provide PatternMatchCache instance with mocked monitor."""
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            return PatternMatchCache()

    def test_given_new_result_when_set_then_caches_result(self, cache):
        """Given new result to cache
        When set is called
        Then it stores result in cache.
        """
        # Given
        context = {"domain": "testing"}
        result = {"data": "value"}

        # When
        cache.set(context, result)

        # Then
        key = cache._make_key(context)
        assert key in cache._cache
        assert cache._cache[key] == result

    def test_given_new_result_when_set_then_adds_to_access_order(self, cache):
        """Given new result to cache
        When set is called
        Then it adds key to access order.
        """
        # Given
        context = {"domain": "testing"}
        result = {"data": "value"}

        # When
        cache.set(context, result)

        # Then
        key = cache._make_key(context)
        assert key in cache._access_order
        assert cache._access_order[-1] == key

    def test_given_existing_key_when_set_then_updates_result(self, cache):
        """Given existing cached result
        When set is called with same context
        Then it updates the cached result.
        """
        # Given
        context = {"domain": "testing"}
        old_result = {"data": "old_value"}
        new_result = {"data": "new_value"}
        key = cache._make_key(context)
        cache._cache[key] = old_result
        cache._access_order.append(key)

        # When
        cache.set(context, new_result)

        # Then
        assert cache._cache[key] == new_result

    def test_given_existing_key_when_set_then_moves_to_end_of_access_order(self, cache):
        """Given existing cached result
        When set is called with same context
        Then it moves key to end of access order.
        """
        # Given
        context1 = {"id": 1}
        context2 = {"id": 2}
        key1 = cache._make_key(context1)
        key2 = cache._make_key(context2)
        cache._cache[key1] = "result1"
        cache._cache[key2] = "result2"
        cache._access_order = [key1, key2]

        # When - update first item
        cache.set(context1, "updated_result1")

        # Then - key1 should be moved to end
        assert cache._access_order == [key2, key1]

    def test_given_cache_at_max_size_when_set_then_evicts_lru_item(self, cache):
        """Given cache at maximum size
        When set is called with new item
        Then it evicts least recently used item.
        """
        # Given - small cache
        cache.max_size = 3
        contexts = [{"id": i} for i in range(3)]
        for ctx in contexts:
            cache.set(ctx, f"result_{ctx['id']}")

        # When - add fourth item
        new_context = {"id": 999}
        cache.set(new_context, "new_result")

        # Then - first item should be evicted
        first_key = cache._make_key(contexts[0])
        assert first_key not in cache._cache
        assert len(cache._cache) == 3

    def test_given_none_result_when_set_then_caches_none(self, cache):
        """Given None as result
        When set is called
        Then it caches None value.
        """
        # Given
        context = {"domain": "testing"}
        result = None

        # When
        cache.set(context, result)

        # Then
        key = cache._make_key(context)
        assert key in cache._cache
        assert cache._cache[key] is None

    def test_given_complex_result_when_set_then_caches_complex_object(self, cache):
        """Given complex result object
        When set is called
        Then it caches the complex object.
        """
        # Given
        context = {"domain": "testing"}
        result = {
            "nested": {"deep": {"value": 42}},
            "list": [1, 2, 3],
            "mixed": [{"key": "val"}],
        }

        # When
        cache.set(context, result)

        # Then
        key = cache._make_key(context)
        assert cache._cache[key] == result


class TestPatternMatchCacheIntegration:
    """Integration tests for PatternMatchCache."""

    @pytest.fixture
    def cache(self):
        """Provide PatternMatchCache instance with mocked monitor."""
        with patch("empathy_os.pattern_cache.CacheMonitor") as mock_monitor_class:
            mock_monitor = Mock()
            mock_monitor_class.get_instance.return_value = mock_monitor
            return PatternMatchCache(max_size=5)

    def test_given_multiple_operations_when_set_and_get_then_maintains_consistency(
        self, cache
    ):
        """Given multiple set and get operations
        When performed in sequence
        Then cache maintains consistency.
        """
        # Given
        contexts = [{"id": i, "type": "test"} for i in range(3)]
        results = [f"result_{i}" for i in range(3)]

        with patch("empathy_