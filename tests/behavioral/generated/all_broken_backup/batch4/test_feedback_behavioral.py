"""Behavioral tests for feedback.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest

from empathy_os.socratic.feedback import (
    AgentPerformance,
    FeedbackCollector,
    FeedbackRecommendation,
    FeedbackType,
)
from empathy_os.socratic.blueprint import AgentBlueprint, WorkflowBlueprint
from empathy_os.socratic.success import SuccessEvaluation


# =============================================================================
# FIXTURES
# =============================================================================


@pytest.fixture
def agent_blueprint():
    """Given a basic agent blueprint."""
    return AgentBlueprint(
        name="test_agent",
        role="test_role",
        goal="test_goal",
        backstory="test_backstory",
        template_id="test_template_001",
    )


@pytest.fixture
def workflow_blueprint():
    """Given a basic workflow blueprint."""
    return WorkflowBlueprint(
        name="test_workflow",
        description="test workflow description",
        agents=[],
        tasks=[],
        domain="testing",
        languages=["python"],
        quality_focus=["correctness"],
    )


@pytest.fixture
def success_evaluation():
    """Given a basic success evaluation."""
    return SuccessEvaluation(
        overall_success=True,
        overall_score=0.85,
        metrics={
            "task_completion": 0.9,
            "code_quality": 0.8,
        },
        reasoning="Test execution successful",
    )


@pytest.fixture
def failed_evaluation():
    """Given a failed success evaluation."""
    return SuccessEvaluation(
        overall_success=False,
        overall_score=0.35,
        metrics={
            "task_completion": 0.4,
            "code_quality": 0.3,
        },
        reasoning="Test execution failed",
    )


@pytest.fixture
def agent_performance():
    """Given an agent performance tracker."""
    return AgentPerformance(template_id="test_template_001")


@pytest.fixture
def feedback_collector(tmp_path):
    """Given a feedback collector with temporary storage."""
    storage_path = tmp_path / "feedback.json"
    return FeedbackCollector(storage_path=storage_path)


@pytest.fixture
def populated_feedback_collector(tmp_path, agent_blueprint, workflow_blueprint, success_evaluation):
    """Given a feedback collector with existing data."""
    storage_path = tmp_path / "feedback.json"
    collector = FeedbackCollector(storage_path=storage_path)
    
    # Record some feedback
    for i in range(5):
        collector.record_feedback(
            workflow=workflow_blueprint,
            evaluation=success_evaluation,
        )
    
    return collector


# =============================================================================
# AGENT PERFORMANCE TESTS
# =============================================================================


class TestAgentPerformanceInitialization:
    """Tests for AgentPerformance initialization."""

    def test_given_template_id_when_initialized_then_creates_instance(self):
        """Given a template ID, when initialized, then creates AgentPerformance instance."""
        # When
        performance = AgentPerformance(template_id="test_001")
        
        # Then
        assert performance.template_id == "test_001"
        assert performance.total_uses == 0
        assert performance.successful_uses == 0
        assert performance.average_score == 0.0
        assert performance.scores == []
        assert performance.by_domain == {}
        assert performance.by_language == {}
        assert performance.by_quality_focus == {}
        assert performance.recent_scores == []


class TestAgentPerformanceSuccessRate:
    """Tests for AgentPerformance success_rate property."""

    def test_given_no_uses_when_calculate_success_rate_then_returns_zero(self, agent_performance):
        """Given no uses, when calculating success rate, then returns 0.0."""
        # When
        rate = agent_performance.success_rate
        
        # Then
        assert rate == 0.0

    def test_given_all_successful_when_calculate_success_rate_then_returns_one(self, agent_performance):
        """Given all successful uses, when calculating success rate, then returns 1.0."""
        # Given
        agent_performance.total_uses = 10
        agent_performance.successful_uses = 10
        
        # When
        rate = agent_performance.success_rate
        
        # Then
        assert rate == 1.0

    def test_given_partial_success_when_calculate_success_rate_then_returns_fraction(self, agent_performance):
        """Given partial success, when calculating success rate, then returns correct fraction."""
        # Given
        agent_performance.total_uses = 10
        agent_performance.successful_uses = 7
        
        # When
        rate = agent_performance.success_rate
        
        # Then
        assert rate == 0.7


class TestAgentPerformanceTrend:
    """Tests for AgentPerformance trend property."""

    def test_given_insufficient_data_when_calculate_trend_then_returns_insufficient_data(self, agent_performance):
        """Given less than 5 scores, when calculating trend, then returns 'insufficient_data'."""
        # Given
        agent_performance.recent_scores = [("2024-01-01", 0.5), ("2024-01-02", 0.6)]
        
        # When
        trend = agent_performance.trend
        
        # Then
        assert trend == "insufficient_data"

    def test_given_only_5_scores_when_calculate_trend_then_returns_stable(self, agent_performance):
        """Given only 5 scores, when calculating trend, then returns 'stable'."""
        # Given
        agent_performance.recent_scores = [
            ("2024-01-01", 0.5),
            ("2024-01-02", 0.6),
            ("2024-01-03", 0.55),
            ("2024-01-04", 0.58),
            ("2024-01-05", 0.57),
        ]
        
        # When
        trend = agent_performance.trend
        
        # Then
        assert trend == "stable"

    def test_given_improving_scores_when_calculate_trend_then_returns_improving(self, agent_performance):
        """Given improving scores, when calculating trend, then returns 'improving'."""
        # Given
        agent_performance.recent_scores = [
            ("2024-01-01", 0.4),
            ("2024-01-02", 0.45),
            ("2024-01-03", 0.5),
            ("2024-01-04", 0.55),
            ("2024-01-05", 0.6),
            ("2024-01-06", 0.65),
            ("2024-01-07", 0.7),
            ("2024-01-08", 0.75),
            ("2024-01-09", 0.8),
            ("2024-01-10", 0.85),
        ]
        
        # When
        trend = agent_performance.trend
        
        # Then
        assert trend == "improving"

    def test_given_declining_scores_when_calculate_trend_then_returns_declining(self, agent_performance):
        """Given declining scores, when calculating trend, then returns 'declining'."""
        # Given
        agent_performance.recent_scores = [
            ("2024-01-01", 0.85),
            ("2024-01-02", 0.8),
            ("2024-01-03", 0.75),
            ("2024-01-04", 0.7),
            ("2024-01-05", 0.65),
            ("2024-01-06", 0.6),
            ("2024-01-07", 0.55),
            ("2024-01-08", 0.5),
            ("2024-01-09", 0.45),
            ("2024-01-10", 0.4),
        ]
        
        # When
        trend = agent_performance.trend
        
        # Then
        assert trend == "declining"

    def test_given_stable_scores_when_calculate_trend_then_returns_stable(self, agent_performance):
        """Given stable scores, when calculating trend, then returns 'stable'."""
        # Given
        agent_performance.recent_scores = [
            ("2024-01-01", 0.7),
            ("2024-01-02", 0.72),
            ("2024-01-03", 0.68),
            ("2024-01-04", 0.71),
            ("2024-01-05", 0.69),
            ("2024-01-06", 0.7),
            ("2024-01-07", 0.72),
            ("2024-01-08", 0.68),
            ("2024-01-09", 0.71),
            ("2024-01-10", 0.69),
        ]
        
        # When
        trend = agent_performance.trend
        
        # Then
        assert trend == "stable"


class TestAgentPerformanceRecordUse:
    """Tests for AgentPerformance record_use method."""

    def test_given_successful_use_when_record_use_then_updates_stats(self, agent_performance):
        """Given a successful use, when recording use, then updates statistics."""
        # When
        agent_performance.record_use(success=True, score=0.8)
        
        # Then
        assert agent_performance.total_uses == 1
        assert agent_performance.successful_uses == 1
        assert agent_performance.average_score == 0.8
        assert len(agent_performance.scores) == 1
        assert len(agent_performance.recent_scores) == 1

    def test_given_failed_use_when_record_use_then_updates_stats(self, agent_performance):
        """Given a failed use, when recording use, then updates statistics."""
        # When
        agent_performance.record_use(success=False, score=0.3)
        
        # Then
        assert agent_performance.total_uses == 1
        assert agent_performance.successful_uses == 0
        assert agent_performance.average_score == 0.3
        assert len(agent_performance.scores) == 1

    def test_given_multiple_uses_when_record_use_then_calculates_average(self, agent_performance):
        """Given multiple uses, when recording use, then calculates correct average."""
        # When
        agent_performance.record_use(success=True, score=0.8)
        agent_performance.record_use(success=True, score=0.6)
        agent_performance.record_use(success=False, score=0.4)
        
        # Then
        assert agent_performance.total_uses == 3
        assert agent_performance.successful_uses == 2
        assert agent_performance.average_score == pytest.approx(0.6)

    def test_given_domain_when_record_use_then_tracks_domain_stats(self, agent_performance):
        """Given a domain, when recording use, then tracks domain-specific statistics."""
        # When
        agent_performance.record_use(success=True, score=0.8, domain="testing")
        
        # Then
        assert "testing" in agent_performance.by_domain
        assert agent_performance.by_domain["testing"]["total_uses"] == 1
        assert agent_performance.by_domain["testing"]["successful_uses"] == 1
        assert agent_performance.by_domain["testing"]["average_score"] == 0.8

    def test_given_languages_when_record_use_then_tracks_language_stats(self, agent_performance):
        """Given languages, when recording use, then tracks language-specific statistics."""
        # When
        agent_performance.record_use(success=True, score=0.8, languages=["python", "java"])
        
        # Then
        assert "python" in agent_performance.by_language
        assert "java" in agent_performance.by_language
        assert agent_performance.by_language["python"]["total_uses"] == 1
        assert agent_performance.by_language["java"]["total_uses"] == 1

    def test_given_quality_focus_when_record_use_then_tracks_quality_stats(self, agent_performance):
        """Given quality focus, when recording use, then tracks quality-specific statistics."""
        # When
        agent_performance.record_use(
            success=True,
            score=0.8,
            quality_focus=["correctness", "performance"]
        )
        
        # Then
        assert "correctness" in agent_performance.by_quality_focus
        assert "performance" in agent_performance.by_quality_focus
        assert agent_performance.by_quality_focus["correctness"]["total_uses"] == 1


# =============================================================================
# FEEDBACK COLLECTOR TESTS
# =============================================================================


class TestFeedbackCollectorInitialization:
    """Tests for FeedbackCollector initialization."""

    def test_given_storage_path_when_initialized_then_creates_instance(self, tmp_path):
        """Given a storage path, when initialized, then creates FeedbackCollector instance."""
        # Given
        storage_path = tmp_path / "feedback.json"
        
        # When
        collector = FeedbackCollector(storage_path=storage_path)
        
        # Then
        assert collector.storage_path == storage_path
        assert collector.agent_performances == {}
        assert collector.feedback_history == []

    def test_given_existing_data_when_initialized_then_loads_data(self, tmp_path):
        """Given existing data, when initialized, then loads the data."""
        # Given
        storage_path = tmp_path / "feedback.json"
        existing_data = {
            "agent_performances": {
                "test_001": {
                    "template_id": "test_001",
                    "total_uses": 5,
                    "successful_uses": 4,
                    "average_score": 0.75,
                    "scores": [0.7, 0.8, 0.75, 0.7, 0.8],
                    "by_domain": {},
                    "by_language": {},
                    "by_quality_focus": {},
                    "recent_scores": [],
                }
            },
            "feedback_history": [],
        }
        storage_path.write_text(json.dumps(existing_data))
        
        # When
        collector = FeedbackCollector(storage_path=storage_path)
        
        # Then
        assert "test_001" in collector.agent_performances
        assert collector.agent_performances["test_001"].total_uses == 5

    def test_given_invalid_json_when_initialized_then_starts_fresh(self, tmp_path, caplog):
        """Given invalid JSON, when initialized, then starts with fresh data."""
        # Given
        storage_path = tmp_path / "feedback.json"
        storage_path.write_text("invalid json")
        
        # When
        with caplog.at_level(logging.WARNING):
            collector = FeedbackCollector(storage_path=storage_path)
        
        # Then
        assert collector.agent_performances == {}
        assert "Failed to load feedback data" in caplog.text

    def test_given_nonexistent_file_when_initialized_then_starts_fresh(self, tmp_path):
        """Given nonexistent file, when initialized, then starts with fresh data."""
        # Given
        storage_path = tmp_path / "nonexistent.json"
        
        # When
        collector = FeedbackCollector(storage_path=storage_path)
        
        # Then
        assert collector.agent_performances == {}