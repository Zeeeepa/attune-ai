"""Behavioral tests for caching.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

import logging
from typing import Any
from unittest.mock import MagicMock, Mock, patch

import pytest

from empathy_os.workflows.caching import (
    CachedResponse,
    CacheAwareWorkflow,
    CachingMixin,
)


class TestCachedResponse:
    """Tests for CachedResponse dataclass."""

    def test_given_cached_response_when_created_then_stores_all_fields(self):
        """Given: Valid response data
        When: CachedResponse is created
        Then: All fields are stored correctly
        """
        # Given
        content = "Test response"
        input_tokens = 100
        output_tokens = 50

        # When
        response = CachedResponse(
            content=content,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
        )

        # Then
        assert response.content == content
        assert response.input_tokens == input_tokens
        assert response.output_tokens == output_tokens

    def test_given_cached_response_when_converted_to_dict_then_returns_correct_format(
        self,
    ):
        """Given: A CachedResponse instance
        When: to_dict() is called
        Then: Returns dictionary with all fields
        """
        # Given
        response = CachedResponse(
            content="Hello world",
            input_tokens=200,
            output_tokens=100,
        )

        # When
        result = response.to_dict()

        # Then
        assert isinstance(result, dict)
        assert result["content"] == "Hello world"
        assert result["input_tokens"] == 200
        assert result["output_tokens"] == 100

    def test_given_dict_when_from_dict_called_then_creates_cached_response(self):
        """Given: A dictionary with response data
        When: from_dict() is called
        Then: Returns CachedResponse instance with correct values
        """
        # Given
        data = {
            "content": "Cached content",
            "input_tokens": 150,
            "output_tokens": 75,
        }

        # When
        response = CachedResponse.from_dict(data)

        # Then
        assert isinstance(response, CachedResponse)
        assert response.content == "Cached content"
        assert response.input_tokens == 150
        assert response.output_tokens == 75

    def test_given_cached_response_when_roundtrip_to_dict_and_back_then_preserves_data(
        self,
    ):
        """Given: A CachedResponse instance
        When: Converting to dict and back
        Then: Data is preserved correctly
        """
        # Given
        original = CachedResponse(
            content="Test roundtrip",
            input_tokens=300,
            output_tokens=150,
        )

        # When
        dict_form = original.to_dict()
        restored = CachedResponse.from_dict(dict_form)

        # Then
        assert restored.content == original.content
        assert restored.input_tokens == original.input_tokens
        assert restored.output_tokens == original.output_tokens

    def test_given_zero_tokens_when_created_then_handles_edge_case(self):
        """Given: Zero token counts
        When: CachedResponse is created
        Then: Handles zero values correctly
        """
        # Given/When
        response = CachedResponse(
            content="",
            input_tokens=0,
            output_tokens=0,
        )

        # Then
        assert response.content == ""
        assert response.input_tokens == 0
        assert response.output_tokens == 0

    def test_given_large_token_counts_when_created_then_handles_large_numbers(self):
        """Given: Very large token counts
        When: CachedResponse is created
        Then: Handles large numbers correctly
        """
        # Given/When
        response = CachedResponse(
            content="Large response",
            input_tokens=1000000,
            output_tokens=500000,
        )

        # Then
        assert response.input_tokens == 1000000
        assert response.output_tokens == 500000


class TestCacheAwareWorkflow:
    """Tests for CacheAwareWorkflow protocol."""

    def test_given_class_with_protocol_methods_when_checked_then_is_instance(self):
        """Given: A class implementing CacheAwareWorkflow protocol
        When: Checking with isinstance
        Then: Returns True
        """

        # Given
        class MockWorkflow:
            name = "test_workflow"
            _cache = None
            _enable_cache = True

            def get_model_for_tier(self, tier: Any) -> str:
                return "test-model"

        workflow = MockWorkflow()

        # When/Then
        assert isinstance(workflow, CacheAwareWorkflow)

    def test_given_class_missing_protocol_methods_when_checked_then_is_not_instance(
        self,
    ):
        """Given: A class missing protocol methods
        When: Checking with isinstance
        Then: Returns False
        """

        # Given
        class IncompleteWorkflow:
            name = "incomplete"

        workflow = IncompleteWorkflow()

        # When/Then
        assert not isinstance(workflow, CacheAwareWorkflow)


class TestCachingMixin:
    """Tests for CachingMixin class."""

    @pytest.fixture
    def mock_cache(self):
        """Fixture providing a mock cache."""
        cache = Mock()
        cache.get = Mock(return_value=None)
        cache.set = Mock()
        return cache

    @pytest.fixture
    def workflow_with_mixin(self):
        """Fixture providing a workflow class with CachingMixin."""

        class TestWorkflow(CachingMixin):
            def __init__(self):
                self.name = "test_workflow"
                self._cache = None
                self._enable_cache = True
                self._cache_setup_attempted = False

        return TestWorkflow()

    def test_given_new_mixin_when_initialized_then_has_default_values(
        self, workflow_with_mixin
    ):
        """Given: A new CachingMixin instance
        When: Checking initial state
        Then: Has correct default values
        """
        # Given/When
        workflow = workflow_with_mixin

        # Then
        assert workflow._cache is None
        assert workflow._enable_cache is True
        assert workflow._cache_setup_attempted is False

    def test_given_cache_disabled_when_setup_called_then_does_nothing(
        self, workflow_with_mixin
    ):
        """Given: Caching is disabled
        When: _maybe_setup_cache is called
        Then: No cache setup is attempted
        """
        # Given
        workflow = workflow_with_mixin
        workflow._enable_cache = False

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is None
        assert workflow._cache_setup_attempted is False

    def test_given_cache_already_setup_when_called_again_then_skips_setup(
        self, workflow_with_mixin
    ):
        """Given: Cache setup already attempted
        When: _maybe_setup_cache is called again
        Then: Skips setup
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache_setup_attempted = True

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is None

    def test_given_cache_provided_when_setup_called_then_uses_existing_cache(
        self, workflow_with_mixin, mock_cache
    ):
        """Given: Cache is already provided
        When: _maybe_setup_cache is called
        Then: Uses existing cache without modification
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is mock_cache
        assert workflow._cache_setup_attempted is True

    @patch("empathy_os.workflows.caching.auto_setup_cache")
    @patch("empathy_os.workflows.caching.create_cache")
    def test_given_no_cache_when_auto_setup_succeeds_then_creates_cache(
        self,
        mock_create_cache,
        mock_auto_setup,
        workflow_with_mixin,
        mock_cache,
    ):
        """Given: No cache is provided and auto-setup succeeds
        When: _maybe_setup_cache is called
        Then: Creates and assigns cache
        """
        # Given
        workflow = workflow_with_mixin
        mock_auto_setup.return_value = {"type": "redis", "host": "localhost"}
        mock_create_cache.return_value = mock_cache

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is mock_cache
        assert workflow._cache_setup_attempted is True
        mock_auto_setup.assert_called_once()
        mock_create_cache.assert_called_once_with(
            {"type": "redis", "host": "localhost"}
        )

    @patch("empathy_os.workflows.caching.auto_setup_cache")
    def test_given_no_cache_when_auto_setup_returns_none_then_no_cache_created(
        self, mock_auto_setup, workflow_with_mixin
    ):
        """Given: No cache and auto-setup returns None
        When: _maybe_setup_cache is called
        Then: No cache is created
        """
        # Given
        workflow = workflow_with_mixin
        mock_auto_setup.return_value = None

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is None
        assert workflow._cache_setup_attempted is True

    @patch("empathy_os.workflows.caching.auto_setup_cache")
    @patch("empathy_os.workflows.caching.create_cache")
    @patch("empathy_os.workflows.caching.logger")
    def test_given_cache_creation_fails_when_setup_called_then_logs_error(
        self,
        mock_logger,
        mock_create_cache,
        mock_auto_setup,
        workflow_with_mixin,
    ):
        """Given: Cache creation raises exception
        When: _maybe_setup_cache is called
        Then: Logs error and continues without cache
        """
        # Given
        workflow = workflow_with_mixin
        mock_auto_setup.return_value = {"type": "redis"}
        mock_create_cache.side_effect = Exception("Connection failed")

        # When
        workflow._maybe_setup_cache()

        # Then
        assert workflow._cache is None
        assert workflow._cache_setup_attempted is True
        mock_logger.info.assert_called()

    def test_given_cache_exists_when_try_cache_lookup_called_then_returns_cached_data(
        self, workflow_with_mixin, mock_cache
    ):
        """Given: Cache exists with cached data
        When: _try_cache_lookup is called
        Then: Returns cached response
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache
        cached_data = {
            "content": "Cached result",
            "input_tokens": 50,
            "output_tokens": 25,
        }
        mock_cache.get.return_value = cached_data

        # When
        result = workflow._try_cache_lookup("test_key", "gpt-4")

        # Then
        assert isinstance(result, CachedResponse)
        assert result.content == "Cached result"
        assert result.input_tokens == 50
        assert result.output_tokens == 25
        mock_cache.get.assert_called_once_with("workflow:test_workflow:test_key:gpt-4")

    def test_given_cache_exists_when_no_cached_data_then_returns_none(
        self, workflow_with_mixin, mock_cache
    ):
        """Given: Cache exists but no data for key
        When: _try_cache_lookup is called
        Then: Returns None
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache
        mock_cache.get.return_value = None

        # When
        result = workflow._try_cache_lookup("test_key", "gpt-4")

        # Then
        assert result is None

    def test_given_no_cache_when_try_cache_lookup_called_then_returns_none(
        self, workflow_with_mixin
    ):
        """Given: No cache is configured
        When: _try_cache_lookup is called
        Then: Returns None
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = None

        # When
        result = workflow._try_cache_lookup("test_key", "gpt-4")

        # Then
        assert result is None

    @patch("empathy_os.workflows.caching.logger")
    def test_given_cache_lookup_fails_when_called_then_logs_warning_and_returns_none(
        self, mock_logger, workflow_with_mixin, mock_cache
    ):
        """Given: Cache lookup raises exception
        When: _try_cache_lookup is called
        Then: Logs warning and returns None
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache
        mock_cache.get.side_effect = Exception("Cache error")

        # When
        result = workflow._try_cache_lookup("test_key", "gpt-4")

        # Then
        assert result is None
        mock_logger.warning.assert_called()

    def test_given_cache_exists_when_store_in_cache_called_then_stores_data(
        self, workflow_with_mixin, mock_cache
    ):
        """Given: Cache exists
        When: _store_in_cache is called
        Then: Stores data in cache
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache
        response = CachedResponse(
            content="New result",
            input_tokens=100,
            output_tokens=50,
        )

        # When
        workflow._store_in_cache("test_key", "gpt-4", response)

        # Then
        mock_cache.set.assert_called_once()
        call_args = mock_cache.set.call_args
        assert call_args[0][0] == "workflow:test_workflow:test_key:gpt-4"
        assert call_args[0][1]["content"] == "New result"
        assert call_args[0][1]["input_tokens"] == 100
        assert call_args[0][1]["output_tokens"] == 50

    def test_given_cache_exists_when_store_with_ttl_then_uses_ttl(
        self, workflow_with_mixin, mock_cache
    ):
        """Given: Cache exists
        When: _store_in_cache is called with TTL
        Then: Stores data with specified TTL
        """
        # Given
        workflow = workflow_with_mixin
        workflow._cache = mock_cache
        response = CachedResponse(content="Result", input_tokens=10, output_tokens=5)

        # When
        workflow._store_in_cache("test_key", "gpt-4", response, ttl=3600)

        # Then
        mock_cache.set.assert_called_once()
        call_args = mock_cache.set.call_args
        assert call_args[1]["ttl"] == 3600

    def test_given_no_cache_when_store_in_cache_called_then_does_nothing(