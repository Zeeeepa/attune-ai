"""Behavioral tests for token_estimator.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

from unittest.mock import MagicMock, Mock, patch

import pytest

from empathy_os.models.token_estimator import (
    TIKTOKEN_AVAILABLE,
    TOKENS_PER_CHAR_HEURISTIC,
    _get_encoding,
    estimate_tokens,
    estimate_workflow_cost,
)


class TestGetEncoding:
    """Behavioral tests for _get_encoding function."""

    def test_get_encoding_when_tiktoken_unavailable_then_returns_none(self):
        """Given tiktoken is not available
        When _get_encoding is called
        Then it returns None
        """
        # Given
        with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", False):
            # When
            result = _get_encoding("gpt-4")
            
            # Then
            assert result is None

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_claude_model_then_returns_cl100k_base(self):
        """Given a Claude model ID
        When _get_encoding is called
        Then it returns cl100k_base encoding
        """
        # Given
        model_id = "claude-sonnet-4-5"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None
        assert result.name == "cl100k_base"

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_anthropic_model_then_returns_cl100k_base(self):
        """Given an Anthropic model ID
        When _get_encoding is called
        Then it returns cl100k_base encoding
        """
        # Given
        model_id = "anthropic.claude-v2"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None
        assert result.name == "cl100k_base"

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_gpt4_model_then_returns_gpt4_encoding(self):
        """Given a GPT-4 model ID
        When _get_encoding is called
        Then it returns gpt-4 encoding
        """
        # Given
        model_id = "gpt-4-turbo"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_gpt35_model_then_returns_gpt4_encoding(self):
        """Given a GPT-3.5 model ID
        When _get_encoding is called
        Then it returns gpt-4 encoding
        """
        # Given
        model_id = "gpt-3.5-turbo"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_o1_model_then_returns_gpt4_encoding(self):
        """Given an O1 model ID
        When _get_encoding is called
        Then it returns gpt-4 encoding
        """
        # Given
        model_id = "o1-preview"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_when_unknown_model_then_returns_default_encoding(self):
        """Given an unknown model ID
        When _get_encoding is called
        Then it returns cl100k_base as default
        """
        # Given
        model_id = "unknown-model-xyz"
        
        # When
        result = _get_encoding(model_id)
        
        # Then
        assert result is not None
        assert result.name == "cl100k_base"

    @pytest.mark.skipif(not TIKTOKEN_AVAILABLE, reason="tiktoken not available")
    def test_get_encoding_uses_cache_for_repeated_calls(self):
        """Given multiple calls with same model ID
        When _get_encoding is called repeatedly
        Then it uses cache and returns same instance
        """
        # Given
        model_id = "gpt-4"
        
        # When
        result1 = _get_encoding(model_id)
        result2 = _get_encoding(model_id)
        
        # Then
        assert result1 is result2


class TestEstimateTokens:
    """Behavioral tests for estimate_tokens function."""

    def test_estimate_tokens_when_empty_model_id_then_raises_value_error(self):
        """Given an empty model_id
        When estimate_tokens is called
        Then it raises ValueError
        """
        # Given
        text = "Some text"
        model_id = ""
        
        # When/Then
        with pytest.raises(ValueError, match="model_id cannot be empty"):
            estimate_tokens(text, model_id)

    def test_estimate_tokens_when_whitespace_model_id_then_raises_value_error(self):
        """Given a whitespace-only model_id
        When estimate_tokens is called
        Then it raises ValueError
        """
        # Given
        text = "Some text"
        model_id = "   "
        
        # When/Then
        with pytest.raises(ValueError, match="model_id cannot be empty"):
            estimate_tokens(text, model_id)

    def test_estimate_tokens_when_empty_text_then_returns_zero(self):
        """Given an empty text string
        When estimate_tokens is called
        Then it returns 0
        """
        # Given
        text = ""
        model_id = "claude-sonnet-4-5"
        
        # When
        result = estimate_tokens(text, model_id)
        
        # Then
        assert result == 0

    def test_estimate_tokens_when_toolkit_available_then_uses_count_tokens(self):
        """Given empathy_llm_toolkit is available
        When estimate_tokens is called
        Then it uses count_tokens from toolkit
        """
        # Given
        text = "Hello, world!"
        model_id = "claude-sonnet-4-5"
        mock_count = 10
        
        with patch("empathy_os.models.token_estimator.count_tokens", return_value=mock_count) as mock_count_tokens:
            # When
            result = estimate_tokens(text, model_id)
            
            # Then
            assert result == mock_count
            mock_count_tokens.assert_called_once_with(text, model=model_id, use_api=False)

    def test_estimate_tokens_when_toolkit_unavailable_then_falls_back_to_tiktoken(self):
        """Given empathy_llm_toolkit is not available
        When estimate_tokens is called
        Then it falls back to tiktoken
        """
        # Given
        text = "Hello, world!"
        model_id = "gpt-4"
        mock_encoding = Mock()
        mock_encoding.encode.return_value = [1, 2, 3, 4]
        
        with patch("empathy_os.models.token_estimator.count_tokens", side_effect=ImportError):
            with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", True):
                with patch("empathy_os.models.token_estimator._get_encoding", return_value=mock_encoding):
                    # When
                    result = estimate_tokens(text, model_id)
                    
                    # Then
                    assert result == 4
                    mock_encoding.encode.assert_called_once_with(text)

    def test_estimate_tokens_when_tiktoken_fails_then_falls_back_to_heuristic(self):
        """Given tiktoken encoding fails
        When estimate_tokens is called
        Then it uses heuristic fallback
        """
        # Given
        text = "Hello, world!"
        model_id = "gpt-4"
        expected_tokens = max(1, int(len(text) * TOKENS_PER_CHAR_HEURISTIC))
        
        with patch("empathy_os.models.token_estimator.count_tokens", side_effect=ImportError):
            with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", True):
                with patch("empathy_os.models.token_estimator._get_encoding", side_effect=Exception("Encoding error")):
                    # When
                    result = estimate_tokens(text, model_id)
                    
                    # Then
                    assert result == expected_tokens

    def test_estimate_tokens_when_no_toolkit_no_tiktoken_then_uses_heuristic(self):
        """Given neither toolkit nor tiktoken are available
        When estimate_tokens is called
        Then it uses heuristic fallback
        """
        # Given
        text = "Hello, world!"
        model_id = "gpt-4"
        expected_tokens = max(1, int(len(text) * TOKENS_PER_CHAR_HEURISTIC))
        
        with patch("empathy_os.models.token_estimator.count_tokens", side_effect=ImportError):
            with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", False):
                # When
                result = estimate_tokens(text, model_id)
                
                # Then
                assert result == expected_tokens

    def test_estimate_tokens_heuristic_minimum_is_one(self):
        """Given a very short text
        When heuristic fallback is used
        Then it returns at least 1 token
        """
        # Given
        text = "a"  # Single character
        model_id = "gpt-4"
        
        with patch("empathy_os.models.token_estimator.count_tokens", side_effect=ImportError):
            with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", False):
                # When
                result = estimate_tokens(text, model_id)
                
                # Then
                assert result >= 1

    def test_estimate_tokens_uses_default_model_id(self):
        """Given no model_id is provided
        When estimate_tokens is called
        Then it uses default claude-sonnet-4-5-20250514
        """
        # Given
        text = "Hello, world!"
        mock_count = 10
        
        with patch("empathy_os.models.token_estimator.count_tokens", return_value=mock_count) as mock_count_tokens:
            # When
            result = estimate_tokens(text)
            
            # Then
            assert result == mock_count
            mock_count_tokens.assert_called_once_with(text, model="claude-sonnet-4-5-20250514", use_api=False)

    def test_estimate_tokens_with_long_text_returns_reasonable_count(self):
        """Given a long text string
        When estimate_tokens is called with heuristic
        Then it returns a reasonable token count
        """
        # Given
        text = "word " * 100  # 100 words
        model_id = "gpt-4"
        
        with patch("empathy_os.models.token_estimator.count_tokens", side_effect=ImportError):
            with patch("empathy_os.models.token_estimator.TIKTOKEN_AVAILABLE", False):
                # When
                result = estimate_tokens(text, model_id)
                
                # Then
                expected = max(1, int(len(text) * TOKENS_PER_CHAR_HEURISTIC))
                assert result == expected
                assert result > 50  # Should be substantial for 100 words

    def test_estimate_tokens_with_special_characters(self):
        """Given text with special characters
        When estimate_tokens is called
        Then it handles them correctly
        """
        # Given
        text = "Hello! @#$%^&*() ä½ å¥½ ðŸŽ‰"
        model_id = "gpt-4"
        mock_count = 15
        
        with patch("empathy_os.models.token_estimator.count_tokens", return_value=mock_count):
            # When
            result = estimate_tokens(text, model_id)
            
            # Then
            assert result == mock_count

    def test_estimate_tokens_with_multiline_text(self):
        """Given multiline text
        When estimate_tokens is called
        Then it handles newlines correctly
        """
        # Given
        text = "Line 1\nLine 2\nLine 3"
        model_id = "gpt-4"
        mock_count = 8
        
        with patch("empathy_os.models.token_estimator.count_tokens", return_value=mock_count):
            # When
            result = estimate_tokens(text, model_id)
            
            # Then
            assert result == mock_count


class TestEstimateWorkflowCost:
    """Behavioral tests for estimate_workflow_cost function."""

    def test_estimate_workflow_cost_signature_exists(self):
        """Given the estimate_workflow_cost function
        When checking its signature
        Then it has the expected parameters
        """
        # Given/When
        import inspect
        sig = inspect.signature(estimate_workflow_cost)
        
        # Then
        assert "workflow_name" in sig.parameters
        assert "input_text" in sig.parameters
        assert "provider" in sig.parameters
        assert "target_path" in sig.parameters

    def test_estimate_workflow_cost_has_default_provider(self):
        """Given the estimate_workflow_cost function
        When checking parameter defaults
        Then provider defaults to 'anthropic'
        """
        # Given/When
        import inspect
        sig = inspect.signature(estimate_workflow_cost)
        
        # Then
        assert sig.parameters["provider"].default == "anthropic"

    def test_estimate_workflow_cost_has_optional_target_path(self):
        """Given the estimate_workflow_cost function
        When checking parameter defaults
        Then target_path defaults to None
        """
        # Given/When
        import inspect
        sig = inspect.signature(estimate_workflow_cost)
        
        # Then
        assert sig.parameters["target_path"].default is None

    def test_estimate_workflow_cost_returns_dict(self):
        """Given valid workflow parameters
        When estimate_workflow_cost is called
        Then it returns a dictionary
        """
        # Given
        workflow_name = "test_workflow"
        input_text = "Sample input"
        
        # When
        result = estimate_workflow_cost(workflow_name, input_text)
        
        # Then
        assert isinstance(result, dict)


class TestModuleConstants:
    """Behavioral tests for module constants."""

    def test_tiktoken_available_is_boolean(self):
        """Given the TIKTOKEN_AVAILABLE constant
        When checking its type
        Then it is a boolean
        """
        # Given/When/Then
        assert isinstance(TIKTOKEN_AVAILABLE, bool)

    def test_tokens_per_char_heuristic_is_reasonable(self):
        """Given the TOKENS_PER_CHAR_HEURISTIC constant
        When checking its value
        Then it is 0.25 (reasonable heuristic)
        """
        # Given/When/