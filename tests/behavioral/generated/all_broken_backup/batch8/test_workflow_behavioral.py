"""Behavioral tests for workflow.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from empathy_os.workflows.base import ModelTier
from empathy_os.workflows.keyboard_shortcuts.workflow import KeyboardShortcutWorkflow
from empathy_os.workflows.keyboard_shortcuts.schema import (
    FeatureManifest,
    FrequencyTier,
    GeneratedShortcuts,
    KeyboardLayout,
    LayoutShortcuts,
    ScaleAssignments,
    ShortcutAssignment,
)


# Fixtures


@pytest.fixture
def workflow():
    """Given a KeyboardShortcutWorkflow instance."""
    return KeyboardShortcutWorkflow()


@pytest.fixture
def mock_parser():
    """Given a mocked CompositeParser."""
    with patch("empathy_os.workflows.keyboard_shortcuts.workflow.CompositeParser") as mock:
        yield mock.return_value


@pytest.fixture
def mock_generator():
    """Given a mocked ComprehensiveGenerator."""
    with patch("empathy_os.workflows.keyboard_shortcuts.workflow.ComprehensiveGenerator") as mock:
        yield mock.return_value


@pytest.fixture
def sample_features():
    """Given sample features for testing."""
    return {
        "features": [
            {
                "id": "save",
                "name": "Save File",
                "description": "Save current file",
                "category": "file",
            },
            {
                "id": "open",
                "name": "Open File",
                "description": "Open a file",
                "category": "file",
            },
            {
                "id": "undo",
                "name": "Undo",
                "description": "Undo last action",
                "category": "edit",
            },
        ]
    }


@pytest.fixture
def sample_manifest():
    """Given a sample FeatureManifest."""
    return FeatureManifest(
        features=[
            {
                "id": "save",
                "name": "Save File",
                "description": "Save current file",
                "category": "file",
                "frequency": FrequencyTier.DAILY,
                "mnemonic": "Save",
            },
            {
                "id": "open",
                "name": "Open File",
                "description": "Open a file",
                "category": "file",
                "frequency": FrequencyTier.FREQUENT,
                "mnemonic": "Open",
            },
        ]
    )


@pytest.fixture
def sample_shortcuts():
    """Given sample generated shortcuts."""
    return GeneratedShortcuts(
        layouts={
            KeyboardLayout.QWERTY: LayoutShortcuts(
                scale_1=[
                    ShortcutAssignment(
                        feature_id="save",
                        keys=["Cmd", "S"],
                        rationale="Common save shortcut",
                    )
                ],
                scale_2=[
                    ShortcutAssignment(
                        feature_id="open",
                        keys=["Cmd", "O"],
                        rationale="Common open shortcut",
                    )
                ],
                scale_3=[],
            )
        }
    )


# Test KeyboardShortcutWorkflow initialization


class TestKeyboardShortcutWorkflowInit:
    """Tests for KeyboardShortcutWorkflow initialization."""

    def test_workflow_attributes_initialized(self):
        """Given a new workflow instance
        When the workflow is created
        Then it should have correct attributes set.
        """
        workflow = KeyboardShortcutWorkflow()

        assert workflow.name == "keyboard-shortcuts"
        assert workflow.description == "Generate ergonomic keyboard shortcuts with multi-layout support"
        assert workflow.stages == ["discover", "analyze", "generate", "validate", "export"]
        assert len(workflow.tier_map) == 5

    def test_workflow_tier_mapping(self):
        """Given a new workflow instance
        When checking tier mappings
        Then each stage should have appropriate tier.
        """
        workflow = KeyboardShortcutWorkflow()

        assert workflow.tier_map["discover"] == ModelTier.CHEAP
        assert workflow.tier_map["analyze"] == ModelTier.CAPABLE
        assert workflow.tier_map["generate"] == ModelTier.CAPABLE
        assert workflow.tier_map["validate"] == ModelTier.CHEAP
        assert workflow.tier_map["export"] == ModelTier.CHEAP

    def test_workflow_parser_and_generator_initialized(self, mock_parser, mock_generator):
        """Given CompositeParser and ComprehensiveGenerator classes
        When creating a workflow
        Then parser and generator should be initialized.
        """
        workflow = KeyboardShortcutWorkflow()

        assert workflow.parser is not None
        assert workflow.generator is not None


# Test run_stage method


class TestRunStage:
    """Tests for run_stage method."""

    @pytest.mark.asyncio
    async def test_run_discover_stage(self, workflow):
        """Given a workflow with mocked _discover_features
        When run_stage is called with 'discover'
        Then it should call _discover_features.
        """
        workflow._discover_features = AsyncMock(return_value=({"data": "test"}, 0, 0))
        input_data = {"source_dir": "/test"}

        result, inp_tokens, out_tokens = await workflow.run_stage(
            "discover", ModelTier.CHEAP, input_data
        )

        workflow._discover_features.assert_called_once_with(input_data)
        assert result == {"data": "test"}
        assert inp_tokens == 0
        assert out_tokens == 0

    @pytest.mark.asyncio
    async def test_run_analyze_stage(self, workflow):
        """Given a workflow with mocked _analyze_features
        When run_stage is called with 'analyze'
        Then it should call _analyze_features with tier.
        """
        workflow._analyze_features = AsyncMock(return_value=({"data": "analyzed"}, 100, 50))
        input_data = {"features": []}

        result, inp_tokens, out_tokens = await workflow.run_stage(
            "analyze", ModelTier.CAPABLE, input_data
        )

        workflow._analyze_features.assert_called_once_with(input_data, ModelTier.CAPABLE)
        assert result == {"data": "analyzed"}
        assert inp_tokens == 100
        assert out_tokens == 50

    @pytest.mark.asyncio
    async def test_run_generate_stage(self, workflow):
        """Given a workflow with mocked _generate_shortcuts
        When run_stage is called with 'generate'
        Then it should call _generate_shortcuts with tier.
        """
        workflow._generate_shortcuts = AsyncMock(return_value=({"shortcuts": []}, 200, 100))
        input_data = {"manifest": {}}

        result, inp_tokens, out_tokens = await workflow.run_stage(
            "generate", ModelTier.CAPABLE, input_data
        )

        workflow._generate_shortcuts.assert_called_once_with(input_data, ModelTier.CAPABLE)
        assert result == {"shortcuts": []}

    @pytest.mark.asyncio
    async def test_run_validate_stage(self, workflow):
        """Given a workflow with mocked _validate_shortcuts
        When run_stage is called with 'validate'
        Then it should call _validate_shortcuts with tier.
        """
        workflow._validate_shortcuts = AsyncMock(return_value=({"valid": True}, 50, 25))
        input_data = {"shortcuts": {}}

        result, inp_tokens, out_tokens = await workflow.run_stage(
            "validate", ModelTier.CHEAP, input_data
        )

        workflow._validate_shortcuts.assert_called_once_with(input_data, ModelTier.CHEAP)
        assert result == {"valid": True}

    @pytest.mark.asyncio
    async def test_run_export_stage(self, workflow):
        """Given a workflow with mocked _export_outputs
        When run_stage is called with 'export'
        Then it should call _export_outputs.
        """
        workflow._export_outputs = AsyncMock(return_value=({"exported": True}, 0, 0))
        input_data = {"shortcuts": {}}

        result, inp_tokens, out_tokens = await workflow.run_stage(
            "export", ModelTier.CHEAP, input_data
        )

        workflow._export_outputs.assert_called_once_with(input_data)
        assert result == {"exported": True}

    @pytest.mark.asyncio
    async def test_run_stage_unknown_stage_raises_error(self, workflow):
        """Given a workflow
        When run_stage is called with unknown stage name
        Then it should raise ValueError.
        """
        with pytest.raises(ValueError, match="Unknown stage: invalid_stage"):
            await workflow.run_stage("invalid_stage", ModelTier.CHEAP, {})


# Test should_skip_stage method


class TestShouldSkipStage:
    """Tests for should_skip_stage method."""

    def test_should_skip_analyze_stage_when_frequencies_set(self, workflow, sample_manifest):
        """Given input data with features that have frequencies
        When checking if analyze stage should be skipped
        Then it should return True with reason.
        """
        input_data = {"manifest": sample_manifest.model_dump()}

        should_skip, reason = workflow.should_skip_stage("analyze", input_data)

        assert should_skip is True
        assert "already have frequencies" in reason.lower()

    def test_should_skip_analyze_stage_when_frequencies_not_set(self, workflow, sample_features):
        """Given input data with features without frequencies
        When checking if analyze stage should be skipped
        Then it should return False.
        """
        input_data = {"manifest": {"features": sample_features["features"]}}

        should_skip, reason = workflow.should_skip_stage("analyze", input_data)

        assert should_skip is False
        assert reason is None

    def test_should_skip_analyze_stage_when_no_manifest(self, workflow):
        """Given input data without manifest
        When checking if analyze stage should be skipped
        Then it should return False.
        """
        input_data = {}

        should_skip, reason = workflow.should_skip_stage("analyze", input_data)

        assert should_skip is False
        assert reason is None

    def test_should_skip_other_stages_always_false(self, workflow):
        """Given any other stage name
        When checking if stage should be skipped
        Then it should always return False.
        """
        stages = ["discover", "generate", "validate", "export"]

        for stage in stages:
            should_skip, reason = workflow.should_skip_stage(stage, {})
            assert should_skip is False
            assert reason is None


# Test _discover_features method


class TestDiscoverFeatures:
    """Tests for _discover_features method."""

    @pytest.mark.asyncio
    async def test_discover_features_from_source_dir(self, workflow, mock_parser):
        """Given a workflow with mocked parser
        When _discover_features is called with source_dir
        Then it should parse the directory and return features.
        """
        mock_parser.parse_directory.return_value = {"features": ["feature1", "feature2"]}
        input_data = {"source_dir": "/test/path"}

        result, inp_tokens, out_tokens = await workflow._discover_features(input_data)

        mock_parser.parse_directory.assert_called_once_with(Path("/test/path"))
        assert result == {"features": ["feature1", "feature2"]}
        assert inp_tokens == 0
        assert out_tokens == 0

    @pytest.mark.asyncio
    async def test_discover_features_from_manifest_file(self, workflow, sample_features):
        """Given input data with manifest_file
        When _discover_features is called
        Then it should load features from file.
        """
        input_data = {"manifest_file": "/test/manifest.json"}

        with patch("builtins.open", create=True) as mock_open:
            mock_open.return_value.__enter__.return_value.read.return_value = json.dumps(
                sample_features
            )

            result, inp_tokens, out_tokens = await workflow._discover_features(input_data)

            assert result == sample_features
            assert inp_tokens == 0
            assert out_tokens == 0

    @pytest.mark.asyncio
    async def test_discover_features_from_yaml_file(self, workflow, sample_features):
        """Given input data with yaml manifest_file
        When _discover_features is called
        Then it should load features from yaml file.
        """
        input_data = {"manifest_file": "/test/manifest.yaml"}

        with patch("builtins.open", create=True) as mock_open:
            mock_open.return_value.__enter__.return_value.read.return_value = yaml.dump(
                sample_features
            )

            result, inp_tokens, out_tokens = await workflow._discover_features(input_data)

            assert result == sample_features

    @pytest.mark.asyncio
    async def test_discover_features_missing_source_raises_error(self, workflow):
        """Given input data without source_dir or manifest_file
        When _discover_features is called
        Then it should raise ValueError.
        """
        input_data = {}

        with pytest.raises(ValueError, match="source_dir or manifest_file"):
            await workflow._discover_features(input_data)


# Test _analyze_features method


class TestAnalyzeFeatures:
    """Tests for _analyze_features method."""

    @pytest.mark.asyncio
    async def test_analyze_features_calls_llm(self, workflow, sample_features):
        """Given a workflow with input features
        When _analyze_features is called
        Then it should call LLM and return analyzed manifest.
        """
        workflow.client = AsyncMock()
        workflow.client.generate_text.return_value = json.dumps(
            {
                "features": [
                    {
                        "id": "save",
                        "name": "Save File",
                        "description": "Save current file",
                        "category": "file",
                        "frequency": "daily",
                        "mnemonic": "Save",
                    }
                ]
            }
        )
        workflow.client.count_tokens.return_value = 100
        input_data = {"features": sample_features["features"]}

        with patch(
            "empathy_os.workflows.keyboard_shortcuts.workflow.format_analyze_prompt"
        ) as mock_format:
            mock_format.return_value = "test prompt"

            result, inp_tokens, out_tokens = await workflow._analyze_features(
                input_data, ModelTier.CAPABLE
            )

            workflow.client.generate_text.assert_called_once()
            assert "features" in result
            assert inp_tokens > 0
            assert out_tokens > 0

    @pytest.mark.asyncio
    async def test_analyze_features_with_existing_manifest(self, workflow, sample_manifest):
        """Given input with existing manifest
        When _analyze_features is called
        Then it should use existing manifest features.
        """
        workflow.client = AsyncMock()
        workflow.client.generate_text.return_value = json.dumps(sample_manifest.model_dump())
        workflow.client.count_tokens.return_value = 50
        input_data = {"manifest": sample_manifest.model_dump()}

        result, inp_tokens, out_tokens = await workflow._analyze_features(
            input_data, ModelTier.CAPABLE
        )

        assert "features" in result


# Test _generate_shortcuts