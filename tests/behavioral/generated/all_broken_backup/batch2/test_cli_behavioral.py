"""Behavioral tests for cli.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import argparse
import json
import sys
from datetime import datetime, timedelta
from io import StringIO
from unittest.mock import MagicMock, Mock, call, patch

import pytest

from empathy_os.models.cli import (
    configure_cli,
    print_costs,
    print_registry,
    print_tasks,
    print_telemetry,
    validate_config,
)


@pytest.fixture
def mock_registry():
    """Fixture providing a mock model registry."""
    return {
        "openai": {
            "cheap": Mock(
                id="gpt-3.5-turbo",
                input_cost_per_million=0.50,
                output_cost_per_million=1.50,
                max_tokens=4096,
                supports_vision=False,
                supports_tools=True,
            ),
            "capable": Mock(
                id="gpt-4",
                input_cost_per_million=30.00,
                output_cost_per_million=60.00,
                max_tokens=8192,
                supports_vision=False,
                supports_tools=True,
            ),
            "premium": Mock(
                id="gpt-4-turbo",
                input_cost_per_million=10.00,
                output_cost_per_million=30.00,
                max_tokens=128000,
                supports_vision=True,
                supports_tools=True,
            ),
        },
        "anthropic": {
            "cheap": Mock(
                id="claude-3-haiku",
                input_cost_per_million=0.25,
                output_cost_per_million=1.25,
                max_tokens=4096,
                supports_vision=True,
                supports_tools=True,
            ),
            "capable": Mock(
                id="claude-3-sonnet",
                input_cost_per_million=3.00,
                output_cost_per_million=15.00,
                max_tokens=200000,
                supports_vision=True,
                supports_tools=True,
            ),
        },
    }


@pytest.fixture
def mock_tasks():
    """Fixture providing mock task definitions."""
    return {
        "chat": "cheap",
        "analysis": "capable",
        "code_generation": "capable",
        "vision_analysis": "premium",
        "complex_reasoning": "premium",
    }


@pytest.fixture
def mock_telemetry_store():
    """Fixture providing a mock telemetry store."""
    store = Mock()
    store.get_records.return_value = [
        {
            "timestamp": datetime.now().isoformat(),
            "provider": "openai",
            "tier": "capable",
            "model_id": "gpt-4",
            "input_tokens": 1000,
            "output_tokens": 500,
            "cost": 0.045,
            "latency_ms": 1200,
            "task_type": "analysis",
        },
        {
            "timestamp": (datetime.now() - timedelta(hours=1)).isoformat(),
            "provider": "anthropic",
            "tier": "cheap",
            "model_id": "claude-3-haiku",
            "input_tokens": 500,
            "output_tokens": 200,
            "cost": 0.00275,
            "latency_ms": 800,
            "task_type": "chat",
        },
    ]
    return store


@pytest.fixture
def mock_telemetry_analytics(mock_telemetry_store):
    """Fixture providing mock telemetry analytics."""
    analytics = Mock()
    analytics.store = mock_telemetry_store
    analytics.get_summary.return_value = {
        "total_requests": 2,
        "total_cost": 0.04775,
        "total_input_tokens": 1500,
        "total_output_tokens": 700,
        "avg_latency_ms": 1000,
    }
    analytics.get_cost_breakdown.return_value = {
        "openai": 0.045,
        "anthropic": 0.00275,
    }
    analytics.get_provider_stats.return_value = {
        "openai": {
            "requests": 1,
            "cost": 0.045,
            "avg_latency_ms": 1200,
        },
        "anthropic": {
            "requests": 1,
            "cost": 0.00275,
            "avg_latency_ms": 800,
        },
    }
    return analytics


class TestPrintRegistry:
    """Tests for print_registry function."""

    def test_print_registry_table_format_all_providers(
        self, mock_registry, capsys
    ):
        """
        Given: A registry with multiple providers and tiers
        When: print_registry is called with default table format
        Then: A formatted table with all providers and models is printed
        """
        with patch(
            "empathy_os.models.cli.get_all_models", return_value=mock_registry
        ):
            print_registry()

        captured = capsys.readouterr()
        assert "MODEL REGISTRY" in captured.out
        assert "[OPENAI]" in captured.out
        assert "[ANTHROPIC]" in captured.out
        assert "gpt-3.5-turbo" in captured.out
        assert "gpt-4" in captured.out
        assert "claude-3-haiku" in captured.out
        assert "$0.50" in captured.out
        assert "$30.00" in captured.out

    def test_print_registry_json_format_all_providers(
        self, mock_registry, capsys
    ):
        """
        Given: A registry with multiple providers
        When: print_registry is called with json format
        Then: JSON output with all provider data is printed
        """
        with patch(
            "empathy_os.models.cli.get_all_models", return_value=mock_registry
        ):
            print_registry(format="json")

        captured = capsys.readouterr()
        output = json.loads(captured.out)

        assert "openai" in output
        assert "anthropic" in output
        assert output["openai"]["cheap"]["id"] == "gpt-3.5-turbo"
        assert output["openai"]["cheap"]["input_cost_per_million"] == 0.50
        assert output["anthropic"]["capable"]["id"] == "claude-3-sonnet"
        assert output["anthropic"]["capable"]["supports_vision"] is True

    def test_print_registry_filter_by_provider_table(
        self, mock_registry, capsys
    ):
        """
        Given: A registry with multiple providers
        When: print_registry is called with a specific provider filter
        Then: Only the filtered provider's models are shown
        """
        with patch(
            "empathy_os.models.cli.get_all_models", return_value=mock_registry
        ):
            print_registry(provider="openai")

        captured = capsys.readouterr()
        assert "[OPENAI]" in captured.out
        assert "gpt-3.5-turbo" in captured.out
        assert "gpt-4" in captured.out
        assert "[ANTHROPIC]" not in captured.out
        assert "claude" not in captured.out

    def test_print_registry_filter_by_provider_json(
        self, mock_registry, capsys
    ):
        """
        Given: A registry with multiple providers
        When: print_registry is called with provider filter and json format
        Then: JSON output contains only the filtered provider
        """
        with patch(
            "empathy_os.models.cli.get_all_models", return_value=mock_registry
        ):
            print_registry(provider="anthropic", format="json")

        captured = capsys.readouterr()
        output = json.loads(captured.out)

        assert "anthropic" in output
        assert "openai" not in output
        assert len(output) == 1

    def test_print_registry_unknown_provider_exits(self, mock_registry, capsys):
        """
        Given: A registry with known providers
        When: print_registry is called with an unknown provider
        Then: An error message is printed and system exits
        """
        with patch(
            "empathy_os.models.cli.get_all_models", return_value=mock_registry
        ):
            with pytest.raises(SystemExit) as exc_info:
                print_registry(provider="unknown_provider")

            assert exc_info.value.code == 1

        captured = capsys.readouterr()
        assert "Error: Unknown provider 'unknown_provider'" in captured.out
        assert "Available providers:" in captured.out
        assert "openai" in captured.out
        assert "anthropic" in captured.out

    def test_print_registry_empty_registry(self, capsys):
        """
        Given: An empty model registry
        When: print_registry is called
        Then: The registry structure is shown without errors
        """
        with patch("empathy_os.models.cli.get_all_models", return_value={}):
            print_registry()

        captured = capsys.readouterr()
        assert "MODEL REGISTRY" in captured.out


class TestPrintTasks:
    """Tests for print_tasks function."""

    def test_print_tasks_table_format(self, mock_tasks, capsys):
        """
        Given: Task-to-tier mappings
        When: print_tasks is called with table format
        Then: A formatted table of tasks and tiers is printed
        """
        with patch(
            "empathy_os.models.cli.get_all_tasks", return_value=mock_tasks
        ):
            print_tasks()

        captured = capsys.readouterr()
        assert "TASK MAPPINGS" in captured.out
        assert "chat" in captured.out
        assert "cheap" in captured.out
        assert "analysis" in captured.out
        assert "capable" in captured.out
        assert "vision_analysis" in captured.out
        assert "premium" in captured.out

    def test_print_tasks_json_format(self, mock_tasks, capsys):
        """
        Given: Task-to-tier mappings
        When: print_tasks is called with json format
        Then: JSON output with all task mappings is printed
        """
        with patch(
            "empathy_os.models.cli.get_all_tasks", return_value=mock_tasks
        ):
            print_tasks(format="json")

        captured = capsys.readouterr()
        output = json.loads(captured.out)

        assert output["chat"] == "cheap"
        assert output["analysis"] == "capable"
        assert output["vision_analysis"] == "premium"

    def test_print_tasks_filter_by_tier(self, mock_tasks, capsys):
        """
        Given: Task-to-tier mappings with multiple tiers
        When: print_tasks is called with a tier filter
        Then: Only tasks matching the tier are shown
        """
        with patch(
            "empathy_os.models.cli.get_all_tasks", return_value=mock_tasks
        ):
            print_tasks(tier="capable")

        captured = capsys.readouterr()
        assert "analysis" in captured.out
        assert "code_generation" in captured.out
        assert "chat" not in captured.out
        assert "vision_analysis" not in captured.out

    def test_print_tasks_filter_by_tier_json(self, mock_tasks, capsys):
        """
        Given: Task-to-tier mappings
        When: print_tasks is called with tier filter and json format
        Then: JSON output contains only tasks for that tier
        """
        with patch(
            "empathy_os.models.cli.get_all_tasks", return_value=mock_tasks
        ):
            print_tasks(tier="premium", format="json")

        captured = capsys.readouterr()
        output = json.loads(captured.out)

        assert "vision_analysis" in output
        assert "complex_reasoning" in output
        assert "chat" not in output
        assert "analysis" not in output

    def test_print_tasks_empty_mappings(self, capsys):
        """
        Given: No task mappings
        When: print_tasks is called
        Then: The task structure is shown without errors
        """
        with patch("empathy_os.models.cli.get_all_tasks", return_value={}):
            print_tasks()

        captured = capsys.readouterr()
        assert "TASK MAPPINGS" in captured.out


class TestValidateConfig:
    """Tests for validate_config function."""

    def test_validate_config_valid_file(self, capsys, tmp_path):
        """
        Given: A valid YAML configuration file
        When: validate_config is called
        Then: A success message is printed and no errors occur
        """
        config_file = tmp_path / "config.yaml"
        config_file.write_text("providers:\n  openai:\n    api_key: test")

        with patch(
            "empathy_os.models.cli.validate_yaml_file", return_value=[]
        ):
            validate_config(str(config_file))

        captured = capsys.readouterr()
        assert "valid" in captured.out.lower()
        assert str(config_file) in captured.out

    def test_validate_config_with_errors(self, capsys, tmp_path):
        """
        Given: An invalid YAML configuration file
        When: validate_config is called
        Then: Validation errors are printed and system exits
        """
        config_file = tmp_path / "config.yaml"
        config_file.write_text("invalid: yaml: content:")

        errors = ["Invalid YAML syntax", "Missing required field 'providers'"]

        with patch(
            "empathy_os.models.cli.validate_yaml_file", return_value=errors
        ):
            with pytest.raises(SystemExit) as exc_info:
                validate_config(str(config_file))

            assert exc_info.value.code == 1

        captured = capsys.readouterr()
        assert "Validation errors" in captured.out
        assert "Invalid YAML syntax" in captured.out
        assert "Missing required field 'providers'" in captured.out

    def test_validate_config_file_not_found(self, capsys):
        """
        Given: A non-existent configuration file path
        When: validate_config is called
        Then: An error message is printed and system exits
        """
        with patch(
            "empathy_os.models.cli.validate_yaml_file",
            side_effect=FileNotFoundError("File not found"),
        ):
            with pytest.raises(SystemExit) as exc_info:
                validate_config("nonexistent.yaml")

            assert exc_info.value.code == 1

        captured = capsys.readouterr()
        assert "Error" in captured.out

    def test_validate_config_with_warnings(self, capsys, tmp_path):
        """
        Given: A valid config file with warnings
        When: validate_config is called
        Then: Warnings are shown but validation succeeds
        """
        config_file = tmp_path / "config.yaml"
        config_file.write_text("providers:\n  openai:\n    api_key: test")

        with patch(
            "empathy_os.models.cli.validate_yaml_file", return_value=[]
        ):
            validate_config(str(config_file))

        captured = capsys.readouterr()
        assert "valid" in captured.out.lower()


class TestPrintCosts:
    """Tests for print_costs function."""

    def test_print_costs_all_providers_table(self, mock_registry, capsys):
        """
        Given: A registry with multiple providers and token counts