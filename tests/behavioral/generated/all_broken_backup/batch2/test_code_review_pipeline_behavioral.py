"""Behavioral tests for code_review_pipeline.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from empathy_os.workflows.code_review_pipeline import (
    CodeReviewPipeline,
    CodeReviewPipelineResult,
)


# Fixtures


@pytest.fixture
def mock_code_review_crew():
    """Mock CodeReviewCrew for testing."""
    with patch("empathy_os.workflows.code_review_pipeline.CodeReviewCrew") as mock:
        crew_instance = MagicMock()
        crew_instance.execute = AsyncMock(
            return_value={
                "verdict": "approve_with_suggestions",
                "quality_score": 0.85,
                "findings": [
                    {"severity": "high", "message": "Security issue found"},
                    {"severity": "medium", "message": "Code smell detected"},
                ],
                "agents_used": ["security", "quality", "architecture"],
                "recommendations": ["Add input validation"],
                "blockers": [],
            }
        )
        mock.return_value = crew_instance
        yield mock


@pytest.fixture
def mock_code_review_workflow():
    """Mock CodeReviewWorkflow for testing."""
    with patch(
        "empathy_os.workflows.code_review_pipeline.CodeReviewWorkflow"
    ) as mock:
        workflow_instance = MagicMock()
        workflow_result = MagicMock()
        workflow_result.verdict = "approve_with_suggestions"
        workflow_result.quality_score = 0.80
        workflow_result.findings = [
            {"severity": "critical", "message": "Critical bug"},
            {"severity": "low", "message": "Minor issue"},
        ]
        workflow_result.recommendations = ["Fix critical bug"]
        workflow_result.blockers = ["Critical bug blocks merge"]
        workflow_instance.execute = AsyncMock(return_value=workflow_result)
        mock.return_value = workflow_instance
        yield mock


@pytest.fixture
def sample_diff():
    """Sample git diff for testing."""
    return """
diff --git a/src/main.py b/src/main.py
index 123..456 789
--- a/src/main.py
+++ b/src/main.py
@@ -1,3 +1,5 @@
+import os
+
 def main():
-    print("Hello")
+    print("Hello, World!")
"""


@pytest.fixture
def sample_files_changed():
    """Sample list of changed files."""
    return ["src/main.py", "src/utils.py", "tests/test_main.py"]


# CodeReviewPipelineResult Tests


class TestCodeReviewPipelineResult:
    """Behavioral tests for CodeReviewPipelineResult dataclass."""

    def test_given_all_fields_when_created_then_result_has_correct_values(self):
        """GIVEN all required and optional fields
        WHEN CodeReviewPipelineResult is created
        THEN all fields are properly set
        """
        # Given
        crew_report = {"verdict": "approve"}
        findings = [{"severity": "high", "message": "Issue"}]
        agents = ["security", "quality"]
        recommendations = ["Fix issue"]
        blockers = []
        metadata = {"pr_id": 123}

        # When
        result = CodeReviewPipelineResult(
            success=True,
            verdict="approve_with_suggestions",
            quality_score=0.85,
            crew_report=crew_report,
            workflow_result=None,
            combined_findings=findings,
            critical_count=0,
            high_count=1,
            medium_count=0,
            agents_used=agents,
            recommendations=recommendations,
            blockers=blockers,
            mode="full",
            duration_seconds=5.5,
            cost=0.05,
            metadata=metadata,
        )

        # Then
        assert result.success is True
        assert result.verdict == "approve_with_suggestions"
        assert result.quality_score == 0.85
        assert result.crew_report == crew_report
        assert result.combined_findings == findings
        assert result.critical_count == 0
        assert result.high_count == 1
        assert result.medium_count == 0
        assert result.agents_used == agents
        assert result.recommendations == recommendations
        assert result.blockers == blockers
        assert result.mode == "full"
        assert result.duration_seconds == 5.5
        assert result.cost == 0.05
        assert result.metadata == metadata

    def test_given_no_metadata_when_created_then_metadata_is_empty_dict(self):
        """GIVEN no metadata provided
        WHEN CodeReviewPipelineResult is created
        THEN metadata defaults to empty dict
        """
        # When
        result = CodeReviewPipelineResult(
            success=True,
            verdict="approve",
            quality_score=0.9,
            crew_report=None,
            workflow_result=None,
            combined_findings=[],
            critical_count=0,
            high_count=0,
            medium_count=0,
            agents_used=[],
            recommendations=[],
            blockers=[],
            mode="quick",
            duration_seconds=1.0,
            cost=0.01,
        )

        # Then
        assert result.metadata == {}


# CodeReviewPipeline Initialization Tests


class TestCodeReviewPipelineInit:
    """Behavioral tests for CodeReviewPipeline initialization."""

    def test_given_no_args_when_initialized_then_defaults_are_set(self):
        """GIVEN no arguments
        WHEN CodeReviewPipeline is initialized
        THEN default values are properly set
        """
        # When
        pipeline = CodeReviewPipeline()

        # Then
        assert pipeline.provider == "anthropic"
        assert pipeline.mode == "full"
        assert pipeline.parallel_crew is True
        assert pipeline.crew_enabled is True
        assert pipeline.crew_config == {"provider": "anthropic"}

    def test_given_custom_provider_when_initialized_then_provider_is_set(self):
        """GIVEN custom provider
        WHEN CodeReviewPipeline is initialized
        THEN provider is properly configured
        """
        # When
        pipeline = CodeReviewPipeline(provider="openai")

        # Then
        assert pipeline.provider == "openai"
        assert pipeline.crew_config["provider"] == "openai"

    def test_given_standard_mode_when_initialized_then_crew_disabled(self):
        """GIVEN mode is 'standard'
        WHEN CodeReviewPipeline is initialized
        THEN crew is disabled
        """
        # When
        pipeline = CodeReviewPipeline(mode="standard")

        # Then
        assert pipeline.mode == "standard"
        assert pipeline.crew_enabled is False

    def test_given_quick_mode_when_initialized_then_crew_disabled(self):
        """GIVEN mode is 'quick'
        WHEN CodeReviewPipeline is initialized
        THEN crew is disabled
        """
        # When
        pipeline = CodeReviewPipeline(mode="quick")

        # Then
        assert pipeline.mode == "quick"
        assert pipeline.crew_enabled is False

    def test_given_full_mode_when_initialized_then_crew_enabled(self):
        """GIVEN mode is 'full'
        WHEN CodeReviewPipeline is initialized
        THEN crew is enabled
        """
        # When
        pipeline = CodeReviewPipeline(mode="full")

        # Then
        assert pipeline.mode == "full"
        assert pipeline.crew_enabled is True

    def test_given_crew_config_when_initialized_then_config_merged_with_provider(self):
        """GIVEN custom crew config
        WHEN CodeReviewPipeline is initialized
        THEN config is merged with provider
        """
        # Given
        custom_config = {"timeout": 300, "retries": 3}

        # When
        pipeline = CodeReviewPipeline(provider="openai", crew_config=custom_config)

        # Then
        assert pipeline.crew_config["provider"] == "openai"
        assert pipeline.crew_config["timeout"] == 300
        assert pipeline.crew_config["retries"] == 3

    def test_given_parallel_crew_false_when_initialized_then_flag_is_set(self):
        """GIVEN parallel_crew is False
        WHEN CodeReviewPipeline is initialized
        THEN parallel_crew flag is False
        """
        # When
        pipeline = CodeReviewPipeline(parallel_crew=False)

        # Then
        assert pipeline.parallel_crew is False

    def test_given_extra_kwargs_when_initialized_then_no_error(self):
        """GIVEN extra keyword arguments
        WHEN CodeReviewPipeline is initialized
        THEN no error is raised (CLI compatibility)
        """
        # When/Then (no exception)
        pipeline = CodeReviewPipeline(extra_arg="value", another_arg=123)

        # Should still work normally
        assert pipeline.provider == "anthropic"


# Factory Methods Tests


class TestCodeReviewPipelineFactoryMethods:
    """Behavioral tests for CodeReviewPipeline factory methods."""

    def test_given_few_files_when_for_pr_review_then_returns_standard_mode(self):
        """GIVEN PR with few files changed
        WHEN for_pr_review is called
        THEN standard mode pipeline is returned
        """
        # When
        pipeline = CodeReviewPipeline.for_pr_review(files_changed=3)

        # Then
        assert isinstance(pipeline, CodeReviewPipeline)
        assert pipeline.mode == "standard"
        assert pipeline.crew_enabled is False

    def test_given_many_files_when_for_pr_review_then_returns_full_mode(self):
        """GIVEN PR with many files changed
        WHEN for_pr_review is called
        THEN full mode pipeline is returned
        """
        # When
        pipeline = CodeReviewPipeline.for_pr_review(files_changed=5)

        # Then
        assert isinstance(pipeline, CodeReviewPipeline)
        assert pipeline.mode == "full"
        assert pipeline.crew_enabled is True

    def test_given_exactly_five_files_when_for_pr_review_then_returns_full_mode(self):
        """GIVEN PR with exactly 5 files changed
        WHEN for_pr_review is called
        THEN full mode pipeline is returned (boundary test)
        """
        # When
        pipeline = CodeReviewPipeline.for_pr_review(files_changed=5)

        # Then
        assert pipeline.mode == "full"

    def test_given_zero_files_when_for_pr_review_then_returns_standard_mode(self):
        """GIVEN PR with zero files (edge case)
        WHEN for_pr_review is called
        THEN standard mode pipeline is returned
        """
        # When
        pipeline = CodeReviewPipeline.for_pr_review(files_changed=0)

        # Then
        assert pipeline.mode == "standard"

    def test_when_for_quick_check_then_returns_quick_mode(self):
        """GIVEN no arguments
        WHEN for_quick_check is called
        THEN quick mode pipeline is returned
        """
        # When
        pipeline = CodeReviewPipeline.for_quick_check()

        # Then
        assert isinstance(pipeline, CodeReviewPipeline)
        assert pipeline.mode == "quick"
        assert pipeline.crew_enabled is False

    def test_when_for_security_audit_then_returns_full_mode_with_crew(self):
        """GIVEN no arguments
        WHEN for_security_audit is called
        THEN full mode pipeline with crew is returned
        """
        # When
        pipeline = CodeReviewPipeline.for_security_audit()

        # Then
        assert isinstance(pipeline, CodeReviewPipeline)
        assert pipeline.mode == "full"
        assert pipeline.crew_enabled is True
        assert "focus" in pipeline.crew_config
        assert pipeline.crew_config["focus"] == "security"


# Execute Method Tests


class TestCodeReviewPipelineExecute:
    """Behavioral tests for CodeReviewPipeline.execute method."""

    @pytest.mark.asyncio
    async def test_given_full_mode_when_execute_then_runs_crew_and_workflow(
        self,
        mock_code_review_crew,
        mock_code_review_workflow,
        sample_diff,
        sample_files_changed,
    ):
        """GIVEN pipeline in full mode
        WHEN execute is called
        THEN both crew and workflow are run
        """
        # Given
        pipeline = CodeReviewPipeline(mode="full")

        # When
        result = await pipeline.execute(
            diff=sample_diff, files_changed=sample_files_changed
        )

        # Then
        assert result.success is True
        assert result.mode == "full"
        assert mock_code_review_crew.called
        assert mock_code_review_workflow.called
        assert result.crew_report is not None
        assert result.workflow_result is not None

    @pytest.mark.asyncio
    async def test_given_standard_mode_when_execute_then_runs_workflow_only(
        self,
        mock_code_review_crew,
        mock_code_review_workflow,
        sample_diff,
        sample_files_changed,
    ):
        """GIVEN pipeline in standard mode
        WHEN execute is called
        THEN only workflow is run
        """
        # Given
        pipeline = CodeReviewPipeline(mode="standard")

        # When
        result = await pipeline.execute(
            diff=sample_diff, files_changed=sample_files_changed
        )

        # Then
        assert result.success is True
        assert result.mode == "standard"
        assert not mock_code_review_crew.called
        assert mock_code_review_workflow.called
        assert result.crew_report is None
        assert result.workflow_result is not None

    @pytest.mark.asyncio
    async def test_given_quick_mode_when_execute_then_runs_minimal_workflow(
        self, mock_code_review_workflow, sample_diff, sample_files_changed
    ):
        """GIVEN pipeline in quick mode
        WHEN execute is called
        THEN workflow runs with minimal stages
        """
        # Given
        pipeline = CodeReviewPipeline(mode="quick")

        # When
        result = await pipeline.execute(
            diff=sample_diff, files_changed=sample_files_changed
        )

        # Then
        assert result.success is True
        assert result.mode == "quick"
        assert mock_code_review_workflow.called
        # Verify workflow was created with skip_architect=True
        call_kwargs = mock_code_review_workflow.call_args[1]
        assert call_kwargs.get("skip_architect") is True

    @pytest.mark.asyncio
    async def test_given_parallel_crew_true_when_execute_then_runs_concurrently(
        self,
        mock_code_review_crew,
        mock_code_review_workflow,
        sample_diff,
        sample_files_changed,
    ):
        """GIVEN pipeline with parallel_crew enabled
        WHEN execute is called in full mode
        THEN crew and workflow run in parallel
        """
        # Given
        pipeline = CodeReviewPipeline(mode="full", parallel_crew=True)

        # Track execution order
        execution_order = []

        async def crew_execute(*args, **kwargs):
            execution_order.append("crew_start")
            await asyncio.sleep(0.01)
            execution_order.append("crew_end")
            return {
                "verdict": "approve",
                "quality_score": 0.9,