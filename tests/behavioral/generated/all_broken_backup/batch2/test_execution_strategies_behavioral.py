"""Behavioral tests for execution_strategies.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
import json
import time
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from empathy_os.orchestration.execution_strategies import (
    AdaptiveStrategy,
    AgentResult,
    Condition,
    ConditionType,
    ConditionalStrategy,
    DebateStrategy,
    ExecutionStrategy,
    ParallelStrategy,
    RefinementStrategy,
    SequentialStrategy,
    StrategyResult,
    TeachingStrategy,
)


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def mock_agent():
    """Given a mock agent with standard interface."""
    agent = Mock()
    agent.agent_id = "test-agent-1"
    agent.execute = AsyncMock(
        return_value={
            "result": "success",
            "confidence": 0.9,
            "output": "test output",
        }
    )
    return agent


@pytest.fixture
def mock_agents():
    """Given a list of mock agents."""
    agents = []
    for i in range(3):
        agent = Mock()
        agent.agent_id = f"agent-{i}"
        agent.execute = AsyncMock(
            return_value={
                "result": f"output-{i}",
                "confidence": 0.8 + i * 0.05,
                "data": f"data-{i}",
            }
        )
        agents.append(agent)
    return agents


@pytest.fixture
def context():
    """Given a basic execution context."""
    return {
        "user_input": "test input",
        "session_id": "test-session",
        "metadata": {"source": "test"},
    }


@pytest.fixture
def agent_result():
    """Given a standard agent result."""
    return AgentResult(
        agent_id="test-agent",
        success=True,
        output={"result": "test output"},
        confidence=0.85,
        duration_seconds=0.5,
    )


@pytest.fixture
def strategy_result():
    """Given a standard strategy result."""
    return StrategyResult(
        success=True,
        outputs=[
            AgentResult(
                agent_id="agent-1",
                success=True,
                output={"data": "output1"},
                confidence=0.9,
            )
        ],
        aggregated_output={"final": "result"},
        total_duration=1.5,
    )


# =============================================================================
# AgentResult Tests
# =============================================================================


class TestAgentResult:
    """Behavioral tests for AgentResult dataclass."""

    def test_agent_result_creation_with_all_fields(self):
        """
        Given all required and optional fields
        When creating an AgentResult
        Then all fields should be properly initialized
        """
        result = AgentResult(
            agent_id="agent-1",
            success=True,
            output={"key": "value"},
            confidence=0.95,
            duration_seconds=2.5,
            error="",
        )

        assert result.agent_id == "agent-1"
        assert result.success is True
        assert result.output == {"key": "value"}
        assert result.confidence == 0.95
        assert result.duration_seconds == 2.5
        assert result.error == ""

    def test_agent_result_with_defaults(self):
        """
        Given only required fields
        When creating an AgentResult
        Then default values should be applied
        """
        result = AgentResult(
            agent_id="agent-2", success=False, output={"error": "failed"}
        )

        assert result.confidence == 0.0
        assert result.duration_seconds == 0.0
        assert result.error == ""

    def test_agent_result_with_error(self):
        """
        Given a failed execution
        When creating an AgentResult with error
        Then error message should be stored
        """
        result = AgentResult(
            agent_id="agent-3",
            success=False,
            output={},
            error="Timeout exceeded",
        )

        assert result.success is False
        assert result.error == "Timeout exceeded"


# =============================================================================
# StrategyResult Tests
# =============================================================================


class TestStrategyResult:
    """Behavioral tests for StrategyResult dataclass."""

    def test_strategy_result_creation(self):
        """
        Given valid strategy execution data
        When creating a StrategyResult
        Then all fields should be properly initialized
        """
        outputs = [
            AgentResult(
                agent_id="agent-1", success=True, output={"data": "test"}
            )
        ]
        result = StrategyResult(
            success=True,
            outputs=outputs,
            aggregated_output={"final": "result"},
            total_duration=3.0,
            errors=["warning1"],
        )

        assert result.success is True
        assert len(result.outputs) == 1
        assert result.aggregated_output == {"final": "result"}
        assert result.total_duration == 3.0
        assert result.errors == ["warning1"]

    def test_strategy_result_post_init_empty_errors(self):
        """
        Given no errors list provided
        When creating a StrategyResult
        Then errors should initialize to empty list
        """
        result = StrategyResult(
            success=True, outputs=[], aggregated_output={}
        )

        assert result.errors == []

    def test_strategy_result_with_multiple_outputs(self):
        """
        Given multiple agent outputs
        When creating a StrategyResult
        Then all outputs should be stored
        """
        outputs = [
            AgentResult(
                agent_id=f"agent-{i}", success=True, output={"index": i}
            )
            for i in range(5)
        ]
        result = StrategyResult(
            success=True, outputs=outputs, aggregated_output={}
        )

        assert len(result.outputs) == 5
        assert result.outputs[2].agent_id == "agent-2"


# =============================================================================
# Condition Tests
# =============================================================================


class TestCondition:
    """Behavioral tests for Condition class."""

    def test_condition_creation_json_predicate(self):
        """
        Given a JSON predicate
        When creating a Condition
        Then it should initialize with JSON_PREDICATE type
        """
        condition = Condition(
            predicate={"confidence": {"$gt": 0.8}},
            condition_type=ConditionType.JSON_PREDICATE,
        )

        assert condition.condition_type == ConditionType.JSON_PREDICATE
        assert condition.predicate == {"confidence": {"$gt": 0.8}}

    def test_condition_creation_natural_language(self):
        """
        Given a natural language condition
        When creating a Condition
        Then it should initialize with NATURAL_LANGUAGE type
        """
        condition = Condition(
            predicate="if confidence is low",
            condition_type=ConditionType.NATURAL_LANGUAGE,
        )

        assert condition.condition_type == ConditionType.NATURAL_LANGUAGE
        assert condition.predicate == "if confidence is low"

    @pytest.mark.asyncio
    async def test_condition_evaluate_simple_equality(self):
        """
        Given a simple equality predicate
        When evaluating against matching context
        Then it should return True
        """
        condition = Condition(predicate={"status": "active"})
        context = {"status": "active", "other": "data"}

        result = await condition.evaluate(context)

        assert result is True

    @pytest.mark.asyncio
    async def test_condition_evaluate_simple_inequality(self):
        """
        Given a simple equality predicate
        When evaluating against non-matching context
        Then it should return False
        """
        condition = Condition(predicate={"status": "active"})
        context = {"status": "inactive"}

        result = await condition.evaluate(context)

        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_gt_operator(self):
        """
        Given a greater-than predicate
        When evaluating against context
        Then it should compare correctly
        """
        condition = Condition(predicate={"confidence": {"$gt": 0.7}})

        # Test greater than
        result = await condition.evaluate({"confidence": 0.8})
        assert result is True

        # Test not greater than
        result = await condition.evaluate({"confidence": 0.6})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_lt_operator(self):
        """
        Given a less-than predicate
        When evaluating against context
        Then it should compare correctly
        """
        condition = Condition(predicate={"score": {"$lt": 50}})

        result = await condition.evaluate({"score": 30})
        assert result is True

        result = await condition.evaluate({"score": 60})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_gte_operator(self):
        """
        Given a greater-than-or-equal predicate
        When evaluating against context
        Then it should compare correctly including equality
        """
        condition = Condition(predicate={"value": {"$gte": 10}})

        result = await condition.evaluate({"value": 10})
        assert result is True

        result = await condition.evaluate({"value": 15})
        assert result is True

        result = await condition.evaluate({"value": 5})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_lte_operator(self):
        """
        Given a less-than-or-equal predicate
        When evaluating against context
        Then it should compare correctly including equality
        """
        condition = Condition(predicate={"value": {"$lte": 100}})

        result = await condition.evaluate({"value": 100})
        assert result is True

        result = await condition.evaluate({"value": 50})
        assert result is True

        result = await condition.evaluate({"value": 150})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_eq_operator(self):
        """
        Given an explicit equality predicate
        When evaluating against context
        Then it should check for equality
        """
        condition = Condition(predicate={"type": {"$eq": "premium"}})

        result = await condition.evaluate({"type": "premium"})
        assert result is True

        result = await condition.evaluate({"type": "basic"})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_ne_operator(self):
        """
        Given a not-equal predicate
        When evaluating against context
        Then it should check for inequality
        """
        condition = Condition(predicate={"status": {"$ne": "error"}})

        result = await condition.evaluate({"status": "success"})
        assert result is True

        result = await condition.evaluate({"status": "error"})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_missing_field(self):
        """
        Given a predicate referencing a field
        When the field is missing from context
        Then it should return False
        """
        condition = Condition(predicate={"missing_field": "value"})

        result = await condition.evaluate({"other_field": "data"})

        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_nested_field(self):
        """
        Given a nested field predicate
        When evaluating against nested context
        Then it should access nested values
        """
        condition = Condition(predicate={"user.role": "admin"})

        result = await condition.evaluate({"user": {"role": "admin"}})
        assert result is True

        result = await condition.evaluate({"user": {"role": "user"}})
        assert result is False

    @pytest.mark.asyncio
    async def test_condition_evaluate_invalid_operator(self):
        """
        Given an invalid operator in predicate
        When evaluating the condition
        Then it should return False safely
        """
        condition = Condition(
            predicate={"field": {"$invalid_op": "value"}}
        )

        result = await condition.evaluate({"field": "value"})

        assert result is False


# =============================================================================
# SequentialStrategy Tests
# =============================================================================


class TestSequentialStrategy:
    """Behavioral tests for SequentialStrategy (Pattern 1: A → B → C)."""

    @pytest.mark.asyncio
    async def test_sequential_execution_success(self, mock_agents, context):
        """
        Given multiple agents in sequence
        When executing the strategy
        Then agents should execute in order with passed context
        """
        strategy = SequentialStrategy()

        result = await strategy.execute(mock_agents, context)

        assert result.success is True
        assert len(result.outputs) == 3
        for i, agent in enumerate(mock_agents):
            agent.execute.assert_called_once()

    @pytest.mark.asyncio
    async def test_sequential_execution_context_chaining(
        self, mock_agents, context
    ):
        """
        Given multiple agents
        When executing sequentially
        Then each agent should receive updated context from previous
        """
        strategy = SequentialStrategy()

        await strategy.execute(mock_agents, context)

        # First agent gets original context
        first_call_context = mock_agents[0].execute.call_args[0][0]
        assert "user_input" in first_call_context

        # Subsequent agents should get enhanced context
        for agent in mock_agents[1:]:
            call_context = agent.execute.call_args[0][0]
            assert call_context is not None

    @pytest.mark.asyncio
    async def test_sequential_execution_single_agent(
        self, mock_agent, context
    ):
        """
        Given a single agent
        When executing sequentially
        Then it should execute successfully
        """
        strategy = SequentialStrategy()

        result = await strategy.execute([mock_agent], context)

        assert result.success is True
        assert len(result.outputs) == 1
        mock_agent.execute.assert_called_once()

    @pytest.mark.asyncio
    async def test_sequential_execution_empty_agents(self, context):
        """
        Given an empty agent list
        When executing the strategy
        Then it should return empty result
        """
        strategy = SequentialStrategy()

        result = await strategy.execute([], context)

        assert result.success is True
        assert len(result.outputs) == 0
        assert result.aggregated_output == {}

    @pytest.mark.asyncio
    async def test_sequential_execution_agent_failure(self, context):
        """
        Given agents where one fails
        When executing the strategy
        Then it should handle the failure gracefully
        """
        agents = []
        for i in range(3):
            agent = Mock()
            agent.agent_id = f"agent-{i}"
            if i == 1:
                agent.execute = AsyncMock(
                    side_effect=Exception("Agent failure")
                )
            else:
                agent.execute = AsyncMock(return_value={"result": f"ok-{i}"})
            agents.append(agent)

        strategy = SequentialStrategy()

        result = await strategy.execute(agents, context)

        assert len(result.outputs) >= 1
        # Check that error was recorded
        failed_output = next(
            (o for o in result.outputs if not o.success), None
        )
        assert failed_output is not None
        assert "Agent failure" in failed_output.error