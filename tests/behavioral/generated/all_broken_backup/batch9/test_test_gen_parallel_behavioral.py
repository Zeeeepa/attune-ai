"""Behavioral tests for test_gen_parallel.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
import ast
import json
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch, mock_open

import pytest

from empathy_os.workflows.test_gen_parallel import (
    ParallelTestGenerationWorkflow,
    TestGenerationTask,
)
from empathy_os.workflows.base import ModelTier, WorkflowResult, WorkflowStage


class TestTestGenerationTask:
    """Tests for TestGenerationTask dataclass."""

    def test_task_creation_with_defaults(self):
        """Given: Task parameters without status
        When: Creating a TestGenerationTask
        Then: Task is created with 'pending' status
        """
        task = TestGenerationTask(
            module_path="src/module.py",
            coverage=75.5,
            output_path="tests/test_module.py"
        )

        assert task.module_path == "src/module.py"
        assert task.coverage == 75.5
        assert task.output_path == "tests/test_module.py"
        assert task.status == "pending"

    def test_task_creation_with_custom_status(self):
        """Given: Task parameters with custom status
        When: Creating a TestGenerationTask
        Then: Task is created with specified status
        """
        task = TestGenerationTask(
            module_path="src/module.py",
            coverage=85.0,
            output_path="tests/test_module.py",
            status="completed"
        )

        assert task.status == "completed"

    def test_task_status_transitions(self):
        """Given: A pending task
        When: Status is updated through workflow stages
        Then: Status reflects current stage
        """
        task = TestGenerationTask(
            module_path="src/module.py",
            coverage=60.0,
            output_path="tests/test_module.py"
        )

        assert task.status == "pending"
        task.status = "generated"
        assert task.status == "generated"
        task.status = "completed"
        assert task.status == "completed"
        task.status = "validated"
        assert task.status == "validated"


class TestParallelTestGenerationWorkflowInitialization:
    """Tests for ParallelTestGenerationWorkflow initialization."""

    def test_workflow_initialization(self):
        """Given: No parameters
        When: Creating a ParallelTestGenerationWorkflow
        Then: Workflow is initialized with correct configuration
        """
        workflow = ParallelTestGenerationWorkflow()

        assert workflow.name == "parallel-test-generation"
        assert workflow.description == "Generate behavioral tests in parallel with AI completion"
        assert "discover" in workflow.stages
        assert "generate_templates" in workflow.stages
        assert "complete_tests" in workflow.stages
        assert "validate" in workflow.stages

    def test_discover_stage_configuration(self):
        """Given: Initialized workflow
        When: Checking discover stage
        Then: Stage has correct configuration
        """
        workflow = ParallelTestGenerationWorkflow()
        discover_stage = workflow.stages["discover"]

        assert discover_stage.name == "discover"
        assert discover_stage.tier_hint == ModelTier.CHEAP
        assert discover_stage.task_type == "analysis"
        assert "coverage" in discover_stage.system_prompt.lower()

    def test_generate_templates_stage_configuration(self):
        """Given: Initialized workflow
        When: Checking generate_templates stage
        Then: Stage uses cheap tier for speed
        """
        workflow = ParallelTestGenerationWorkflow()
        stage = workflow.stages["generate_templates"]

        assert stage.name == "generate_templates"
        assert stage.tier_hint == ModelTier.CHEAP
        assert stage.task_type == "code_generation"

    def test_complete_tests_stage_configuration(self):
        """Given: Initialized workflow
        When: Checking complete_tests stage
        Then: Stage uses capable tier for quality
        """
        workflow = ParallelTestGenerationWorkflow()
        stage = workflow.stages["complete_tests"]

        assert stage.name == "complete_tests"
        assert stage.tier_hint == ModelTier.CAPABLE
        assert stage.task_type == "code_generation"
        assert "TODO" in stage.system_prompt
        assert "assertions" in stage.system_prompt.lower()

    def test_validate_stage_configuration(self):
        """Given: Initialized workflow
        When: Checking validate stage
        Then: Stage is configured for validation
        """
        workflow = ParallelTestGenerationWorkflow()
        stage = workflow.stages["validate"]

        assert stage.name == "validate"
        assert stage.tier_hint == ModelTier.CHEAP


class TestParallelTestGenerationWorkflowExecution:
    """Tests for workflow execution methods."""

    @pytest.fixture
    def workflow(self):
        """Provide a workflow instance."""
        return ParallelTestGenerationWorkflow()

    @pytest.fixture
    def mock_coverage_data(self):
        """Provide mock coverage data."""
        return {
            "src/empathy_os/module1.py": 45.5,
            "src/empathy_os/module2.py": 60.0,
            "src/empathy_os/module3.py": 30.2,
            "src/empathy_os/module4.py": 75.0,
        }

    @pytest.fixture
    def sample_tasks(self):
        """Provide sample test generation tasks."""
        return [
            TestGenerationTask(
                module_path="src/empathy_os/module1.py",
                coverage=45.5,
                output_path="tests/test_module1.py"
            ),
            TestGenerationTask(
                module_path="src/empathy_os/module2.py",
                coverage=60.0,
                output_path="tests/test_module2.py"
            ),
        ]

    @patch('empathy_os.workflows.test_gen_parallel.Path')
    def test_discover_modules_with_low_coverage(self, mock_path, workflow, mock_coverage_data):
        """Given: Project with varying coverage
        When: Discovering modules needing tests
        Then: Returns modules below coverage threshold
        """
        mock_path.return_value.exists.return_value = True
        
        with patch.object(workflow, '_read_coverage_data', return_value=mock_coverage_data):
            result = workflow._discover_modules(threshold=70.0, top_n=10)
            
            assert len(result) > 0
            for task in result:
                assert isinstance(task, TestGenerationTask)
                assert task.coverage < 70.0
                assert task.status == "pending"

    @patch('empathy_os.workflows.test_gen_parallel.Path')
    def test_discover_modules_respects_top_n(self, mock_path, workflow):
        """Given: More modules than top_n limit
        When: Discovering modules
        Then: Returns at most top_n modules
        """
        coverage_data = {f"module{i}.py": float(i) for i in range(100)}
        
        with patch.object(workflow, '_read_coverage_data', return_value=coverage_data):
            result = workflow._discover_modules(threshold=90.0, top_n=5)
            
            assert len(result) <= 5

    @patch('empathy_os.workflows.test_gen_parallel.Path')
    def test_discover_modules_sorts_by_priority(self, mock_path, workflow, mock_coverage_data):
        """Given: Modules with different coverage levels
        When: Discovering modules
        Then: Returns modules sorted by lowest coverage first
        """
        with patch.object(workflow, '_read_coverage_data', return_value=mock_coverage_data):
            result = workflow._discover_modules(threshold=80.0, top_n=10)
            
            if len(result) > 1:
                for i in range(len(result) - 1):
                    assert result[i].coverage <= result[i + 1].coverage

    @pytest.mark.asyncio
    async def test_generate_template_for_module(self, workflow):
        """Given: A module path
        When: Generating test template
        Then: Returns template with TODO markers
        """
        mock_llm_response = '''
def test_example():
    # TODO: Add test implementation
    pass
'''
        
        with patch.object(workflow, '_call_llm', return_value=mock_llm_response):
            task = TestGenerationTask(
                module_path="src/module.py",
                coverage=50.0,
                output_path="tests/test_module.py"
            )
            
            template = await workflow._generate_template(task)
            
            assert "TODO" in template
            assert "test_" in template

    @pytest.mark.asyncio
    async def test_generate_templates_in_parallel(self, workflow, sample_tasks):
        """Given: Multiple tasks
        When: Generating templates in parallel
        Then: All templates are generated concurrently
        """
        mock_template = "def test_example():\n    # TODO: implement\n    pass"
        
        with patch.object(workflow, '_generate_template', return_value=mock_template) as mock_gen:
            results = await workflow._generate_templates_parallel(sample_tasks, max_workers=2)
            
            assert len(results) == len(sample_tasks)
            assert mock_gen.call_count == len(sample_tasks)

    @pytest.mark.asyncio
    async def test_complete_test_implementation(self, workflow):
        """Given: A test template with TODOs
        When: Completing test implementation
        Then: Returns fully implemented test
        """
        template = '''
def test_example():
    # TODO: Add assertions
    pass
'''
        
        completed_test = '''
def test_example():
    result = some_function()
    assert result == expected_value
'''
        
        with patch.object(workflow, '_call_llm', return_value=completed_test):
            task = TestGenerationTask(
                module_path="src/module.py",
                coverage=50.0,
                output_path="tests/test_module.py"
            )
            
            result = await workflow._complete_test(task, template)
            
            assert "assert" in result
            assert "TODO" not in result

    @pytest.mark.asyncio
    async def test_complete_tests_in_parallel(self, workflow, sample_tasks):
        """Given: Multiple test templates
        When: Completing tests in parallel
        Then: All tests are completed concurrently
        """
        templates = {task.module_path: "# template" for task in sample_tasks}
        completed = "# completed test"
        
        with patch.object(workflow, '_complete_test', return_value=completed) as mock_complete:
            results = await workflow._complete_tests_parallel(sample_tasks, templates, max_workers=2)
            
            assert len(results) == len(sample_tasks)
            assert mock_complete.call_count == len(sample_tasks)

    @pytest.mark.asyncio
    async def test_validate_generated_test_success(self, workflow):
        """Given: Valid test code
        When: Validating the test
        Then: Returns True for valid syntax
        """
        valid_test = '''
import pytest

def test_valid():
    assert True
'''
        
        result = await workflow._validate_test(valid_test)
        
        assert result is True

    @pytest.mark.asyncio
    async def test_validate_generated_test_syntax_error(self, workflow):
        """Given: Test code with syntax error
        When: Validating the test
        Then: Returns False for invalid syntax
        """
        invalid_test = '''
def test_invalid()
    assert True  # Missing colon
'''
        
        result = await workflow._validate_test(invalid_test)
        
        assert result is False

    @pytest.mark.asyncio
    async def test_validate_generated_test_missing_imports(self, workflow):
        """Given: Test code without pytest import
        When: Validating the test
        Then: Validation detects missing imports
        """
        test_without_imports = '''
def test_something():
    assert True
'''
        
        # Should still parse but might flag warnings
        result = await workflow._validate_test(test_without_imports)
        
        # Basic syntax is valid even without imports
        assert isinstance(result, bool)

    @patch('builtins.open', new_callable=mock_open)
    @patch('empathy_os.workflows.test_gen_parallel.Path')
    def test_write_test_file(self, mock_path, mock_file, workflow):
        """Given: Generated test content
        When: Writing to file
        Then: Test file is created with correct content
        """
        test_content = "def test_example():\n    assert True"
        output_path = Path("tests/test_module.py")
        
        mock_path.return_value.parent.mkdir = Mock()
        
        workflow._write_test_file(output_path, test_content)
        
        mock_file.assert_called_once()
        mock_file().write.assert_called_once_with(test_content)

    @patch('builtins.open', new_callable=mock_open)
    @patch('empathy_os.workflows.test_gen_parallel.Path')
    def test_write_test_file_creates_directory(self, mock_path, mock_file, workflow):
        """Given: Output path in non-existent directory
        When: Writing test file
        Then: Directory is created before writing
        """
        test_content = "# test"
        output_path = Path("tests/nested/test_module.py")
        
        mock_parent = Mock()
        mock_path.return_value.parent = mock_parent
        
        workflow._write_test_file(output_path, test_content)
        
        mock_parent.mkdir.assert_called_once_with(parents=True, exist_ok=True)

    @pytest.mark.asyncio
    async def test_workflow_execution_end_to_end(self, workflow):
        """Given: Workflow with all stages
        When: Executing complete workflow
        Then: All stages execute successfully
        """
        mock_tasks = [
            TestGenerationTask(
                module_path="src/module.py",
                coverage=50.0,
                output_path="tests/test_module.py"
            )
        ]
        
        with patch.object(workflow, '_discover_modules', return_value=mock_tasks), \
             patch.object(workflow, '_generate_templates_parallel', return_value={"src/module.py": "# template"}), \
             patch.object(workflow, '_complete_tests_parallel', return_value={"src/module.py": "# complete"}), \
             patch.object(workflow, '_validate_test', return_value=True), \
             patch.object(workflow, '_write_test_file'):
            
            result = await workflow.execute(top=1, parallel=1, threshold=70.0)
            
            assert isinstance(result, (WorkflowResult, dict))

    def test_workflow_handles_empty_module_list(self, workflow):
        """Given: No modules needing tests
        When: Discovering modules
        Then: Returns empty list gracefully
        """
        with patch.object(workflow, '_read_coverage_data', return_value={}):
            result = workflow._discover_modules(threshold=50.0, top_n=10)
            
            assert result == []

    @pytest.mark.asyncio
    async def test_parallel_execution_with_failures(self, workflow, sample_tasks):
        """Given: Some tasks that fail during generation
        When: Executing parallel generation