"""Behavioral tests for empathy_executor.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import logging
import uuid
from datetime import datetime
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from empathy_os.models.empathy_executor import EmpathyLLMExecutor
from empathy_os.models.executor import ExecutionContext, LLMResponse
from empathy_os.models.telemetry import LLMCallRecord


@pytest.fixture
def mock_empathy_llm():
    """Given a mock EmpathyLLM instance."""
    llm = Mock()
    llm.generate = AsyncMock(return_value="Generated response")
    llm.model = "claude-3-5-sonnet-20241022"
    llm.provider = "anthropic"
    return llm


@pytest.fixture
def mock_telemetry_store():
    """Given a mock telemetry store."""
    store = Mock()
    store.record_call = Mock()
    return store


@pytest.fixture
def mock_workflow_config():
    """Given a mock workflow config."""
    config = Mock()
    config.custom_models = {
        "hybrid": {
            "tier1": "claude-3-5-sonnet-20241022",
            "tier2": "gpt-4",
            "tier3": "gemini-pro",
        }
    }
    return config


class TestEmpathyLLMExecutorInitialization:
    """Test suite for EmpathyLLMExecutor initialization."""

    def test_init_with_defaults(self):
        """Given default parameters
        When initializing executor
        Then it should set default values correctly.
        """
        # When
        executor = EmpathyLLMExecutor()

        # Then
        assert executor._provider == "anthropic"
        assert executor._api_key is None
        assert executor._llm is None
        assert executor._telemetry is None
        assert executor._hybrid_llms == {}
        assert executor._hybrid_config is None

    def test_init_with_custom_provider(self):
        """Given custom provider
        When initializing executor
        Then it should use custom provider.
        """
        # When
        executor = EmpathyLLMExecutor(provider="openai", api_key="test-key")

        # Then
        assert executor._provider == "openai"
        assert executor._api_key == "test-key"

    def test_init_with_empathy_llm(self, mock_empathy_llm):
        """Given pre-configured EmpathyLLM
        When initializing executor
        Then it should use provided instance.
        """
        # When
        executor = EmpathyLLMExecutor(empathy_llm=mock_empathy_llm)

        # Then
        assert executor._llm == mock_empathy_llm

    def test_init_with_telemetry_store(self, mock_telemetry_store):
        """Given telemetry store
        When initializing executor
        Then it should store telemetry reference.
        """
        # When
        executor = EmpathyLLMExecutor(telemetry_store=mock_telemetry_store)

        # Then
        assert executor._telemetry == mock_telemetry_store

    def test_init_with_llm_kwargs(self):
        """Given additional LLM kwargs
        When initializing executor
        Then it should store kwargs for later use.
        """
        # When
        executor = EmpathyLLMExecutor(temperature=0.7, max_tokens=1000)

        # Then
        assert executor._llm_kwargs == {"temperature": 0.7, "max_tokens": 1000}

    @patch("empathy_os.models.empathy_executor.WorkflowConfig")
    def test_init_hybrid_mode_loads_config(self, mock_config_class, mock_workflow_config):
        """Given hybrid provider
        When initializing executor
        Then it should load hybrid configuration.
        """
        # Given
        mock_config_class.load.return_value = mock_workflow_config

        # When
        executor = EmpathyLLMExecutor(provider="hybrid")

        # Then
        assert executor._provider == "hybrid"
        assert executor._hybrid_config == mock_workflow_config.custom_models["hybrid"]
        mock_config_class.load.assert_called_once()

    @patch("empathy_os.models.empathy_executor.WorkflowConfig")
    def test_init_hybrid_mode_handles_config_error(self, mock_config_class):
        """Given hybrid provider with config error
        When initializing executor
        Then it should handle error gracefully.
        """
        # Given
        mock_config_class.load.side_effect = Exception("Config load error")

        # When
        executor = EmpathyLLMExecutor(provider="hybrid")

        # Then
        assert executor._provider == "hybrid"
        assert executor._hybrid_config is None

    @patch("empathy_os.models.empathy_executor.WorkflowConfig")
    def test_init_hybrid_mode_no_hybrid_config(self, mock_config_class):
        """Given hybrid provider but no hybrid config
        When initializing executor
        Then it should handle missing config.
        """
        # Given
        config = Mock()
        config.custom_models = {}
        mock_config_class.load.return_value = config

        # When
        executor = EmpathyLLMExecutor(provider="hybrid")

        # Then
        assert executor._hybrid_config is None


class TestGetProviderForModel:
    """Test suite for _get_provider_for_model method."""

    def test_get_provider_for_anthropic_model(self):
        """Given Anthropic model ID
        When getting provider
        Then it should return anthropic.
        """
        # Given
        executor = EmpathyLLMExecutor()

        # When
        provider = executor._get_provider_for_model("claude-3-5-sonnet-20241022")

        # Then
        assert provider == "anthropic"

    def test_get_provider_for_openai_model(self):
        """Given OpenAI model ID
        When getting provider
        Then it should return openai.
        """
        # Given
        executor = EmpathyLLMExecutor()

        # When
        provider = executor._get_provider_for_model("gpt-4")

        # Then
        assert provider == "openai"

    def test_get_provider_for_google_model(self):
        """Given Google model ID
        When getting provider
        Then it should return google.
        """
        # Given
        executor = EmpathyLLMExecutor()

        # When
        provider = executor._get_provider_for_model("gemini-pro")

        # Then
        assert provider == "google"

    def test_get_provider_for_ollama_model(self):
        """Given Ollama model ID
        When getting provider
        Then it should return ollama.
        """
        # Given
        executor = EmpathyLLMExecutor()

        # When
        provider = executor._get_provider_for_model("llama2")

        # Then
        assert provider == "ollama"

    def test_get_provider_for_unknown_model(self):
        """Given unknown model ID
        When getting provider
        Then it should return anthropic as default.
        """
        # Given
        executor = EmpathyLLMExecutor()

        # When
        provider = executor._get_provider_for_model("unknown-model")

        # Then
        assert provider == "anthropic"


class TestGetLLMForHybridMode:
    """Test suite for _get_llm_for_hybrid_mode method."""

    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_creates_new_instance(self, mock_get_model):
        """Given hybrid mode with uncached provider
        When getting LLM
        Then it should create new instance.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="hybrid")
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm

        # When
        result = executor._get_llm_for_hybrid_mode("anthropic", "claude-3-5-sonnet-20241022")

        # Then
        assert result == mock_llm
        mock_get_model.assert_called_once_with(
            "claude-3-5-sonnet-20241022",
            provider="anthropic",
            api_key=None
        )

    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_caches_instance(self, mock_get_model):
        """Given hybrid mode with cached provider
        When getting LLM again
        Then it should return cached instance.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="hybrid")
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm

        # When
        result1 = executor._get_llm_for_hybrid_mode("anthropic", "claude-3-5-sonnet-20241022")
        result2 = executor._get_llm_for_hybrid_mode("anthropic", "claude-3-5-sonnet-20241022")

        # Then
        assert result1 == result2
        mock_get_model.assert_called_once()

    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_uses_api_key(self, mock_get_model):
        """Given hybrid mode with API key
        When getting LLM
        Then it should pass API key.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="hybrid", api_key="test-key")
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm

        # When
        executor._get_llm_for_hybrid_mode("anthropic", "claude-3-5-sonnet-20241022")

        # Then
        mock_get_model.assert_called_once_with(
            "claude-3-5-sonnet-20241022",
            provider="anthropic",
            api_key="test-key"
        )


class TestGetLLM:
    """Test suite for _get_llm method."""

    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_returns_existing_instance(self, mock_get_model):
        """Given executor with existing LLM
        When getting LLM
        Then it should return existing instance.
        """
        # Given
        mock_llm = Mock()
        executor = EmpathyLLMExecutor(empathy_llm=mock_llm)

        # When
        result = executor._get_llm("tier1")

        # Then
        assert result == mock_llm
        mock_get_model.assert_not_called()

    @patch("empathy_os.models.empathy_executor.get_tier_for_task")
    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_creates_new_instance_normal_mode(
        self, mock_get_model, mock_get_tier
    ):
        """Given executor without LLM in normal mode
        When getting LLM
        Then it should create new instance.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="anthropic")
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm
        mock_get_tier.return_value = "tier1"

        # When
        result = executor._get_llm("tier1")

        # Then
        assert result == mock_llm
        mock_get_model.assert_called_once()

    @patch("empathy_os.models.empathy_executor.get_tier_for_task")
    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_hybrid_mode_with_config(
        self, mock_get_model, mock_get_tier
    ):
        """Given executor in hybrid mode with config
        When getting LLM
        Then it should use tier-specific model.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="hybrid")
        executor._hybrid_config = {"tier1": "claude-3-5-sonnet-20241022"}
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm
        mock_get_tier.return_value = "tier1"

        # When
        result = executor._get_llm("tier1")

        # Then
        assert result == mock_llm

    @patch("empathy_os.models.empathy_executor.get_tier_for_task")
    @patch("empathy_os.models.empathy_executor.get_model")
    def test_get_llm_hybrid_mode_without_config(
        self, mock_get_model, mock_get_tier
    ):
        """Given executor in hybrid mode without config
        When getting LLM
        Then it should fall back to default model.
        """
        # Given
        executor = EmpathyLLMExecutor(provider="hybrid")
        mock_llm = Mock()
        mock_get_model.return_value = mock_llm
        mock_get_tier.return_value = "tier1"

        # When
        result = executor._get_llm("tier1")

        # Then
        assert result == mock_llm
        mock_get_model.assert_called_once()


class TestRun:
    """Test suite for run method."""

    @pytest.mark.asyncio
    @patch("empathy_os.models.empathy_executor.get_tier_for_task")
    @patch("empathy_os.models.empathy_executor.time.time")
    async def test_run_successful_execution(
        self, mock_time, mock_get_tier, mock_empathy_llm
    ):
        """Given valid task and prompt
        When running executor
        Then it should return successful response.
        """
        # Given
        executor = EmpathyLLMExecutor(empathy_llm=mock_empathy_llm)
        mock_get_tier.return_value = "tier1"
        mock_time.side_effect = [1000.0, 1002.0]  # 2 second duration
        mock_empathy_llm.model = "claude-3-5-sonnet-20241022"
        mock_empathy_llm.provider = "anthropic"

        # When
        response = await executor.run(
            task_type="summarize",
            prompt="Test prompt"
        )

        # Then
        assert isinstance(response, LLMResponse)
        assert response.output == "Generated response"
        assert response.model_used == "claude-3-5-sonnet-20241022"
        assert response.success is True
        assert response.duration_seconds == 2.0
        mock_empathy_llm.generate.assert_called_once_with(
            "Test prompt",
            context=None,
            **{}
        )

    @pytest.mark.asyncio
    @patch("empathy_os.models.empathy_executor.get_tier_for_task")
    async def test_run_with_context(self, mock_get_tier, mock_empathy_llm):
        """Given task with execution context
        When running executor
        Then it should pass context to LLM.
        """
        # Given
        executor = EmpathyLLMExecutor(empathy_llm=mock_empathy_llm)
        mock_get_tier.return