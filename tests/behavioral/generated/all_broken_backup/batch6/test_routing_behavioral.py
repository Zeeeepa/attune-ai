"""Behavioral tests for routing.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import logging
from typing import Any
from unittest.mock import MagicMock, patch

import pytest

from empathy_os.cli.commands.routing import cmd_routing_stats


@pytest.fixture
def mock_args_basic():
    """Create basic mock arguments for routing commands."""
    args = MagicMock()
    args.workflow = "test_workflow"
    args.stage = None
    args.days = 7
    return args


@pytest.fixture
def mock_args_with_stage():
    """Create mock arguments with stage specified."""
    args = MagicMock()
    args.workflow = "test_workflow"
    args.stage = "preprocessing"
    args.days = 14
    return args


@pytest.fixture
def mock_routing_stats_success():
    """Create mock routing statistics for successful scenarios."""
    return {
        "workflow": "test_workflow",
        "stage": "all",
        "days_analyzed": 7,
        "total_calls": 1000,
        "avg_cost": 0.0025,
        "avg_success_rate": 0.95,
        "models_used": ["gpt-3.5-turbo", "gpt-4"],
        "performance_by_model": {
            "gpt-3.5-turbo": {
                "calls": 700,
                "success_rate": 0.92,
                "avg_cost": 0.0015,
                "avg_latency_ms": 350.5,
            },
            "gpt-4": {
                "calls": 300,
                "success_rate": 0.98,
                "avg_cost": 0.0045,
                "avg_latency_ms": 650.2,
            },
        },
    }


@pytest.fixture
def mock_routing_stats_empty():
    """Create mock routing statistics with no data."""
    return {
        "workflow": "test_workflow",
        "stage": "all",
        "days_analyzed": 7,
        "total_calls": 0,
        "avg_cost": 0.0,
        "avg_success_rate": 0.0,
        "models_used": [],
        "performance_by_model": {},
    }


@pytest.fixture
def mock_routing_stats_single_model():
    """Create mock routing statistics with single model."""
    return {
        "workflow": "test_workflow",
        "stage": "preprocessing",
        "days_analyzed": 7,
        "total_calls": 500,
        "avg_cost": 0.0020,
        "avg_success_rate": 0.94,
        "models_used": ["gpt-3.5-turbo"],
        "performance_by_model": {
            "gpt-3.5-turbo": {
                "calls": 500,
                "success_rate": 0.94,
                "avg_cost": 0.0020,
                "avg_latency_ms": 400.0,
            }
        },
    }


@pytest.fixture
def mock_routing_stats_multi_model():
    """Create mock routing statistics with multiple models."""
    return {
        "workflow": "complex_workflow",
        "stage": "all",
        "days_analyzed": 30,
        "total_calls": 5000,
        "avg_cost": 0.0030,
        "avg_success_rate": 0.96,
        "models_used": ["gpt-3.5-turbo", "gpt-4", "claude-2"],
        "performance_by_model": {
            "gpt-3.5-turbo": {
                "calls": 3000,
                "success_rate": 0.93,
                "avg_cost": 0.0015,
                "avg_latency_ms": 300.0,
            },
            "gpt-4": {
                "calls": 1500,
                "success_rate": 0.98,
                "avg_cost": 0.0050,
                "avg_latency_ms": 700.0,
            },
            "claude-2": {
                "calls": 500,
                "success_rate": 0.99,
                "avg_cost": 0.0040,
                "avg_latency_ms": 500.0,
            },
        },
    }


class TestCmdRoutingStatsSuccess:
    """Test successful execution paths for cmd_routing_stats."""

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_valid_workflow_when_routing_stats_called_then_displays_stats(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_success, capsys
    ):
        """Given a valid workflow with data, when routing stats command is called, then statistics are displayed."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_success

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 0
        mock_tracker_class.get_instance.assert_called_once()
        mock_router_class.assert_called_once_with(telemetry=mock_tracker)
        mock_router.get_routing_stats.assert_called_once_with(
            workflow="test_workflow", stage=None, days=7
        )

        captured = capsys.readouterr()
        assert "ADAPTIVE ROUTING STATISTICS" in captured.out
        assert "test_workflow" in captured.out
        assert "Total calls: 1,000" in captured.out
        assert "gpt-3.5-turbo" in captured.out
        assert "gpt-4" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_workflow_with_stage_when_routing_stats_called_then_displays_stage_stats(
        self, mock_tracker_class, mock_router_class, mock_args_with_stage, mock_routing_stats_single_model, capsys
    ):
        """Given a workflow with specific stage, when routing stats called, then stage-specific stats are shown."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_single_model

        # When
        result = cmd_routing_stats(mock_args_with_stage)

        # Then
        assert result == 0
        mock_router.get_routing_stats.assert_called_once_with(
            workflow="test_workflow", stage="preprocessing", days=14
        )

        captured = capsys.readouterr()
        assert "Stage: preprocessing" in captured.out
        assert "Total calls: 500" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_multiple_models_when_routing_stats_called_then_displays_all_model_performance(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_multi_model, capsys
    ):
        """Given multiple models used, when routing stats called, then all model performances are displayed."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_multi_model

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 0
        captured = capsys.readouterr()
        assert "gpt-3.5-turbo" in captured.out
        assert "gpt-4" in captured.out
        assert "claude-2" in captured.out
        assert "Calls: 3,000" in captured.out
        assert "Calls: 1,500" in captured.out
        assert "Calls: 500" in captured.out
        assert "Models used: 3" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_stats_data_when_routing_stats_called_then_displays_quality_scores(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_success, capsys
    ):
        """Given stats data, when routing stats called, then quality scores are calculated and displayed."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_success

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 0
        captured = capsys.readouterr()
        assert "Quality score:" in captured.out
        # gpt-3.5-turbo quality: 92 - (0.0015 * 10) = 91.985
        # gpt-4 quality: 98 - (0.0045 * 10) = 97.955
        assert "91.98" in captured.out or "92.00" in captured.out
        assert "97.95" in captured.out or "97.96" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_stats_data_when_routing_stats_called_then_displays_recommendations(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_success, capsys
    ):
        """Given stats data, when routing stats called, then recommendations are displayed."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_success

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 0
        captured = capsys.readouterr()
        assert "üí° Recommendations" in captured.out
        assert "Best model:" in captured.out
        # gpt-4 should be best: 98 - (0.0045 * 10) > 92 - (0.0015 * 10)
        assert "gpt-4" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_multiple_models_when_routing_stats_called_then_displays_cost_savings(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_success, capsys
    ):
        """Given multiple models, when routing stats called, then cost savings potential is displayed."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_success

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 0
        captured = capsys.readouterr()
        # Should show cost savings when multiple models exist
        assert "gpt-3.5-turbo" in captured.out
        assert "gpt-4" in captured.out


class TestCmdRoutingStatsNoData:
    """Test cmd_routing_stats when no data is available."""

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_no_data_when_routing_stats_called_then_returns_error(
        self, mock_tracker_class, mock_router_class, mock_args_basic, mock_routing_stats_empty, capsys
    ):
        """Given no data for workflow, when routing stats called, then error message is shown."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_empty

        # When
        result = cmd_routing_stats(mock_args_basic)

        # Then
        assert result == 1
        captured = capsys.readouterr()
        assert "‚ùå No data found" in captured.out
        assert "test_workflow" in captured.out
        assert "7 days" in captured.out

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_no_data_with_stage_when_routing_stats_called_then_returns_error(
        self, mock_tracker_class, mock_router_class, mock_args_with_stage, mock_routing_stats_empty, capsys
    ):
        """Given no data for workflow and stage, when routing stats called, then error is shown."""
        # Given
        mock_tracker = MagicMock()
        mock_tracker_class.get_instance.return_value = mock_tracker

        mock_router = MagicMock()
        mock_router_class.return_value = mock_router
        mock_router.get_routing_stats.return_value = mock_routing_stats_empty

        # When
        result = cmd_routing_stats(mock_args_with_stage)

        # Then
        assert result == 1
        captured = capsys.readouterr()
        assert "‚ùå No data found" in captured.out
        assert "14 days" in captured.out


class TestCmdRoutingStatsErrorHandling:
    """Test error handling in cmd_routing_stats."""

    @patch("empathy_os.cli.commands.routing.AdaptiveModelRouter")
    @patch("empathy_os.cli.commands.routing.UsageTracker")
    def test_given_tracker_error_when_routing_stats_called_then_returns_error(
        self, mock_tracker_class, mock_router_class, mock_args_basic, capsys
    ):
        """Given UsageTracker error, when routing stats called, then error code is returned."""
        # Given
        mock_tracker_class.get_instance.side_effect = Exception("Tracker initialization