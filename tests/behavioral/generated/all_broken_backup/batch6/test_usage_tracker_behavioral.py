"""Behavioral tests for usage_tracker.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import hashlib
import json
import logging
import threading
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock, Mock, call, mock_open, patch

import pytest

from empathy_os.telemetry.usage_tracker import UsageTracker


@pytest.fixture
def temp_telemetry_dir(tmp_path: Path) -> Path:
    """Create a temporary telemetry directory for testing.

    Args:
        tmp_path: Pytest temporary directory fixture

    Returns:
        Path to temporary telemetry directory
    """
    telemetry_dir = tmp_path / "telemetry"
    telemetry_dir.mkdir(parents=True, exist_ok=True)
    return telemetry_dir


@pytest.fixture
def usage_tracker(temp_telemetry_dir: Path) -> UsageTracker:
    """Create a UsageTracker instance for testing.

    Args:
        temp_telemetry_dir: Temporary telemetry directory

    Returns:
        UsageTracker instance
    """
    # Reset singleton
    UsageTracker._instance = None
    return UsageTracker(telemetry_dir=temp_telemetry_dir)


@pytest.fixture
def sample_llm_data() -> dict[str, Any]:
    """Provide sample LLM call data for testing.

    Returns:
        Dictionary with sample LLM call parameters
    """
    return {
        "workflow": "test_workflow",
        "stage": "test_stage",
        "tier": "tier_1",
        "model": "gpt-4",
        "provider": "openai",
        "cost": 0.03,
        "tokens": {"input": 100, "output": 50, "total": 150},
        "cache_hit": False,
        "cache_type": None,
        "duration_ms": 500,
        "user_id": "test_user_123",
        "prompt_cache_hit": False,
        "prompt_cache_creation_tokens": 0,
        "prompt_cache_read_tokens": 0,
    }


class TestUsageTrackerInitialization:
    """Test UsageTracker initialization behavior."""

    def test_given_no_args_when_init_then_uses_default_telemetry_dir(self) -> None:
        """Test that default telemetry directory is used when no args provided."""
        # Given: No initialization arguments
        UsageTracker._instance = None

        # When: Creating UsageTracker with defaults
        tracker = UsageTracker()

        # Then: Default telemetry directory is set
        expected_dir = Path.home() / ".empathy" / "telemetry"
        assert tracker.telemetry_dir == expected_dir
        assert tracker.usage_file == expected_dir / "usage.jsonl"

    def test_given_custom_dir_when_init_then_uses_custom_dir(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that custom telemetry directory is used when provided."""
        # Given: Custom telemetry directory
        custom_dir = temp_telemetry_dir / "custom"

        # When: Creating UsageTracker with custom directory
        tracker = UsageTracker(telemetry_dir=custom_dir)

        # Then: Custom directory is set
        assert tracker.telemetry_dir == custom_dir
        assert tracker.usage_file == custom_dir / "usage.jsonl"

    def test_given_custom_retention_when_init_then_sets_retention_days(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that custom retention days are set correctly."""
        # Given: Custom retention days
        retention_days = 30

        # When: Creating UsageTracker with custom retention
        tracker = UsageTracker(
            telemetry_dir=temp_telemetry_dir, retention_days=retention_days
        )

        # Then: Retention days are set
        assert tracker.retention_days == retention_days

    def test_given_custom_max_size_when_init_then_sets_max_file_size(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that custom max file size is set correctly."""
        # Given: Custom max file size
        max_size_mb = 5

        # When: Creating UsageTracker with custom max size
        tracker = UsageTracker(
            telemetry_dir=temp_telemetry_dir, max_file_size_mb=max_size_mb
        )

        # Then: Max file size is set
        assert tracker.max_file_size_mb == max_size_mb

    def test_given_nonexistent_dir_when_init_then_creates_directory(
        self, tmp_path: Path
    ) -> None:
        """Test that telemetry directory is created if it doesn't exist."""
        # Given: Non-existent directory
        new_dir = tmp_path / "new_telemetry"
        assert not new_dir.exists()

        # When: Creating UsageTracker with non-existent directory
        tracker = UsageTracker(telemetry_dir=new_dir)

        # Then: Directory is created
        assert new_dir.exists()
        assert new_dir.is_dir()

    def test_given_permission_error_when_init_then_handles_gracefully(
        self, caplog: pytest.LogCaptureFixture
    ) -> None:
        """Test that permission errors are handled gracefully during init."""
        # Given: Mock mkdir to raise PermissionError
        with patch("pathlib.Path.mkdir", side_effect=PermissionError("Access denied")):
            # When: Creating UsageTracker
            with caplog.at_level(logging.DEBUG):
                tracker = UsageTracker(telemetry_dir=Path("/root/telemetry"))

            # Then: Error is logged and tracker is created
            assert tracker is not None
            assert any("Failed to create telemetry directory" in record.message 
                      for record in caplog.records)


class TestUsageTrackerSingleton:
    """Test UsageTracker singleton pattern behavior."""

    def test_given_no_instance_when_get_instance_then_creates_new(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that get_instance creates new instance when none exists."""
        # Given: No existing instance
        UsageTracker._instance = None

        # When: Getting instance
        instance = UsageTracker.get_instance(telemetry_dir=temp_telemetry_dir)

        # Then: New instance is created
        assert instance is not None
        assert UsageTracker._instance is instance

    def test_given_existing_instance_when_get_instance_then_returns_same(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that get_instance returns existing instance."""
        # Given: Existing instance
        UsageTracker._instance = None
        first_instance = UsageTracker.get_instance(telemetry_dir=temp_telemetry_dir)

        # When: Getting instance again
        second_instance = UsageTracker.get_instance()

        # Then: Same instance is returned
        assert first_instance is second_instance

    def test_given_kwargs_when_get_instance_second_time_then_ignores_kwargs(
        self, temp_telemetry_dir: Path
    ) -> None:
        """Test that kwargs are ignored for existing instance."""
        # Given: Existing instance with specific directory
        UsageTracker._instance = None
        first_dir = temp_telemetry_dir / "first"
        first_instance = UsageTracker.get_instance(telemetry_dir=first_dir)

        # When: Getting instance with different kwargs
        second_dir = temp_telemetry_dir / "second"
        second_instance = UsageTracker.get_instance(telemetry_dir=second_dir)

        # Then: Same instance is returned with original directory
        assert first_instance is second_instance
        assert second_instance.telemetry_dir == first_dir


class TestTrackLLMCall:
    """Test LLM call tracking behavior."""

    def test_given_valid_data_when_track_llm_call_then_writes_to_file(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that LLM call data is written to usage file."""
        # Given: Valid LLM call data
        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: Data is written to file
        assert usage_tracker.usage_file.exists()
        with open(usage_tracker.usage_file) as f:
            line = f.readline()
            data = json.loads(line)

        assert data["workflow"] == sample_llm_data["workflow"]
        assert data["tier"] == sample_llm_data["tier"]
        assert data["model"] == sample_llm_data["model"]
        assert data["cost"] == sample_llm_data["cost"]

    def test_given_user_id_when_track_llm_call_then_hashes_user_id(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that user_id is hashed for privacy."""
        # Given: Data with user_id
        user_id = "test_user_123"
        sample_llm_data["user_id"] = user_id

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: User ID is hashed in file
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        expected_hash = hashlib.sha256(user_id.encode()).hexdigest()
        assert data["user_id_hash"] == expected_hash

    def test_given_no_user_id_when_track_llm_call_then_no_hash_in_data(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that no user_id_hash is included when user_id is None."""
        # Given: Data without user_id
        sample_llm_data["user_id"] = None

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: No user_id_hash in data
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        assert "user_id_hash" not in data

    def test_given_cache_hit_when_track_llm_call_then_records_cache_info(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that cache hit information is recorded."""
        # Given: Data with cache hit
        sample_llm_data["cache_hit"] = True
        sample_llm_data["cache_type"] = "semantic"

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: Cache info is in data
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        assert data["cache_hit"] is True
        assert data["cache_type"] == "semantic"

    def test_given_prompt_cache_when_track_llm_call_then_records_prompt_cache(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that prompt cache metrics are recorded."""
        # Given: Data with prompt cache metrics
        sample_llm_data["prompt_cache_hit"] = True
        sample_llm_data["prompt_cache_creation_tokens"] = 100
        sample_llm_data["prompt_cache_read_tokens"] = 50

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: Prompt cache metrics are in data
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        assert data["prompt_cache_hit"] is True
        assert data["prompt_cache_creation_tokens"] == 100
        assert data["prompt_cache_read_tokens"] == 50

    def test_given_multiple_calls_when_track_llm_call_then_appends_to_file(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that multiple LLM calls are appended to file."""
        # Given: Multiple LLM calls
        # When: Tracking multiple calls
        for i in range(3):
            data = sample_llm_data.copy()
            data["workflow"] = f"workflow_{i}"
            usage_tracker.track_llm_call(**data)

        # Then: All calls are in file
        with open(usage_tracker.usage_file) as f:
            lines = f.readlines()

        assert len(lines) == 3
        for i, line in enumerate(lines):
            data = json.loads(line)
            assert data["workflow"] == f"workflow_{i}"

    def test_given_timestamp_when_track_llm_call_then_records_iso_timestamp(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that ISO timestamp is recorded."""
        # Given: LLM call data
        before = datetime.utcnow()

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: Timestamp is in ISO format
        after = datetime.utcnow()
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        timestamp = datetime.fromisoformat(data["timestamp"].replace("Z", "+00:00"))
        assert before <= timestamp <= after

    def test_given_write_error_when_track_llm_call_then_handles_gracefully(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any],
        caplog: pytest.LogCaptureFixture
    ) -> None:
        """Test that write errors are handled gracefully."""
        # Given: Mock file write to raise error
        with patch("builtins.open", side_effect=OSError("Disk full")):
            # When: Tracking LLM call
            with caplog.at_level(logging.ERROR):
                usage_tracker.track_llm_call(**sample_llm_data)

            # Then: Error is logged and execution continues
            assert any("Failed to write telemetry" in record.message 
                      for record in caplog.records)

    def test_given_stage_none_when_track_llm_call_then_handles_optional_stage(
        self, usage_tracker: UsageTracker, sample_llm_data: dict[str, Any]
    ) -> None:
        """Test that optional stage parameter is handled correctly."""
        # Given: Data with stage=None
        sample_llm_data["stage"] = None

        # When: Tracking LLM call
        usage_tracker.track_llm_call(**sample_llm_data)

        # Then: Stage is None in data
        with open(usage_tracker.usage_file) as f:
            data = json.loads(f.readline())

        assert data["stage"]