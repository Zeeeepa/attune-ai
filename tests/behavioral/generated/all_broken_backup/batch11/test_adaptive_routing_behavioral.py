"""Behavioral tests for adaptive_routing.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

from dataclasses import dataclass
from unittest.mock import MagicMock, Mock, patch

import pytest

from empathy_os.models.adaptive_routing import (
    AdaptiveModelRouter,
    ModelPerformance,
    _get_registry,
)


@dataclass
class MockTelemetryEntry:
    """Mock telemetry entry for testing."""

    workflow: str
    stage: str
    model_id: str
    success: bool
    latency_ms: float
    cost: float
    error: str | None = None


class MockUsageTracker:
    """Mock usage tracker for testing."""

    def __init__(self):
        self.entries = []

    def get_telemetry_history(
        self, workflow: str | None = None, stage: str | None = None
    ) -> list[MockTelemetryEntry]:
        """Get filtered telemetry history."""
        filtered = self.entries
        if workflow:
            filtered = [e for e in filtered if e.workflow == workflow]
        if stage:
            filtered = [e for e in filtered if e.stage == stage]
        return filtered

    def add_entry(self, entry: MockTelemetryEntry):
        """Add telemetry entry."""
        self.entries.append(entry)


class MockModelRegistry:
    """Mock model registry for testing."""

    def __init__(self):
        self.models = {
            "gpt-4o-mini": {"tier": "CHEAP", "cost_per_1k_tokens": 0.00015},
            "claude-3-5-haiku-20241022": {"tier": "CAPABLE", "cost_per_1k_tokens": 0.001},
            "claude-3-5-sonnet-20241022": {"tier": "PREMIUM", "cost_per_1k_tokens": 0.003},
            "gpt-4o": {"tier": "PREMIUM", "cost_per_1k_tokens": 0.0025},
        }

    def get_models_by_tier(self, tier: str) -> list[str]:
        """Get models by tier."""
        return [mid for mid, info in self.models.items() if info["tier"] == tier]

    def get_tier(self, model_id: str) -> str | None:
        """Get tier for model."""
        return self.models.get(model_id, {}).get("tier")


@pytest.fixture
def mock_tracker():
    """Provide a mock usage tracker."""
    return MockUsageTracker()


@pytest.fixture
def mock_registry():
    """Provide a mock model registry."""
    return MockModelRegistry()


@pytest.fixture
def router(mock_tracker):
    """Provide an AdaptiveModelRouter instance."""
    with patch("empathy_os.models.adaptive_routing._get_registry") as mock_get_reg:
        mock_get_reg.return_value = MockModelRegistry()
        return AdaptiveModelRouter(mock_tracker)


class TestModelPerformance:
    """Test suite for ModelPerformance dataclass."""

    def test_quality_score_calculation_high_success_low_cost(self):
        """
        Given a model with high success rate and low cost
        When calculating quality score
        Then score prioritizes success rate over cost
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="CHEAP",
            success_rate=0.95,
            avg_latency_ms=100.0,
            avg_cost=0.001,
            sample_size=100,
        )

        # When
        score = perf.quality_score

        # Then
        assert score > 90  # 95 - (0.001 * 10) = ~94.99
        assert isinstance(score, float)

    def test_quality_score_calculation_low_success_high_cost(self):
        """
        Given a model with low success rate and high cost
        When calculating quality score
        Then score reflects both poor metrics
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="PREMIUM",
            success_rate=0.60,
            avg_latency_ms=500.0,
            avg_cost=0.05,
            sample_size=50,
        )

        # When
        score = perf.quality_score

        # Then
        assert score < 60  # 60 - (0.05 * 10) = 59.5
        assert score > 0

    def test_quality_score_perfect_performance_no_cost(self):
        """
        Given a model with perfect success and zero cost
        When calculating quality score
        Then score equals 100
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="CHEAP",
            success_rate=1.0,
            avg_latency_ms=50.0,
            avg_cost=0.0,
            sample_size=200,
        )

        # When
        score = perf.quality_score

        # Then
        assert score == 100.0

    def test_quality_score_zero_success_rate(self):
        """
        Given a model with zero success rate
        When calculating quality score
        Then score is negative due to cost penalty
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="PREMIUM",
            success_rate=0.0,
            avg_latency_ms=1000.0,
            avg_cost=0.1,
            sample_size=10,
        )

        # When
        score = perf.quality_score

        # Then
        assert score < 0  # 0 - (0.1 * 10) = -1.0

    def test_model_performance_dataclass_initialization(self):
        """
        Given model performance parameters
        When creating ModelPerformance instance
        Then all attributes are correctly set
        """
        # Given/When
        perf = ModelPerformance(
            model_id="test-id",
            tier="CAPABLE",
            success_rate=0.85,
            avg_latency_ms=250.0,
            avg_cost=0.005,
            sample_size=150,
            recent_failures=3,
        )

        # Then
        assert perf.model_id == "test-id"
        assert perf.tier == "CAPABLE"
        assert perf.success_rate == 0.85
        assert perf.avg_latency_ms == 250.0
        assert perf.avg_cost == 0.005
        assert perf.sample_size == 150
        assert perf.recent_failures == 3

    def test_model_performance_default_recent_failures(self):
        """
        Given model performance without recent_failures
        When creating ModelPerformance instance
        Then recent_failures defaults to 0
        """
        # Given/When
        perf = ModelPerformance(
            model_id="test-id",
            tier="CHEAP",
            success_rate=0.9,
            avg_latency_ms=100.0,
            avg_cost=0.001,
            sample_size=50,
        )

        # Then
        assert perf.recent_failures == 0


class TestGetRegistry:
    """Test suite for _get_registry function."""

    def test_get_registry_lazy_loading(self):
        """
        Given _get_registry is called for the first time
        When loading the registry
        Then it imports and caches MODEL_REGISTRY
        """
        # Given
        with patch("empathy_os.models.adaptive_routing._model_registry", None):
            with patch(
                "empathy_os.models.adaptive_routing.MODEL_REGISTRY", MockModelRegistry()
            ) as mock_reg:
                # When
                registry = _get_registry()

                # Then
                assert registry is not None

    def test_get_registry_returns_cached_instance(self):
        """
        Given _get_registry has been called before
        When calling it again
        Then it returns the cached instance
        """
        # Given
        with patch("empathy_os.models.adaptive_routing._model_registry", MockModelRegistry()) as cached:
            # When
            registry1 = _get_registry()
            registry2 = _get_registry()

            # Then
            assert registry1 is registry2


class TestAdaptiveModelRouterInitialization:
    """Test suite for AdaptiveModelRouter initialization."""

    def test_router_initialization_with_tracker(self):
        """
        Given a usage tracker instance
        When creating AdaptiveModelRouter
        Then router is initialized with the tracker
        """
        # Given
        tracker = MockUsageTracker()

        # When
        with patch("empathy_os.models.adaptive_routing._get_registry"):
            router = AdaptiveModelRouter(tracker)

            # Then
            assert router.tracker is tracker

    def test_router_initialization_loads_registry(self):
        """
        Given AdaptiveModelRouter is being created
        When initialization occurs
        Then registry is loaded via _get_registry
        """
        # Given
        tracker = MockUsageTracker()

        # When
        with patch("empathy_os.models.adaptive_routing._get_registry") as mock_get_reg:
            mock_get_reg.return_value = MockModelRegistry()
            router = AdaptiveModelRouter(tracker)

            # Then
            mock_get_reg.assert_called_once()
            assert router.registry is not None


class TestAdaptiveModelRouterAnalyzePerformance:
    """Test suite for analyze_performance method."""

    def test_analyze_performance_with_successful_entries(self, router, mock_tracker):
        """
        Given telemetry history with successful model calls
        When analyzing performance
        Then correct performance metrics are calculated
        """
        # Given
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="test-workflow",
                stage="test-stage",
                model_id="gpt-4o-mini",
                success=True,
                latency_ms=100.0,
                cost=0.001,
            )
        )
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="test-workflow",
                stage="test-stage",
                model_id="gpt-4o-mini",
                success=True,
                latency_ms=120.0,
                cost=0.0012,
            )
        )

        # When
        performance = router.analyze_performance("test-workflow", "test-stage")

        # Then
        assert len(performance) == 1
        assert performance[0].model_id == "gpt-4o-mini"
        assert performance[0].success_rate == 1.0
        assert performance[0].sample_size == 2
        assert 0.0011 <= performance[0].avg_cost <= 0.0012

    def test_analyze_performance_with_mixed_results(self, router, mock_tracker):
        """
        Given telemetry history with both successes and failures
        When analyzing performance
        Then success rate reflects actual results
        """
        # Given
        for i in range(8):
            mock_tracker.add_entry(
                MockTelemetryEntry(
                    workflow="test-workflow",
                    stage="test-stage",
                    model_id="claude-3-5-haiku-20241022",
                    success=True,
                    latency_ms=150.0,
                    cost=0.002,
                )
            )
        for i in range(2):
            mock_tracker.add_entry(
                MockTelemetryEntry(
                    workflow="test-workflow",
                    stage="test-stage",
                    model_id="claude-3-5-haiku-20241022",
                    success=False,
                    latency_ms=200.0,
                    cost=0.002,
                    error="timeout",
                )
            )

        # When
        performance = router.analyze_performance("test-workflow", "test-stage")

        # Then
        assert len(performance) == 1
        assert performance[0].model_id == "claude-3-5-haiku-20241022"
        assert performance[0].success_rate == 0.8
        assert performance[0].sample_size == 10

    def test_analyze_performance_multiple_models(self, router, mock_tracker):
        """
        Given telemetry history with multiple models
        When analyzing performance
        Then separate metrics are calculated for each model
        """
        # Given
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="test-workflow",
                stage="test-stage",
                model_id="gpt-4o-mini",
                success=True,
                latency_ms=100.0,
                cost=0.001,
            )
        )
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="test-workflow",
                stage="test-stage",
                model_id="gpt-4o",
                success=True,
                latency_ms=200.0,
                cost=0.003,
            )
        )

        # When
        performance = router.analyze_performance("test-workflow", "test-stage")

        # Then
        assert len(performance) == 2
        model_ids = [p.model_id for p in performance]
        assert "gpt-4o-mini" in model_ids
        assert "gpt-4o" in model_ids

    def test_analyze_performance_empty_history(self, router, mock_tracker):
        """
        Given empty telemetry history
        When analyzing performance
        Then returns empty list
        """
        # Given (empty tracker)

        # When
        performance = router.analyze_performance("test-workflow", "test-stage")

        # Then
        assert performance == []

    def test_analyze_performance_recent_failures_tracking(self, router, mock_tracker):
        """
        Given telemetry history with recent failures
        When analyzing performance
        Then recent_failures count reflects last 20 calls
        """
        # Given - 15 successes followed by 5 failures
        for i in range(15):
            mock_tracker.add_entry(
                MockTelemetryEntry(
                    workflow="test-workflow",
                    stage="test-stage",
                    model_id="gpt-4o-mini",
                    success=True,
                    latency_ms=100.0,
                    cost=0.001,
                )
            )
        for i in range(5):
            mock_tracker.add_entry(
                MockTelemetryEntry(
                    workflow="test-workflow",
                    stage="test-stage",
                    model_id="gpt-4o-mini",
                    success=False,
                    latency_ms=100.0,
                    cost=0.001,
                    error="error",
                )
            )

        # When
        performance = router.analyze_performance("test-workflow", "test-stage")

        # Then
        assert len(performance) == 1
        assert performance[0].recent_failures == 5

    def test_analyze_performance_workflow_filtering(self, router, mock_tracker):
        """
        Given telemetry history with multiple workflows
        When analyzing specific workflow
        Then only that workflow's data is included
        """
        # Given
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="workflow-a",
                stage="stage-1",
                model_id="gpt-4o-mini",
                success=True,
                latency_ms=100.0,
                cost=0.001,
            )
        )
        mock_tracker.add_entry(
            MockTelemetryEntry(
                workflow="workflow-b",
                stage="stage-1",
                model_id