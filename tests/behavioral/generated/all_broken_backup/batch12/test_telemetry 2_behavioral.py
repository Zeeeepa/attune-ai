"""Behavioral tests for telemetry 2.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import logging
from datetime import datetime
from typing import Any
from unittest.mock import Mock, patch, MagicMock

import pytest

from empathy_os.workflows.progressive.core import (
    ProgressiveWorkflowResult,
    Tier,
    TierResult,
)
from empathy_os.workflows.progressive.telemetry_2 import ProgressiveTelemetry


@pytest.fixture
def mock_usage_tracker():
    """Fixture providing a mocked UsageTracker instance."""
    with patch("empathy_os.workflows.progressive.telemetry_2.UsageTracker") as mock:
        tracker_instance = MagicMock()
        mock.get_instance.return_value = tracker_instance
        yield tracker_instance


@pytest.fixture
def tier_result():
    """Fixture providing a sample TierResult."""
    result = TierResult(
        tier=Tier.FAST,
        model="gpt-4o-mini",
        output="Test output",
        tokens_used={"input": 100, "output": 50, "total": 150},
        cost=0.001,
        duration=1.5,
        success=True,
        error=None,
        metadata={"test": "data"},
    )
    return result


@pytest.fixture
def workflow_result():
    """Fixture providing a sample ProgressiveWorkflowResult."""
    tier_result1 = TierResult(
        tier=Tier.FAST,
        model="gpt-4o-mini",
        output="Output 1",
        tokens_used={"input": 100, "output": 50, "total": 150},
        cost=0.001,
        duration=1.0,
        success=False,
        error=None,
        metadata={},
    )
    tier_result2 = TierResult(
        tier=Tier.SMART,
        model="gpt-4o",
        output="Output 2",
        tokens_used={"input": 200, "output": 100, "total": 300},
        cost=0.005,
        duration=2.0,
        success=True,
        error=None,
        metadata={},
    )
    result = ProgressiveWorkflowResult(
        final_output="Final output",
        tier_results=[tier_result1, tier_result2],
        final_tier=Tier.SMART,
        total_cost=0.006,
        total_duration=3.0,
        success=True,
        error=None,
    )
    return result


class TestProgressiveTelemetryInitialization:
    """Test suite for ProgressiveTelemetry initialization."""

    def test_given_workflow_name_when_initialize_then_sets_attributes(
        self, mock_usage_tracker
    ):
        """Given a workflow name, when initializing telemetry, then it sets proper attributes."""
        # Given
        workflow_name = "test-workflow"
        user_id = "user123"

        # When
        telemetry = ProgressiveTelemetry(workflow_name, user_id)

        # Then
        assert telemetry.workflow_name == workflow_name
        assert telemetry.user_id == user_id
        assert telemetry.tracker == mock_usage_tracker

    def test_given_no_user_id_when_initialize_then_sets_user_id_to_none(
        self, mock_usage_tracker
    ):
        """Given no user_id, when initializing telemetry, then user_id is None."""
        # Given
        workflow_name = "test-workflow"

        # When
        telemetry = ProgressiveTelemetry(workflow_name)

        # Then
        assert telemetry.workflow_name == workflow_name
        assert telemetry.user_id is None

    def test_given_initialization_when_get_tracker_then_calls_get_instance(
        self, mock_usage_tracker
    ):
        """Given initialization, when getting tracker, then calls UsageTracker.get_instance."""
        # Given/When
        telemetry = ProgressiveTelemetry("test-workflow")

        # Then
        from empathy_os.workflows.progressive.telemetry_2 import UsageTracker

        UsageTracker.get_instance.assert_called_once()


class TestTrackTierExecution:
    """Test suite for track_tier_execution method."""

    def test_given_tier_result_when_track_execution_then_calls_track_llm_call(
        self, mock_usage_tracker, tier_result
    ):
        """Given a tier result, when tracking execution, then calls track_llm_call with proper args."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow", "user123")
        attempt = 1
        escalated = False

        # When
        telemetry.track_tier_execution(tier_result, attempt, escalated)

        # Then
        mock_usage_tracker.track_llm_call.assert_called_once()
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        assert call_args["workflow"] == "test-workflow"
        assert call_args["stage"] == "tier-fast-attempt-1"
        assert call_args["tier"] == "FAST"
        assert call_args["model"] == "gpt-4o-mini"
        assert call_args["cost"] == 0.001
        assert call_args["tokens"]["total_tokens"] == 150
        assert call_args["user_id"] == "user123"

    def test_given_tier_result_when_track_execution_then_extracts_tokens_correctly(
        self, mock_usage_tracker, tier_result
    ):
        """Given a tier result, when tracking execution, then extracts token counts correctly."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        attempt = 2
        escalated = True

        # When
        telemetry.track_tier_execution(
            tier_result, attempt, escalated, "quality_insufficient"
        )

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        tokens = call_args["tokens"]
        assert tokens["input_tokens"] == 100
        assert tokens["output_tokens"] == 50
        assert tokens["total_tokens"] == 150

    def test_given_tier_result_when_track_execution_then_sets_cache_hit_to_false(
        self, mock_usage_tracker, tier_result
    ):
        """Given a tier result, when tracking execution, then sets cache_hit to False."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        telemetry.track_tier_execution(tier_result, 1, False)

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        assert call_args["cache_hit"] is False
        assert call_args["cache_type"] is None

    def test_given_tier_result_when_track_execution_then_converts_duration_to_ms(
        self, mock_usage_tracker, tier_result
    ):
        """Given a tier result, when tracking execution, then converts duration to milliseconds."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        tier_result.duration = 2.5

        # When
        telemetry.track_tier_execution(tier_result, 1, False)

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        assert call_args["duration_ms"] == 2500

    def test_given_escalated_tier_when_track_execution_then_includes_escalation_reason(
        self, mock_usage_tracker, tier_result
    ):
        """Given an escalated tier, when tracking execution, then includes escalation reason."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        escalation_reason = "quality_check_failed"

        # When
        telemetry.track_tier_execution(tier_result, 1, True, escalation_reason)

        # Then
        # The method is called successfully with escalation parameters
        mock_usage_tracker.track_llm_call.assert_called_once()

    def test_given_missing_token_keys_when_track_execution_then_uses_zero_defaults(
        self, mock_usage_tracker
    ):
        """Given missing token keys, when tracking execution, then uses zero as default."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        tier_result = TierResult(
            tier=Tier.FAST,
            model="gpt-4o-mini",
            output="Test",
            tokens_used={},  # Empty tokens
            cost=0.001,
            duration=1.0,
            success=True,
            error=None,
            metadata={},
        )

        # When
        telemetry.track_tier_execution(tier_result, 1, False)

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        tokens = call_args["tokens"]
        assert tokens["input_tokens"] == 0
        assert tokens["output_tokens"] == 0
        assert tokens["total_tokens"] == 0

    def test_given_tracker_exception_when_track_execution_then_logs_warning_and_continues(
        self, mock_usage_tracker, tier_result, caplog
    ):
        """Given tracker exception, when tracking execution, then logs warning and continues."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        mock_usage_tracker.track_llm_call.side_effect = Exception("Tracker error")

        # When
        with caplog.at_level(logging.WARNING):
            telemetry.track_tier_execution(tier_result, 1, False)

        # Then
        assert "Failed to track tier execution" in caplog.text
        assert "Tracker error" in caplog.text

    def test_given_different_tiers_when_track_execution_then_stage_names_differ(
        self, mock_usage_tracker
    ):
        """Given different tiers, when tracking execution, then stage names are properly formatted."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        smart_result = TierResult(
            tier=Tier.SMART,
            model="gpt-4o",
            output="Test",
            tokens_used={"input": 100, "output": 50, "total": 150},
            cost=0.005,
            duration=1.0,
            success=True,
            error=None,
            metadata={},
        )

        # When
        telemetry.track_tier_execution(smart_result, 3, False)

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        assert call_args["stage"] == "tier-smart-attempt-3"
        assert call_args["tier"] == "SMART"

    def test_given_deep_tier_when_track_execution_then_formats_correctly(
        self, mock_usage_tracker
    ):
        """Given deep tier, when tracking execution, then formats tier name correctly."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        deep_result = TierResult(
            tier=Tier.DEEP,
            model="o1-preview",
            output="Test",
            tokens_used={"input": 100, "output": 50, "total": 150},
            cost=0.01,
            duration=5.0,
            success=True,
            error=None,
            metadata={},
        )

        # When
        telemetry.track_tier_execution(deep_result, 1, False)

        # Then
        call_args = mock_usage_tracker.track_llm_call.call_args[1]
        assert call_args["tier"] == "DEEP"


class TestGetProvider:
    """Test suite for _get_provider method."""

    def test_given_gpt_model_when_get_provider_then_returns_openai(
        self, mock_usage_tracker
    ):
        """Given a GPT model, when getting provider, then returns openai."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        provider = telemetry._get_provider("gpt-4o-mini")

        # Then
        assert provider == "openai"

    def test_given_o1_model_when_get_provider_then_returns_openai(
        self, mock_usage_tracker
    ):
        """Given an o1 model, when getting provider, then returns openai."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        provider = telemetry._get_provider("o1-preview")

        # Then
        assert provider == "openai"

    def test_given_claude_model_when_get_provider_then_returns_anthropic(
        self, mock_usage_tracker
    ):
        """Given a Claude model, when getting provider, then returns anthropic."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        provider = telemetry._get_provider("claude-3-5-sonnet")

        # Then
        assert provider == "anthropic"

    def test_given_gemini_model_when_get_provider_then_returns_google(
        self, mock_usage_tracker
    ):
        """Given a Gemini model, when getting provider, then returns google."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        provider = telemetry._get_provider("gemini-1.5-flash")

        # Then
        assert provider == "google"

    def test_given_unknown_model_when_get_provider_then_returns_unknown(
        self, mock_usage_tracker
    ):
        """Given an unknown model, when getting provider, then returns unknown."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")

        # When
        provider = telemetry._get_provider("mystery-model-x")

        # Then
        assert provider == "unknown"


class TestTrackWorkflowCompletion:
    """Test suite for track_workflow_completion method."""

    def test_given_workflow_result_when_track_completion_then_calls_tracker(
        self, mock_usage_tracker, workflow_result
    ):
        """Given a workflow result, when tracking completion, then calls tracker."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow", "user123")

        # When
        telemetry.track_workflow_completion(workflow_result)

        # Then
        # Should track workflow-level metrics
        assert mock_usage_tracker.track_llm_call.called or mock_usage_tracker.track_workflow.called

    def test_given_single_tier_result_when_track_completion_then_calculates_no_savings(
        self, mock_usage_tracker
    ):
        """Given a single tier result, when tracking completion, then calculates no savings."""
        # Given
        telemetry = ProgressiveTelemetry("test-workflow")
        single_tier_result = TierResult(
            tier=Tier.FAST,
            model="gpt-4o-mini",
            output="Output",
            tokens_used={"input": 100, "output": 50, "total": 150},
            cost=0.001,
            duration=1.0,
            success=True,
            error=None,
            metadata={},
        )
        workflow_result