"""Behavioral tests for alerts_cli.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

import sys
from typing import Any
from unittest.mock import MagicMock, Mock, call, patch

import pytest
from click.testing import CliRunner

from empathy_os.monitoring.alerts_cli import alerts, init


@pytest.fixture
def cli_runner():
    """Provide a Click CLI test runner."""
    return CliRunner()


@pytest.fixture
def mock_alert_engine():
    """Provide a mock alert engine."""
    with patch("empathy_os.monitoring.alerts_cli.get_alert_engine") as mock:
        engine = MagicMock()
        mock.return_value = engine
        yield engine


@pytest.fixture
def mock_create_alert():
    """Provide a mock _create_alert function."""
    with patch("empathy_os.monitoring.alerts_cli._create_alert") as mock:
        yield mock


class TestAlertsCommandGroup:
    """Tests for the alerts command group."""

    def test_alerts_group_exists(self, cli_runner: CliRunner):
        """
        Given: The alerts CLI module is imported
        When: Invoking the alerts command with --help
        Then: It should display help text with expected content
        """
        result = cli_runner.invoke(alerts, ["--help"])
        
        assert result.exit_code == 0
        assert "Alert management commands" in result.output


class TestInitCommandNonInteractive:
    """Tests for the init command in non-interactive mode."""

    def test_non_interactive_with_all_required_webhook_params(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: All required parameters for webhook channel are provided
        When: Running init in non-interactive mode with webhook
        Then: Alert should be created with correct parameters
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "10.0",
                "--channel", "webhook",
                "--webhook-url", "https://example.com/webhook",
            ],
        )
        
        assert result.exit_code == 0
        mock_create_alert.assert_called_once_with(
            "daily_cost", 10.0, "webhook", "https://example.com/webhook", None
        )

    def test_non_interactive_with_all_required_email_params(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: All required parameters for email channel are provided
        When: Running init in non-interactive mode with email
        Then: Alert should be created with correct parameters
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "error_rate",
                "--threshold", "5.0",
                "--channel", "email",
                "--email", "test@example.com",
            ],
        )
        
        assert result.exit_code == 0
        mock_create_alert.assert_called_once_with(
            "error_rate", 5.0, "email", None, "test@example.com"
        )

    def test_non_interactive_with_stdout_channel(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: All required parameters for stdout channel are provided
        When: Running init in non-interactive mode with stdout
        Then: Alert should be created without webhook or email
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "avg_latency",
                "--threshold", "500.0",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 0
        mock_create_alert.assert_called_once_with(
            "avg_latency", 500.0, "stdout", None, None
        )

    def test_non_interactive_missing_metric(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode is enabled
        When: Running init without metric parameter
        Then: Should exit with error message
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--threshold", "10.0",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 1
        assert "Error: --metric, --threshold, and --channel required" in result.output

    def test_non_interactive_missing_threshold(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode is enabled
        When: Running init without threshold parameter
        Then: Should exit with error message
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 1
        assert "Error: --metric, --threshold, and --channel required" in result.output

    def test_non_interactive_missing_channel(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode is enabled
        When: Running init without channel parameter
        Then: Should exit with error message
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "10.0",
            ],
        )
        
        assert result.exit_code == 1
        assert "Error: --metric, --threshold, and --channel required" in result.output

    def test_non_interactive_webhook_without_url(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode with webhook channel
        When: Running init without webhook-url parameter
        Then: Should exit with error message
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "10.0",
                "--channel", "webhook",
            ],
        )
        
        assert result.exit_code == 1
        assert "Error: --webhook-url required for webhook channel" in result.output

    def test_non_interactive_email_without_address(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode with email channel
        When: Running init without email parameter
        Then: Should exit with error message
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "10.0",
                "--channel", "email",
            ],
        )
        
        assert result.exit_code == 1
        assert "Error: --email required for email channel" in result.output

    def test_non_interactive_all_metrics(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: Each valid metric type
        When: Running init in non-interactive mode with each metric
        Then: Alert should be created for each metric type
        """
        metrics = ["daily_cost", "error_rate", "avg_latency", "token_usage"]
        
        for metric in metrics:
            mock_create_alert.reset_mock()
            result = cli_runner.invoke(
                init,
                [
                    "--non-interactive",
                    "--metric", metric,
                    "--threshold", "100.0",
                    "--channel", "stdout",
                ],
            )
            
            assert result.exit_code == 0
            mock_create_alert.assert_called_once()
            call_args = mock_create_alert.call_args[0]
            assert call_args[0] == metric

    def test_non_interactive_invalid_metric(self, cli_runner: CliRunner):
        """
        Given: An invalid metric type
        When: Running init in non-interactive mode
        Then: Should fail with click validation error
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "invalid_metric",
                "--threshold", "10.0",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code != 0
        assert "Invalid value" in result.output or "Error" in result.output


class TestInitCommandInteractive:
    """Tests for the init command in interactive mode."""

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_daily_cost_webhook(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Interactive mode for daily cost metric with webhook
        When: Providing valid inputs through prompts
        Then: Alert should be created with correct configuration
        """
        result = cli_runner.invoke(
            init,
            input="a\n10.0\na\nhttps://example.com/webhook\n",
        )
        
        assert result.exit_code == 0
        assert "ðŸ”” Alert Setup Workflow" in result.output
        assert "What metric do you want to monitor?" in result.output
        mock_create_alert.assert_called_once()

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_error_rate_email(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Interactive mode for error rate metric with email
        When: Providing valid inputs through prompts
        Then: Alert should be created with email configuration
        """
        result = cli_runner.invoke(
            init,
            input="b\n5.0\nb\ntest@example.com\n",
        )
        
        assert result.exit_code == 0
        assert "Error Rate" in result.output
        mock_create_alert.assert_called_once()

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_latency_stdout(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Interactive mode for latency metric with stdout
        When: Providing valid inputs through prompts
        Then: Alert should be created with stdout configuration
        """
        result = cli_runner.invoke(
            init,
            input="c\n500.0\nc\n",
        )
        
        assert result.exit_code == 0
        assert "Average Latency" in result.output
        mock_create_alert.assert_called_once()

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_token_usage_stdout(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Interactive mode for token usage metric with stdout
        When: Providing valid inputs through prompts
        Then: Alert should be created with stdout configuration
        """
        result = cli_runner.invoke(
            init,
            input="d\n10000.0\nc\n",
        )
        
        assert result.exit_code == 0
        assert "Token Usage" in result.output
        mock_create_alert.assert_called_once()

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_uses_default_threshold(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Interactive mode with default threshold prompt
        When: Pressing enter to accept default threshold
        Then: Default threshold should be used
        """
        result = cli_runner.invoke(
            init,
            input="a\n\nc\n",
        )
        
        assert result.exit_code == 0
        # Default for daily_cost is 10.0
        call_args = mock_create_alert.call_args[0]
        assert call_args[1] == 10.0

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_all_metric_choices(
        self, mock_create_alert: Mock, cli_runner: CliRunner
    ):
        """
        Given: Each valid metric choice in interactive mode
        When: Selecting each metric option
        Then: Correct metric should be configured
        """
        choices = ["a", "b", "c", "d"]
        expected_metrics = ["daily_cost", "error_rate", "avg_latency", "token_usage"]
        
        for choice, expected_metric in zip(choices, expected_metrics):
            mock_create_alert.reset_mock()
            result = cli_runner.invoke(
                init,
                input=f"{choice}\n10.0\nc\n",
            )
            
            assert result.exit_code == 0
            call_args = mock_create_alert.call_args[0]
            assert call_args[0] == expected_metric


class TestInitCommandEdgeCases:
    """Tests for edge cases in the init command."""

    def test_invalid_threshold_type_non_interactive(self, cli_runner: CliRunner):
        """
        Given: Non-interactive mode with invalid threshold type
        When: Running init with non-numeric threshold
        Then: Should fail with validation error
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "not_a_number",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code != 0

    def test_negative_threshold_non_interactive(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: Non-interactive mode with negative threshold
        When: Running init with negative threshold value
        Then: Should accept negative value (validation in _create_alert)
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "-10.0",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 0
        call_args = mock_create_alert.call_args[0]
        assert call_args[1] == -10.0

    def test_zero_threshold_non_interactive(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: Non-interactive mode with zero threshold
        When: Running init with threshold of 0.0
        Then: Should accept zero value
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "daily_cost",
                "--threshold", "0.0",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 0
        call_args = mock_create_alert.call_args[0]
        assert call_args[1] == 0.0

    def test_very_large_threshold(
        self, cli_runner: CliRunner, mock_create_alert: Mock
    ):
        """
        Given: Non-interactive mode with very large threshold
        When: Running init with large numeric value
        Then: Should accept large value
        """
        result = cli_runner.invoke(
            init,
            [
                "--non-interactive",
                "--metric", "token_usage",
                "--threshold", "999999999.99",
                "--channel", "stdout",
            ],
        )
        
        assert result.exit_code == 0
        call_args = mock_create_alert.call_args[0]
        assert call_args[1] == 999999999.99

    @patch("empathy_os.monitoring.alerts_cli._create_alert")
    def test_interactive_empty_webhook_url(
        self, mock_create_alert: Mock, cli_runner: CliRunner