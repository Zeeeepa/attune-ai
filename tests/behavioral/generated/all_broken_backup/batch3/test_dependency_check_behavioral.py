"""Behavioral tests for dependency_check.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, mock_open, patch

import pytest

from empathy_os.workflows.dependency_check import (
    DEPENDENCY_CHECK_STEPS,
    KNOWN_VULNERABILITIES,
    DependencyCheckWorkflow,
)
from empathy_os.workflows.base import ModelTier


# Fixtures


@pytest.fixture
def sample_requirements_txt():
    """Provide sample requirements.txt content."""
    return """requests==2.20.0
urllib3==1.25.0
pyyaml==5.3.1
django==3.2.0
flask==1.1.0
numpy==1.19.0
"""


@pytest.fixture
def sample_package_json():
    """Provide sample package.json content."""
    return json.dumps({
        "dependencies": {
            "lodash": "4.17.15",
            "axios": "0.19.0",
            "express": "4.17.1"
        }
    })


@pytest.fixture
def sample_pipfile_lock():
    """Provide sample Pipfile.lock content."""
    return json.dumps({
        "default": {
            "requests": {"version": "==2.20.0"},
            "urllib3": {"version": "==1.25.0"}
        }
    })


@pytest.fixture
def workflow():
    """Create a DependencyCheckWorkflow instance."""
    return DependencyCheckWorkflow()


@pytest.fixture
def mock_file_system(sample_requirements_txt, sample_package_json):
    """Mock file system with dependency files."""
    files = {
        "/test/requirements.txt": sample_requirements_txt,
        "/test/package.json": sample_package_json,
        "/test/Pipfile.lock": json.dumps({"default": {"requests": {"version": "==2.20.0"}}}),
    }

    def mock_open_func(path, *args, **kwargs):
        path_str = str(path)
        if path_str in files:
            return mock_open(read_data=files[path_str])()
        raise FileNotFoundError(f"No such file: {path}")

    return mock_open_func


# Test DependencyCheckWorkflow initialization


class TestDependencyCheckWorkflowInitialization:
    """Test workflow initialization and configuration."""

    def test_given_no_args_when_initialize_then_creates_instance(self):
        """Given no arguments, when initializing workflow, then creates instance with defaults."""
        # Given/When
        workflow = DependencyCheckWorkflow()

        # Then
        assert workflow.name == "dependency-check"
        assert workflow.description == "Audit dependencies for vulnerabilities and updates"
        assert workflow.stages == ["inventory", "assess", "report"]

    def test_given_kwargs_when_initialize_then_passes_to_base(self):
        """Given keyword arguments, when initializing, then passes to base class."""
        # Given/When
        workflow = DependencyCheckWorkflow(custom_arg="value")

        # Then
        assert workflow.name == "dependency-check"

    def test_given_workflow_when_check_tier_map_then_has_correct_tiers(self):
        """Given initialized workflow, when checking tier map, then has correct model tiers."""
        # Given
        workflow = DependencyCheckWorkflow()

        # When/Then
        assert workflow.tier_map["inventory"] == ModelTier.CHEAP
        assert workflow.tier_map["assess"] == ModelTier.CAPABLE
        assert workflow.tier_map["report"] == ModelTier.CAPABLE


# Test run_stage routing


class TestRunStageRouting:
    """Test stage routing logic."""

    @pytest.mark.asyncio
    async def test_given_inventory_stage_when_run_stage_then_calls_inventory(self, workflow):
        """Given inventory stage, when running stage, then calls inventory method."""
        # Given
        input_data = {"dependency_files": ["/test/requirements.txt"]}
        workflow._inventory = AsyncMock(return_value=({"packages": []}, 100, 50))

        # When
        result, prompt_tokens, completion_tokens = await workflow.run_stage(
            "inventory", ModelTier.CHEAP, input_data
        )

        # Then
        workflow._inventory.assert_called_once_with(input_data, ModelTier.CHEAP)
        assert prompt_tokens == 100
        assert completion_tokens == 50

    @pytest.mark.asyncio
    async def test_given_assess_stage_when_run_stage_then_calls_assess(self, workflow):
        """Given assess stage, when running stage, then calls assess method."""
        # Given
        input_data = {"packages": []}
        workflow._assess = AsyncMock(return_value=({"vulnerabilities": []}, 200, 100))

        # When
        result, prompt_tokens, completion_tokens = await workflow.run_stage(
            "assess", ModelTier.CAPABLE, input_data
        )

        # Then
        workflow._assess.assert_called_once_with(input_data, ModelTier.CAPABLE)
        assert prompt_tokens == 200
        assert completion_tokens == 100

    @pytest.mark.asyncio
    async def test_given_report_stage_when_run_stage_then_calls_report(self, workflow):
        """Given report stage, when running stage, then calls report method."""
        # Given
        input_data = {"vulnerabilities": []}
        workflow._report = AsyncMock(return_value=({"report": "test"}, 300, 150))

        # When
        result, prompt_tokens, completion_tokens = await workflow.run_stage(
            "report", ModelTier.CAPABLE, input_data
        )

        # Then
        workflow._report.assert_called_once_with(input_data, ModelTier.CAPABLE)
        assert prompt_tokens == 300
        assert completion_tokens == 150

    @pytest.mark.asyncio
    async def test_given_invalid_stage_when_run_stage_then_raises_value_error(self, workflow):
        """Given invalid stage name, when running stage, then raises ValueError."""
        # Given
        input_data = {}

        # When/Then
        with pytest.raises(ValueError, match="Unknown stage: invalid_stage"):
            await workflow.run_stage("invalid_stage", ModelTier.CHEAP, input_data)


# Test _inventory stage


class TestInventoryStage:
    """Test inventory stage functionality."""

    @pytest.mark.asyncio
    async def test_given_requirements_txt_when_inventory_then_parses_packages(
        self, workflow, sample_requirements_txt
    ):
        """Given requirements.txt, when running inventory, then parses all packages."""
        # Given
        input_data = {"dependency_files": ["/test/requirements.txt"]}

        with patch("pathlib.Path.exists", return_value=True):
            with patch("pathlib.Path.read_text", return_value=sample_requirements_txt):
                # When
                result, prompt_tokens, completion_tokens = await workflow._inventory(
                    input_data, ModelTier.CHEAP
                )

        # Then
        assert "packages" in result
        packages = result["packages"]
        assert any(p["name"] == "requests" and p["version"] == "2.20.0" for p in packages)
        assert any(p["name"] == "urllib3" and p["version"] == "1.25.0" for p in packages)
        assert any(p["name"] == "pyyaml" and p["version"] == "5.3.1" for p in packages)

    @pytest.mark.asyncio
    async def test_given_package_json_when_inventory_then_parses_dependencies(
        self, workflow, sample_package_json
    ):
        """Given package.json, when running inventory, then parses dependencies."""
        # Given
        input_data = {"dependency_files": ["/test/package.json"]}

        with patch("pathlib.Path.exists", return_value=True):
            with patch("pathlib.Path.read_text", return_value=sample_package_json):
                # When
                result, prompt_tokens, completion_tokens = await workflow._inventory(
                    input_data, ModelTier.CHEAP
                )

        # Then
        assert "packages" in result
        packages = result["packages"]
        assert any(p["name"] == "lodash" and p["version"] == "4.17.15" for p in packages)
        assert any(p["name"] == "axios" and p["version"] == "0.19.0" for p in packages)

    @pytest.mark.asyncio
    async def test_given_pipfile_lock_when_inventory_then_parses_packages(
        self, workflow, sample_pipfile_lock
    ):
        """Given Pipfile.lock, when running inventory, then parses packages."""
        # Given
        input_data = {"dependency_files": ["/test/Pipfile.lock"]}

        with patch("pathlib.Path.exists", return_value=True):
            with patch("pathlib.Path.read_text", return_value=sample_pipfile_lock):
                # When
                result, prompt_tokens, completion_tokens = await workflow._inventory(
                    input_data, ModelTier.CHEAP
                )

        # Then
        assert "packages" in result
        packages = result["packages"]
        assert any(p["name"] == "requests" and p["version"] == "2.20.0" for p in packages)

    @pytest.mark.asyncio
    async def test_given_nonexistent_file_when_inventory_then_returns_empty_packages(
        self, workflow
    ):
        """Given nonexistent file, when running inventory, then returns empty packages list."""
        # Given
        input_data = {"dependency_files": ["/test/nonexistent.txt"]}

        with patch("pathlib.Path.exists", return_value=False):
            # When
            result, prompt_tokens, completion_tokens = await workflow._inventory(
                input_data, ModelTier.CHEAP
            )

        # Then
        assert result["packages"] == []

    @pytest.mark.asyncio
    async def test_given_empty_dependency_files_when_inventory_then_returns_empty(
        self, workflow
    ):
        """Given empty dependency files list, when running inventory, then returns empty packages."""
        # Given
        input_data = {"dependency_files": []}

        # When
        result, prompt_tokens, completion_tokens = await workflow._inventory(
            input_data, ModelTier.CHEAP
        )

        # Then
        assert result["packages"] == []
        assert prompt_tokens >= 0
        assert completion_tokens >= 0

    @pytest.mark.asyncio
    async def test_given_malformed_json_when_inventory_then_skips_file(self, workflow):
        """Given malformed JSON file, when running inventory, then skips the file."""
        # Given
        input_data = {"dependency_files": ["/test/package.json"]}
        malformed_json = "{invalid json content"

        with patch("pathlib.Path.exists", return_value=True):
            with patch("pathlib.Path.read_text", return_value=malformed_json):
                # When
                result, prompt_tokens, completion_tokens = await workflow._inventory(
                    input_data, ModelTier.CHEAP
                )

        # Then
        assert result["packages"] == []


# Test _assess stage


class TestAssessStage:
    """Test assess stage functionality."""

    @pytest.mark.asyncio
    async def test_given_vulnerable_packages_when_assess_then_identifies_vulnerabilities(
        self, workflow
    ):
        """Given vulnerable packages, when running assess, then identifies all vulnerabilities."""
        # Given
        input_data = {
            "packages": [
                {"name": "requests", "version": "2.20.0", "source": "requirements.txt"},
                {"name": "urllib3", "version": "1.25.0", "source": "requirements.txt"},
                {"name": "pyyaml", "version": "5.3.1", "source": "requirements.txt"},
            ]
        }

        # When
        result, prompt_tokens, completion_tokens = await workflow._assess(
            input_data, ModelTier.CAPABLE
        )

        # Then
        assert "vulnerabilities" in result
        vulnerabilities = result["vulnerabilities"]
        assert len(vulnerabilities) == 3
        assert any(v["package"] == "requests" for v in vulnerabilities)
        assert any(v["package"] == "urllib3" for v in vulnerabilities)
        assert any(v["package"] == "pyyaml" for v in vulnerabilities)

    @pytest.mark.asyncio
    async def test_given_safe_packages_when_assess_then_no_vulnerabilities(self, workflow):
        """Given safe packages, when running assess, then finds no vulnerabilities."""
        # Given
        input_data = {
            "packages": [
                {"name": "requests", "version": "2.26.0", "source": "requirements.txt"},
                {"name": "urllib3", "version": "1.26.6", "source": "requirements.txt"},
            ]
        }

        # When
        result, prompt_tokens, completion_tokens = await workflow._assess(
            input_data, ModelTier.CAPABLE
        )

        # Then
        assert "vulnerabilities" in result
        assert len(result["vulnerabilities"]) == 0

    @pytest.mark.asyncio
    async def test_given_mixed_packages_when_assess_then_identifies_only_vulnerable(
        self, workflow
    ):
        """Given mixed safe and vulnerable packages, when assessing, then identifies only vulnerable ones."""
        # Given
        input_data = {
            "packages": [
                {"name": "requests", "version": "2.20.0", "source": "requirements.txt"},
                {"name": "numpy", "version": "1.19.0", "source": "requirements.txt"},
                {"name": "urllib3", "version": "1.26.6", "source": "requirements.txt"},
            ]
        }

        # When
        result, prompt_tokens, completion_tokens = await workflow._assess(
            input_data, ModelTier.CAPABLE
        )

        # Then
        vulnerabilities = result["vulnerabilities"]
        assert len(vulnerabilities) == 1
        assert vulnerabilities[0]["package"] == "requests"

    @pytest.mark.asyncio
    async def test_given_empty_packages_when_assess_then_returns_empty_vulnerabilities(
        self, workflow
    ):
        """Given empty packages list, when running assess, then returns empty vulnerabilities."""
        # Given
        input_data = {"packages": []}

        # When
        result, prompt_tokens, completion_tokens = await workflow._assess(
            input_data, ModelTier.CAPABLE
        )

        # Then
        assert result["vulnerabilities"] == []

    @pytest.mark.asyncio
    async def test_given_vulnerable_package_when_assess_then_includes_severity(self, workflow):
        """Given vulnerable package, when assessing, then includes severity level."""
        # Given
        input_data = {
            "packages": [
                {"name": "pyyaml", "version": "5.3.1", "source": "requirements.txt"}
            ]
        }

        # When
        result, prompt_tokens, completion_tokens = await workflow._assess(
            input_data, ModelTier.CAPABLE
        )

        # Then
        vulnerabilities = result["vulnerabilities"]
        assert len(vulnerabilities) == 1
        assert vulnerabilities[0]["severity"] == "critical"
        assert "CVE" in vulnerabilities[0]["cve"]


# Test _report stage


class TestReportStage:
    """Test report stage functionality."""

    @pytest.mark.asyncio
    async def test_given_