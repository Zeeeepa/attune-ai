"""Behavioral tests for core.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import pytest
from datetime import datetime
from empathy_os.workflows.progressive.core import (
    Tier,
    FailureAnalysis,
)


class TestTierEnum:
    """Behavioral tests for the Tier enumeration."""

    def test_tier_enum_values(self):
        """Given tier enum, when accessing values, then returns correct strings."""
        # Given / When / Then
        assert Tier.CHEAP.value == "cheap"
        assert Tier.CAPABLE.value == "capable"
        assert Tier.PREMIUM.value == "premium"

    def test_tier_comparison_cheap_less_than_capable(self):
        """Given two tiers, when comparing CHEAP to CAPABLE, then CHEAP is less."""
        # Given
        cheap = Tier.CHEAP
        capable = Tier.CAPABLE

        # When / Then
        assert cheap < capable

    def test_tier_comparison_cheap_less_than_premium(self):
        """Given two tiers, when comparing CHEAP to PREMIUM, then CHEAP is less."""
        # Given
        cheap = Tier.CHEAP
        premium = Tier.PREMIUM

        # When / Then
        assert cheap < premium

    def test_tier_comparison_capable_less_than_premium(self):
        """Given two tiers, when comparing CAPABLE to PREMIUM, then CAPABLE is less."""
        # Given
        capable = Tier.CAPABLE
        premium = Tier.PREMIUM

        # When / Then
        assert capable < premium

    def test_tier_comparison_not_less_than_equal(self):
        """Given same tier, when comparing to itself, then not less than."""
        # Given
        cheap = Tier.CHEAP

        # When / Then
        assert not (cheap < cheap)

    def test_tier_comparison_reverse_order(self):
        """Given two tiers, when comparing in reverse order, then returns false."""
        # Given
        cheap = Tier.CHEAP
        premium = Tier.PREMIUM

        # When / Then
        assert not (premium < cheap)

    def test_tier_comparison_capable_not_less_than_cheap(self):
        """Given two tiers, when comparing CAPABLE to CHEAP, then not less than."""
        # Given
        capable = Tier.CAPABLE
        cheap = Tier.CHEAP

        # When / Then
        assert not (capable < cheap)

    def test_tier_enum_membership(self):
        """Given tier values, when checking membership, then all are in Tier enum."""
        # Given / When / Then
        assert Tier.CHEAP in Tier
        assert Tier.CAPABLE in Tier
        assert Tier.PREMIUM in Tier

    def test_tier_enum_iteration(self):
        """Given Tier enum, when iterating, then yields all tier values."""
        # Given
        tiers = list(Tier)

        # When / Then
        assert len(tiers) == 3
        assert Tier.CHEAP in tiers
        assert Tier.CAPABLE in tiers
        assert Tier.PREMIUM in tiers


class TestFailureAnalysisInitialization:
    """Behavioral tests for FailureAnalysis initialization."""

    def test_failure_analysis_default_initialization(self):
        """Given no arguments, when creating FailureAnalysis, then uses default values."""
        # When
        analysis = FailureAnalysis()

        # Then
        assert analysis.syntax_errors == []
        assert analysis.test_failures == []
        assert analysis.test_pass_rate == 0.0
        assert analysis.coverage_percent == 0.0
        assert analysis.assertion_depth == 0.0
        assert analysis.confidence_score == 0.0
        assert analysis.llm_uncertainty_signals == []

    def test_failure_analysis_with_partial_initialization(self):
        """Given partial arguments, when creating FailureAnalysis, then uses provided and default values."""
        # When
        analysis = FailureAnalysis(
            test_pass_rate=0.85,
            coverage_percent=78.0
        )

        # Then
        assert analysis.test_pass_rate == 0.85
        assert analysis.coverage_percent == 78.0
        assert analysis.syntax_errors == []
        assert analysis.assertion_depth == 0.0

    def test_failure_analysis_with_all_values(self):
        """Given all arguments, when creating FailureAnalysis, then stores all values."""
        # Given
        syntax_error = SyntaxError("test error")
        test_failures = [{"test": "failed", "reason": "assertion"}]
        uncertainty = ["might", "possibly"]

        # When
        analysis = FailureAnalysis(
            syntax_errors=[syntax_error],
            test_failures=test_failures,
            test_pass_rate=0.75,
            coverage_percent=85.5,
            assertion_depth=3.2,
            confidence_score=0.88,
            llm_uncertainty_signals=uncertainty
        )

        # Then
        assert len(analysis.syntax_errors) == 1
        assert analysis.syntax_errors[0] == syntax_error
        assert analysis.test_failures == test_failures
        assert analysis.test_pass_rate == 0.75
        assert analysis.coverage_percent == 85.5
        assert analysis.assertion_depth == 3.2
        assert analysis.confidence_score == 0.88
        assert analysis.llm_uncertainty_signals == uncertainty


class TestFailureAnalysisQualityScore:
    """Behavioral tests for FailureAnalysis quality score calculation."""

    def test_calculate_quality_score_with_perfect_metrics(self):
        """Given perfect metrics, when calculating quality score, then returns high score."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=1.0,
            coverage_percent=100.0,
            assertion_depth=10.0,
            confidence_score=1.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        assert score == pytest.approx(100.0, rel=0.01)

    def test_calculate_quality_score_with_zero_metrics(self):
        """Given zero metrics, when calculating quality score, then returns zero score."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.0,
            coverage_percent=0.0,
            assertion_depth=0.0,
            confidence_score=0.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        assert score == pytest.approx(0.0, rel=0.01)

    def test_calculate_quality_score_with_moderate_metrics(self):
        """Given moderate metrics, when calculating quality score, then returns moderate score."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.85,
            coverage_percent=78.0,
            assertion_depth=5.2,
            confidence_score=0.92
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # Expected: (0.40 * 85) + (0.25 * 78) + (0.20 * 52) + (0.15 * 92)
        # = 34 + 19.5 + 10.4 + 13.8 = 77.7
        assert score == pytest.approx(77.7, rel=0.01)

    def test_calculate_quality_score_with_syntax_errors_penalty(self):
        """Given syntax errors, when calculating quality score, then applies penalty."""
        # Given
        syntax_error = SyntaxError("test error")
        analysis = FailureAnalysis(
            syntax_errors=[syntax_error],
            test_pass_rate=1.0,
            coverage_percent=100.0,
            assertion_depth=10.0,
            confidence_score=1.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # Should be less than perfect due to syntax error penalty
        assert score < 100.0

    def test_calculate_quality_score_with_multiple_syntax_errors(self):
        """Given multiple syntax errors, when calculating quality score, then applies stronger penalty."""
        # Given
        analysis_one_error = FailureAnalysis(
            syntax_errors=[SyntaxError("error1")],
            test_pass_rate=1.0,
            coverage_percent=100.0,
            assertion_depth=10.0,
            confidence_score=1.0
        )
        analysis_two_errors = FailureAnalysis(
            syntax_errors=[SyntaxError("error1"), SyntaxError("error2")],
            test_pass_rate=1.0,
            coverage_percent=100.0,
            assertion_depth=10.0,
            confidence_score=1.0
        )

        # When
        score_one = analysis_one_error.calculate_quality_score()
        score_two = analysis_two_errors.calculate_quality_score()

        # Then
        assert score_two < score_one

    def test_calculate_quality_score_test_pass_rate_weight(self):
        """Given high test pass rate only, when calculating quality score, then reflects 40% weight."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=1.0,
            coverage_percent=0.0,
            assertion_depth=0.0,
            confidence_score=0.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # 40% of 100 = 40
        assert score == pytest.approx(40.0, rel=0.01)

    def test_calculate_quality_score_coverage_weight(self):
        """Given high coverage only, when calculating quality score, then reflects 25% weight."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.0,
            coverage_percent=100.0,
            assertion_depth=0.0,
            confidence_score=0.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # 25% of 100 = 25
        assert score == pytest.approx(25.0, rel=0.01)

    def test_calculate_quality_score_assertion_depth_weight(self):
        """Given high assertion depth only, when calculating quality score, then reflects 20% weight."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.0,
            coverage_percent=0.0,
            assertion_depth=10.0,  # 100% normalized
            confidence_score=0.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # 20% of 100 = 20
        assert score == pytest.approx(20.0, rel=0.01)

    def test_calculate_quality_score_confidence_weight(self):
        """Given high confidence only, when calculating quality score, then reflects 15% weight."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.0,
            coverage_percent=0.0,
            assertion_depth=0.0,
            confidence_score=1.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # 15% of 100 = 15
        assert score == pytest.approx(15.0, rel=0.01)

    def test_calculate_quality_score_with_high_assertion_depth(self):
        """Given assertion depth above 10, when calculating quality score, then caps at 100%."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.0,
            coverage_percent=0.0,
            assertion_depth=20.0,  # Above max of 10
            confidence_score=0.0
        )

        # When
        score = analysis.calculate_quality_score()

        # Then
        # Should still be 20% (capped at 100% of assertion metric)
        assert score == pytest.approx(20.0, rel=0.01)


class TestFailureAnalysisShouldEscalate:
    """Behavioral tests for FailureAnalysis escalation decision."""

    def test_should_escalate_with_low_quality_score(self):
        """Given low quality score, when checking should_escalate, then returns True."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.5,
            coverage_percent=40.0,
            assertion_depth=2.0,
            confidence_score=0.5
        )

        # When
        quality_score = analysis.calculate_quality_score()

        # Then
        # Assuming threshold is around 75-80
        assert quality_score < 75.0

    def test_should_escalate_with_high_quality_score(self):
        """Given high quality score, when checking should_escalate, then returns False."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=0.95,
            coverage_percent=90.0,
            assertion_depth=8.0,
            confidence_score=0.95
        )

        # When
        quality_score = analysis.calculate_quality_score()

        # Then
        assert quality_score > 80.0

    def test_should_escalate_with_syntax_errors(self):
        """Given syntax errors present, when checking should_escalate, then likely returns True."""
        # Given
        analysis = FailureAnalysis(
            syntax_errors=[SyntaxError("error")],
            test_pass_rate=0.8,
            coverage_percent=80.0,
            assertion_depth=5.0,
            confidence_score=0.8
        )

        # When
        quality_score = analysis.calculate_quality_score()

        # Then
        # Syntax errors should reduce score significantly
        assert quality_score < 80.0

    def test_should_escalate_with_many_test_failures(self):
        """Given many test failures, when checking should_escalate, then returns True."""
        # Given
        test_failures = [
            {"test": "test1", "reason": "failed"},
            {"test": "test2", "reason": "failed"},
            {"test": "test3", "reason": "failed"},
        ]
        analysis = FailureAnalysis(
            test_failures=test_failures,
            test_pass_rate=0.3,  # 70% failed
            coverage_percent=80.0,
            assertion_depth=5.0,
            confidence_score=0.8
        )

        # When
        quality_score = analysis.calculate_quality_score()

        # Then
        assert quality_score < 70.0

    def test_should_escalate_with_uncertainty_signals(self):
        """Given LLM uncertainty signals, when present, then indicates potential escalation need."""
        # Given
        uncertainty_signals = ["might", "possibly", "not sure", "maybe"]
        analysis = FailureAnalysis(
            llm_uncertainty_signals=uncertainty_signals,
            test_pass_rate=0.8,
            coverage_percent=75.0,
            assertion_depth=5.0,
            confidence_score=0.6  # Low confidence matches uncertainty
        )

        # When
        quality_score = analysis.calculate_quality_score()

        # Then
        # Low confidence should result in lower score
        assert quality_score < 80.0
        assert len(analysis.llm_uncertainty_signals) == 4


class TestFailureAnalysisEdgeCases:
    """Behavioral tests for FailureAnalysis edge cases."""

    def test_failure_analysis_with_negative_values_clamped(self):
        """Given negative metric values, when calculating score, then treats as zero."""
        # Given
        analysis = FailureAnalysis(
            test_pass_rate=-0.