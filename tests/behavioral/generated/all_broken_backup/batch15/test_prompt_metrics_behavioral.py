"""Behavioral tests for prompt_metrics.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest

from empathy_os.metrics.prompt_metrics import MetricsTracker, PromptMetrics


# Fixtures
@pytest.fixture
def sample_metrics():
    """Given a sample PromptMetrics instance."""
    return PromptMetrics(
        timestamp="2026-01-15T10:30:00.000000",
        workflow="code_review",
        agent_role="Code Reviewer",
        task_description="Review Python code for quality issues",
        model="gpt-4",
        prompt_tokens=1500,
        completion_tokens=500,
        total_tokens=2000,
        latency_ms=2500.5,
        retry_count=0,
        parsing_success=True,
        validation_success=True,
        error_message=None,
        xml_structure_used=True,
    )


@pytest.fixture
def sample_metrics_with_errors():
    """Given a sample PromptMetrics instance with errors."""
    return PromptMetrics(
        timestamp="2026-01-15T11:00:00.000000",
        workflow="bug_predict",
        agent_role="Bug Predictor",
        task_description="Analyze code for potential bugs",
        model="claude-sonnet",
        prompt_tokens=2000,
        completion_tokens=0,
        total_tokens=2000,
        latency_ms=1000.0,
        retry_count=3,
        parsing_success=False,
        validation_success=False,
        error_message="Parsing failed: Invalid XML",
        xml_structure_used=True,
    )


@pytest.fixture
def temp_metrics_file(tmp_path):
    """Given a temporary metrics file path."""
    return tmp_path / ".empathy" / "prompt_metrics.json"


@pytest.fixture
def metrics_tracker(temp_metrics_file):
    """Given a MetricsTracker instance with temporary file."""
    return MetricsTracker(metrics_file=str(temp_metrics_file))


# PromptMetrics Tests
class TestPromptMetrics:
    """Behavioral tests for PromptMetrics dataclass."""

    def test_to_dict_converts_all_fields(self, sample_metrics):
        """
        Given a PromptMetrics instance
        When to_dict is called
        Then all fields should be converted to dictionary
        """
        result = sample_metrics.to_dict()

        assert isinstance(result, dict)
        assert result["timestamp"] == "2026-01-15T10:30:00.000000"
        assert result["workflow"] == "code_review"
        assert result["agent_role"] == "Code Reviewer"
        assert result["task_description"] == "Review Python code for quality issues"
        assert result["model"] == "gpt-4"
        assert result["prompt_tokens"] == 1500
        assert result["completion_tokens"] == 500
        assert result["total_tokens"] == 2000
        assert result["latency_ms"] == 2500.5
        assert result["retry_count"] == 0
        assert result["parsing_success"] is True
        assert result["validation_success"] is True
        assert result["error_message"] is None
        assert result["xml_structure_used"] is True

    def test_to_dict_handles_none_values(self, sample_metrics_with_errors):
        """
        Given a PromptMetrics instance with None values
        When to_dict is called
        Then None values should be preserved in dictionary
        """
        sample_metrics_with_errors.validation_success = None

        result = sample_metrics_with_errors.to_dict()

        assert result["validation_success"] is None
        assert result["error_message"] == "Parsing failed: Invalid XML"

    def test_from_dict_creates_instance(self, sample_metrics):
        """
        Given a dictionary with metric data
        When from_dict is called
        Then a PromptMetrics instance should be created
        """
        data = sample_metrics.to_dict()

        result = PromptMetrics.from_dict(data)

        assert isinstance(result, PromptMetrics)
        assert result.timestamp == sample_metrics.timestamp
        assert result.workflow == sample_metrics.workflow
        assert result.total_tokens == sample_metrics.total_tokens

    def test_from_dict_with_none_values(self):
        """
        Given a dictionary with None values
        When from_dict is called
        Then a PromptMetrics instance with None values should be created
        """
        data = {
            "timestamp": "2026-01-15T10:30:00.000000",
            "workflow": "test",
            "agent_role": "Test Agent",
            "task_description": "Test task",
            "model": "gpt-4",
            "prompt_tokens": 100,
            "completion_tokens": 50,
            "total_tokens": 150,
            "latency_ms": 100.0,
            "retry_count": 0,
            "parsing_success": True,
            "validation_success": None,
            "error_message": None,
            "xml_structure_used": False,
        }

        result = PromptMetrics.from_dict(data)

        assert result.validation_success is None
        assert result.error_message is None

    def test_round_trip_serialization(self, sample_metrics):
        """
        Given a PromptMetrics instance
        When converting to dict and back
        Then the result should match the original
        """
        data = sample_metrics.to_dict()
        result = PromptMetrics.from_dict(data)

        assert result.timestamp == sample_metrics.timestamp
        assert result.workflow == sample_metrics.workflow
        assert result.total_tokens == sample_metrics.total_tokens
        assert result.latency_ms == sample_metrics.latency_ms


# MetricsTracker Tests
class TestMetricsTrackerInitialization:
    """Behavioral tests for MetricsTracker initialization."""

    def test_init_creates_parent_directory(self, tmp_path):
        """
        Given a metrics file path with non-existent parent directory
        When MetricsTracker is initialized
        Then the parent directory should be created
        """
        metrics_file = tmp_path / "nested" / "dir" / "metrics.json"

        tracker = MetricsTracker(metrics_file=str(metrics_file))

        assert tracker.metrics_file.parent.exists()

    def test_init_creates_empty_file_if_not_exists(self, temp_metrics_file):
        """
        Given a non-existent metrics file
        When MetricsTracker is initialized
        Then an empty file should be created
        """
        tracker = MetricsTracker(metrics_file=str(temp_metrics_file))

        assert tracker.metrics_file.exists()
        assert tracker.metrics_file.stat().st_size == 0

    def test_init_preserves_existing_file(self, temp_metrics_file):
        """
        Given an existing metrics file with data
        When MetricsTracker is initialized
        Then the existing data should be preserved
        """
        temp_metrics_file.parent.mkdir(parents=True, exist_ok=True)
        temp_metrics_file.write_text('{"test": "data"}\n')

        tracker = MetricsTracker(metrics_file=str(temp_metrics_file))

        content = tracker.metrics_file.read_text()
        assert '{"test": "data"}' in content

    def test_init_with_default_path(self):
        """
        Given no metrics file path
        When MetricsTracker is initialized
        Then default path should be used
        """
        with patch("empathy_os.metrics.prompt_metrics.Path") as mock_path:
            mock_instance = MagicMock()
            mock_path.return_value = mock_instance
            mock_instance.parent.mkdir = MagicMock()
            mock_instance.exists.return_value = True

            tracker = MetricsTracker()

            mock_path.assert_called_once_with(".empathy/prompt_metrics.json")


class TestMetricsTrackerLogMetric:
    """Behavioral tests for logging metrics."""

    def test_log_metric_appends_to_file(self, metrics_tracker, sample_metrics):
        """
        Given a metrics tracker and a metric
        When log_metric is called
        Then the metric should be appended to the file in JSON Lines format
        """
        metrics_tracker.log_metric(sample_metrics)

        content = metrics_tracker.metrics_file.read_text()
        lines = content.strip().split("\n")
        assert len(lines) == 1

        logged_data = json.loads(lines[0])
        assert logged_data["workflow"] == "code_review"
        assert logged_data["total_tokens"] == 2000

    def test_log_metric_appends_multiple_entries(
        self, metrics_tracker, sample_metrics, sample_metrics_with_errors
    ):
        """
        Given a metrics tracker
        When multiple metrics are logged
        Then all metrics should be appended in order
        """
        metrics_tracker.log_metric(sample_metrics)
        metrics_tracker.log_metric(sample_metrics_with_errors)

        content = metrics_tracker.metrics_file.read_text()
        lines = content.strip().split("\n")
        assert len(lines) == 2

        data1 = json.loads(lines[0])
        data2 = json.loads(lines[1])
        assert data1["workflow"] == "code_review"
        assert data2["workflow"] == "bug_predict"

    def test_log_metric_handles_file_write_error(
        self, metrics_tracker, sample_metrics, caplog
    ):
        """
        Given a metrics tracker with write permissions error
        When log_metric is called
        Then an error should be logged and exception raised
        """
        with patch("builtins.open", side_effect=PermissionError("Access denied")):
            with pytest.raises(PermissionError):
                metrics_tracker.log_metric(sample_metrics)

    def test_log_metric_with_none_values(self, metrics_tracker):
        """
        Given a metric with None values
        When log_metric is called
        Then None values should be serialized correctly
        """
        metric = PromptMetrics(
            timestamp="2026-01-15T10:30:00.000000",
            workflow="test",
            agent_role="Test",
            task_description="Test",
            model="gpt-4",
            prompt_tokens=100,
            completion_tokens=50,
            total_tokens=150,
            latency_ms=100.0,
            retry_count=0,
            parsing_success=True,
            validation_success=None,
            error_message=None,
            xml_structure_used=True,
        )

        metrics_tracker.log_metric(metric)

        content = metrics_tracker.metrics_file.read_text()
        logged_data = json.loads(content.strip())
        assert logged_data["validation_success"] is None
        assert logged_data["error_message"] is None


class TestMetricsTrackerReadMetrics:
    """Behavioral tests for reading metrics."""

    def test_read_metrics_returns_all_entries(
        self, metrics_tracker, sample_metrics, sample_metrics_with_errors
    ):
        """
        Given a metrics file with multiple entries
        When read_metrics is called
        Then all entries should be returned as PromptMetrics instances
        """
        metrics_tracker.log_metric(sample_metrics)
        metrics_tracker.log_metric(sample_metrics_with_errors)

        result = metrics_tracker.read_metrics()

        assert len(result) == 2
        assert all(isinstance(m, PromptMetrics) for m in result)
        assert result[0].workflow == "code_review"
        assert result[1].workflow == "bug_predict"

    def test_read_metrics_returns_empty_list_for_empty_file(self, metrics_tracker):
        """
        Given an empty metrics file
        When read_metrics is called
        Then an empty list should be returned
        """
        result = metrics_tracker.read_metrics()

        assert result == []

    def test_read_metrics_skips_invalid_json_lines(
        self, metrics_tracker, sample_metrics, caplog
    ):
        """
        Given a metrics file with invalid JSON lines
        When read_metrics is called
        Then invalid lines should be skipped with warning
        """
        metrics_tracker.log_metric(sample_metrics)
        with open(metrics_tracker.metrics_file, "a") as f:
            f.write("invalid json line\n")
        metrics_tracker.log_metric(sample_metrics)

        with caplog.at_level(logging.WARNING):
            result = metrics_tracker.read_metrics()

        assert len(result) == 2
        assert "Failed to parse metric line" in caplog.text

    def test_read_metrics_handles_file_not_found(self, tmp_path):
        """
        Given a non-existent metrics file
        When read_metrics is called
        Then an empty list should be returned
        """
        tracker = MetricsTracker(metrics_file=str(tmp_path / "nonexistent.json"))
        tracker.metrics_file.unlink()  # Ensure file doesn't exist

        result = tracker.read_metrics()

        assert result == []

    def test_read_metrics_handles_read_permission_error(
        self, metrics_tracker, sample_metrics, caplog
    ):
        """
        Given a metrics file with read permission error
        When read_metrics is called
        Then error should be logged and empty list returned
        """
        metrics_tracker.log_metric(sample_metrics)

        with patch("builtins.open", side_effect=PermissionError("Access denied")):
            with caplog.at_level(logging.ERROR):
                result = metrics_tracker.read_metrics()

        assert result == []
        assert "Error reading metrics file" in caplog.text


class TestMetricsTrackerGetSummary:
    """Behavioral tests for getting metric summaries."""

    def test_get_summary_all_metrics(self, metrics_tracker):
        """
        Given multiple metrics
        When get_summary is called without filters
        Then summary should include all metrics
        """
        for i in range(3):
            metric = PromptMetrics(
                timestamp=f"2026-01-15T10:30:0{i}.000000",
                workflow="code_review",
                agent_role="Reviewer",
                task_description="Review",
                model="gpt-4",
                prompt_tokens=1000 + i * 100,
                completion_tokens=500 + i * 50,
                total_tokens=1500 + i * 150,
                latency_ms=1000.0 + i * 100,
                retry_count=i,
                parsing_success=True,
                validation_success=True,
                error_message=None,
                xml_structure_used=True,
            )
            metrics_tracker.log_metric(metric)

        summary = metrics_tracker.get_summary()

        assert summary["total_count"] == 3
        assert summary["avg_tokens"] == pytest.approx(1650.0)
        assert summary["avg_latency_ms"] == pytest.approx(1100.0)
        assert summary["success_rate"] == 1.0

    def test_get_summary_with_workflow_filter(self, metrics_tracker, sample_metrics):
        """
        Given metrics from different workflows
        When get_summary is called with workflow filter
        Then only matching workflow metrics should be included
        """
        metrics_tracker.log_metric(sample_metrics)

        other_metric =