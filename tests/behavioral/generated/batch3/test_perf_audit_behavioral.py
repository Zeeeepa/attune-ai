"""Behavioral tests for perf_audit.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import re
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch, mock_open

import pytest

from empathy_os.workflows.perf_audit import (
    PERF_AUDIT_STEPS,
    PERF_PATTERNS,
    PerformanceAuditWorkflow,
)
from empathy_os.workflows.base import ModelTier
from empathy_os.workflows.output import Finding, WorkflowReport


@pytest.fixture
def mock_console():
    """Fixture for mocked console output."""
    with patch("empathy_os.workflows.perf_audit.get_console") as mock:
        console = Mock()
        mock.return_value = console
        yield console


@pytest.fixture
def sample_code_with_issues():
    """Fixture providing sample code with performance issues."""
    return """
import time
import requests
from collections import *

async def fetch_data():
    time.sleep(1)
    data = requests.get("http://example.com")
    return data

def process_users(users):
    result = ""
    for user in users:
        result += user.name + ","
    return result

def query_orders(customer_ids):
    orders = []
    for cid in customer_ids:
        orders.append(db.query(Order).filter(Order.customer_id == cid).first())
    return orders

def search_text(texts):
    results = []
    for text in texts:
        match = re.search(r'\\d+', text)
        if match:
            results.append(match.group())
    return results

def nested_loop(items, tags):
    for item in items:
        for tag in tags:
            if item.tag == tag:
                process(item)
"""


@pytest.fixture
def sample_clean_code():
    """Fixture providing sample code without performance issues."""
    return """
import asyncio

async def fetch_data():
    await asyncio.sleep(1)
    return await http_client.get("http://example.com")

def process_users(users):
    return ",".join(user.name for user in users)

PATTERN = re.compile(r'\\d+')

def search_text(texts):
    return [match.group() for text in texts if (match := PATTERN.search(text))]
"""


@pytest.fixture
def perf_audit_workflow():
    """Fixture for PerformanceAuditWorkflow instance."""
    return PerformanceAuditWorkflow()


class TestPerfAuditSteps:
    """Tests for PERF_AUDIT_STEPS configuration."""

    def test_given_perf_audit_steps_when_accessed_then_contains_optimize_step(self):
        """Given PERF_AUDIT_STEPS configuration
        When accessing the steps
        Then it should contain optimize step configuration.
        """
        # When
        optimize_step = PERF_AUDIT_STEPS.get("optimize")

        # Then
        assert optimize_step is not None
        assert optimize_step.name == "optimize"
        assert optimize_step.task_type == "final_review"
        assert optimize_step.tier_hint == "premium"
        assert optimize_step.max_tokens == 3000

    def test_given_perf_audit_steps_when_checking_description_then_has_meaningful_text(self):
        """Given PERF_AUDIT_STEPS configuration
        When checking description
        Then it should have meaningful description text.
        """
        # When
        optimize_step = PERF_AUDIT_STEPS["optimize"]

        # Then
        assert "optimization" in optimize_step.description.lower()
        assert "performance" in optimize_step.description.lower()


class TestPerfPatterns:
    """Tests for PERF_PATTERNS configuration."""

    def test_given_perf_patterns_when_accessed_then_contains_all_expected_patterns(self):
        """Given PERF_PATTERNS configuration
        When accessed
        Then it should contain all expected pattern types.
        """
        # Then
        expected_patterns = [
            "n_plus_one",
            "sync_in_async",
            "list_comprehension_in_loop",
            "string_concat_loop",
            "global_import",
            "large_list_copy",
            "repeated_regex",
            "nested_loops",
        ]
        for pattern_name in expected_patterns:
            assert pattern_name in PERF_PATTERNS

    def test_given_pattern_config_when_checking_structure_then_has_required_fields(self):
        """Given pattern configuration
        When checking structure
        Then each pattern should have required fields.
        """
        # When/Then
        for pattern_name, pattern_config in PERF_PATTERNS.items():
            assert "patterns" in pattern_config
            assert "description" in pattern_config
            assert "impact" in pattern_config
            assert isinstance(pattern_config["patterns"], list)
            assert len(pattern_config["patterns"]) > 0

    def test_given_pattern_regex_when_compiled_then_no_errors(self):
        """Given pattern regex strings
        When compiling them
        Then they should compile without errors.
        """
        # When/Then
        for pattern_name, pattern_config in PERF_PATTERNS.items():
            for regex_pattern in pattern_config["patterns"]:
                try:
                    re.compile(regex_pattern, re.MULTILINE)
                except re.error as e:
                    pytest.fail(f"Pattern {pattern_name} failed to compile: {regex_pattern}, error: {e}")


class TestPerformanceAuditWorkflowInit:
    """Tests for PerformanceAuditWorkflow initialization."""

    def test_given_no_args_when_initialized_then_creates_instance(self):
        """Given no arguments
        When PerformanceAuditWorkflow is initialized
        Then it should create instance successfully.
        """
        # When
        workflow = PerformanceAuditWorkflow()

        # Then
        assert workflow is not None
        assert isinstance(workflow, PerformanceAuditWorkflow)

    def test_given_initialized_when_checking_name_then_returns_performance_audit(self):
        """Given initialized workflow
        When checking name
        Then it should return 'performance_audit'.
        """
        # Given
        workflow = PerformanceAuditWorkflow()

        # When
        name = workflow.name

        # Then
        assert name == "performance_audit"


class TestPerformanceAuditWorkflowProfile:
    """Tests for profile stage (static analysis)."""

    @pytest.mark.asyncio
    async def test_given_code_with_n_plus_one_when_profiling_then_detects_issue(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with N+1 query pattern
        When profiling
        Then it should detect the issue.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "N+1" in f.description or "query" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_code_with_sync_in_async_when_profiling_then_detects_issue(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with synchronous operation in async context
        When profiling
        Then it should detect the issue.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "async" in f.description.lower() and "synchronous" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_code_with_string_concat_when_profiling_then_detects_issue(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with string concatenation in loop
        When profiling
        Then it should detect the issue.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "string" in f.description.lower() and "loop" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_code_with_wildcard_import_when_profiling_then_detects_issue(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with wildcard import
        When profiling
        Then it should detect the issue.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "import" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_code_with_repeated_regex_when_profiling_then_detects_issue(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with non-precompiled regex
        When profiling
        Then it should detect the issue.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "regex" in f.description.lower() or "pattern" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_clean_code_when_profiling_then_finds_fewer_issues(
        self, perf_audit_workflow, sample_clean_code, mock_console
    ):
        """Given clean code without obvious issues
        When profiling
        Then it should find fewer or no issues.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_clean_code, target_path="test.py")

        # Then
        assert result is not None
        # Clean code should have significantly fewer findings
        assert len(result.findings) < 5

    @pytest.mark.asyncio
    async def test_given_empty_code_when_profiling_then_handles_gracefully(
        self, perf_audit_workflow, mock_console
    ):
        """Given empty code
        When profiling
        Then it should handle gracefully without errors.
        """
        # When
        result = await perf_audit_workflow.run(source="", target_path="test.py")

        # Then
        assert result is not None
        assert isinstance(result, WorkflowReport)

    @pytest.mark.asyncio
    async def test_given_malformed_code_when_profiling_then_handles_gracefully(
        self, perf_audit_workflow, mock_console
    ):
        """Given malformed code
        When profiling
        Then it should handle gracefully without crashing.
        """
        # Given
        malformed_code = "def incomplete_function(\n    # missing closing"

        # When
        result = await perf_audit_workflow.run(source=malformed_code, target_path="test.py")

        # Then
        assert result is not None
        assert isinstance(result, WorkflowReport)


class TestPerformanceAuditWorkflowAnalyze:
    """Tests for analyze stage (algorithmic complexity)."""

    @pytest.mark.asyncio
    async def test_given_code_with_nested_loops_when_analyzing_then_detects_complexity(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with nested loops
        When analyzing
        Then it should detect complexity issues.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        findings = [f for f in result.findings if "nested" in f.description.lower() or "loop" in f.description.lower()]
        assert len(findings) > 0

    @pytest.mark.asyncio
    async def test_given_multiple_issues_when_analyzing_then_prioritizes_high_impact(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with multiple performance issues
        When analyzing
        Then it should prioritize high impact issues.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        assert len(result.findings) > 0
        # Check that high impact issues are present
        high_impact_findings = [f for f in result.findings if f.severity in ["high", "critical"]]
        assert len(high_impact_findings) > 0


class TestPerformanceAuditWorkflowHotspots:
    """Tests for hotspots identification stage."""

    @pytest.mark.asyncio
    async def test_given_code_with_multiple_issues_when_identifying_hotspots_then_finds_locations(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given code with multiple performance issues
        When identifying hotspots
        Then it should identify specific locations.
        """
        # When
        result = await perf_audit_workflow.run(source=sample_code_with_issues, target_path="test.py")

        # Then
        assert result is not None
        assert len(result.findings) > 0
        # Check that findings have location information
        findings_with_location = [f for f in result.findings if hasattr(f, "location") or f.file_path]
        assert len(findings_with_location) > 0


class TestPerformanceAuditWorkflowOptimize:
    """Tests for optimization recommendations stage."""

    @pytest.mark.asyncio
    async def test_given_findings_when_generating_optimizations_then_provides_recommendations(
        self, perf_audit_workflow, sample_code_with_issues, mock_console
    ):
        """Given findings from analysis
        When generating optimizations
        Then it should provide recommendations.
        """
        # When
        result = await perf_audit_workflow.run(
            source=sample_code_with_issues, 
            target_path="test.py",
            force_premium=True
        )

        # Then
        assert result is not None
        assert len(result.findings) > 0
        # Findings should contain actionable information
        for finding in result.findings:
            assert finding.description
            assert len(finding.description) > 10


class TestPerformanceAuditWorkflowIntegration:
    """Integration tests for full workflow execution."""

    @pytest.mark.asyncio
    async def test_given_file_path_when_running_workflow_then_processes_file(
        self, perf_audit_workflow, sample_code_with_issues, tmp_path, mock_console
    ):
        """Given a file path
        When running workflow
        Then it should process the file.
        """
        # Given
        test_file =