"""Behavioral tests for workflow.

Generated by LLM-enhanced test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from attune.workflows.base import ModelTier
from attune.workflows.document_gen.workflow import DocumentGenerationWorkflow


@pytest.fixture
def doc_workflow():
    """Fixture for creating a standard DocumentGenerationWorkflow instance."""
    return DocumentGenerationWorkflow()


@pytest.mark.asyncio
async def test_document_generation_workflow_initialization(doc_workflow):
    """Verify workflow initializes with expected default configurations."""
    assert doc_workflow.name == "doc-gen"
    assert doc_workflow.stages == ["outline", "write", "polish"]
    assert doc_workflow.tier_map == {
        "outline": ModelTier.CHEAP,
        "write": ModelTier.CAPABLE,
        "polish": ModelTier.PREMIUM,
    }


@pytest.mark.asyncio
async def test_document_generation_workflow_custom_initialization():
    """Test custom initialization with non-default parameters."""
    workflow = DocumentGenerationWorkflow(
        skip_polish_threshold=500,
        max_sections=5,
        max_write_tokens=2000,
        section_focus=["API", "Examples"],
        max_cost=3.0,
    )

    assert workflow.skip_polish_threshold == 500
    assert workflow.max_sections == 5
    assert workflow.max_write_tokens == 2000
    assert workflow.section_focus == ["API", "Examples"]
    assert workflow.max_cost == 3.0


@pytest.mark.asyncio
async def test_document_generation_workflow_execute_with_valid_input(doc_workflow):
    """Test successful document generation workflow execution.

    BaseWorkflow.execute() iterates over stages and calls run_stage() for each.
    We mock run_stage to return a (output_dict, input_tokens, output_tokens) tuple
    as required by the BaseWorkflow execute loop.
    """
    mock_source_code = """
    def example_function():
        '''Function docstring'''
        pass
    """

    with patch.object(doc_workflow, "run_stage", new_callable=AsyncMock) as mock_run_stage:
        # run_stage must return (output, input_tokens, output_tokens)
        mock_run_stage.return_value = (
            {"document": "Generated Documentation", "doc_type": "api_reference"},
            100,
            200,
        )

        result = await doc_workflow.execute(
            source_code=mock_source_code, doc_type="api_reference", audience="developers"
        )

        assert mock_run_stage.called
        # execute() is called once per stage (outline, write, polish = 3 calls)
        assert mock_run_stage.call_count == 3
        assert result is not None
        assert result.success is True


@pytest.mark.asyncio
async def test_document_generation_workflow_cost_estimation(doc_workflow):
    """Verify cost estimation and tracking mechanisms."""
    doc_workflow._track_cost = MagicMock()
    doc_workflow._estimate_cost = MagicMock(return_value=2.5)

    estimated_cost = doc_workflow._estimate_cost()
    doc_workflow._track_cost(estimated_cost)

    assert estimated_cost == 2.5
    doc_workflow._track_cost.assert_called_with(2.5)


@pytest.mark.asyncio
async def test_document_generation_workflow_auto_scale_tokens(doc_workflow):
    """Test automatic token scaling mechanism.

    The _auto_scale_tokens formula is:
    - If _user_max_write_tokens is not None, return that value directly.
    - Otherwise: max(16000, min(64000, section_count * 2000))
    So the minimum auto-scaled value is always 16000.
    """
    test_cases = [
        # (max_write_tokens init, num_sections, expected_tokens)
        (None, 5, 16000),  # Auto-scale: max(16000, min(64000, 5*2000=10000)) = 16000
        (2000, 5, 2000),  # Custom token limit preserved (user override)
        (None, 2, 16000),  # Auto-scale: max(16000, min(64000, 2*2000=4000)) = 16000
        (None, 10, 20000),  # Auto-scale: max(16000, min(64000, 10*2000=20000)) = 20000
        (None, 40, 64000),  # Auto-scale: max(16000, min(64000, 40*2000=80000)) = 64000
    ]

    for max_write_tokens, num_sections, expected_tokens in test_cases:
        doc_workflow._user_max_write_tokens = max_write_tokens
        doc_workflow.max_write_tokens = max_write_tokens or 16000
        scaled_tokens = doc_workflow._auto_scale_tokens(num_sections)
        assert scaled_tokens == expected_tokens, (
            f"Failed for max_write_tokens={max_write_tokens}, "
            f"num_sections={num_sections}: got {scaled_tokens}, "
            f"expected {expected_tokens}"
        )


@pytest.mark.parametrize(
    "export_path,should_export",
    [
        (Path("/tmp/test_attune_docs"), True),
        (None, False),
    ],
)
def test_document_generation_workflow_export(doc_workflow, export_path, should_export, tmp_path):
    """Test document export with various path configurations.

    The actual _export_document signature is:
        _export_document(self, document, doc_type, report=None)
    It uses self.export_path (set during __init__) to determine the output directory.
    It writes files using Path.write_text after _validate_file_path validation.
    """
    mock_document = "Sample documentation content"
    doc_type = "api_reference"

    if should_export:
        # Use tmp_path for safe test isolation
        doc_workflow.export_path = tmp_path / "exported_docs"
    else:
        doc_workflow.export_path = None

    doc_path, report_path = doc_workflow._export_document(mock_document, doc_type)

    if should_export:
        assert doc_path is not None
        assert doc_path.exists()
        assert doc_path.read_text(encoding="utf-8") == mock_document
    else:
        assert doc_path is None
        assert report_path is None


@pytest.mark.asyncio
async def test_document_generation_workflow_edge_cases(doc_workflow):
    """Test edge cases and error handling scenarios.

    BaseWorkflow.execute() catches all exceptions and returns a WorkflowResult
    with success=False rather than raising. Verify graceful failure handling.
    """
    with patch.object(doc_workflow, "run_stage", new_callable=AsyncMock) as mock_run_stage:
        mock_run_stage.side_effect = ValueError("Invalid input data")

        result = await doc_workflow.execute(source_code=None, doc_type=None, audience=None)

        assert result is not None
        assert result.success is False
        assert result.error is not None


def test_document_generation_workflow_section_parsing(doc_workflow):
    """Test section parsing functionality.

    _parse_outline_sections expects numbered top-level sections like:
      1. Introduction
      2. Getting Started
    It uses regex: ^(\\d+)\\.\\s+([A-Za-z].*) to match only top-level items.
    Markdown headers (# / ##) are NOT matched.
    """
    test_outline = """
1. Introduction
2. Getting Started
3. Core Concepts
4. Advanced Usage
5. Configuration
6. Performance Tips
    """

    sections = doc_workflow._parse_outline_sections(test_outline)
    assert len(sections) == 6
    assert "Introduction" in sections
    assert "Getting Started" in sections
    assert "Core Concepts" in sections
    assert "Advanced Usage" in sections
    assert "Configuration" in sections
    assert "Performance Tips" in sections


if __name__ == "__main__":
    pytest.main([__file__])
