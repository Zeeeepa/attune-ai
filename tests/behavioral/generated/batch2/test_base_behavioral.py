"""Behavioral tests for base.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import logging
import uuid
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional
from unittest.mock import MagicMock, Mock, PropertyMock, mock_open, patch

import pytest

from empathy_os.models import (
    ExecutionContext,
    LLMExecutor,
    ModelProvider,
    ModelTier,
    TaskRoutingRecord,
    TelemetryBackend,
)
from empathy_os.workflows.base import (
    WORKFLOW_HISTORY_FILE,
    BaseWorkflow,
    CachedResponse,
    CachingMixin,
    ProgressCallback,
    ProgressTracker,
    TelemetryMixin,
)


# Test Fixtures
@pytest.fixture
def mock_llm_executor():
    """Given a mocked LLM executor."""
    executor = Mock(spec=LLMExecutor)
    executor.execute.return_value = {"content": "test response", "tokens": 100}
    return executor


@pytest.fixture
def mock_cache():
    """Given a mocked cache."""
    cache = Mock()
    cache.get.return_value = None
    cache.set.return_value = None
    cache.clear.return_value = None
    return cache


@pytest.fixture
def mock_telemetry_backend():
    """Given a mocked telemetry backend."""
    backend = Mock(spec=TelemetryBackend)
    backend.record_task_routing.return_value = None
    backend.get_routing_stats.return_value = {}
    return backend


@pytest.fixture
def mock_cost_tracker():
    """Given a mocked cost tracker."""
    tracker = Mock()
    tracker.track_usage.return_value = None
    tracker.get_total_cost.return_value = 0.0
    tracker.get_total_tokens.return_value = 0
    tracker.export_to_dict.return_value = {}
    return tracker


@pytest.fixture
def execution_context(mock_llm_executor, mock_cache, mock_telemetry_backend):
    """Given an execution context."""
    return ExecutionContext(
        llm_executor=mock_llm_executor,
        cache=mock_cache,
        telemetry_backend=mock_telemetry_backend,
    )


@pytest.fixture
def sample_workflow_config():
    """Given a sample workflow config."""
    config = Mock()
    config.enable_caching = True
    config.enable_telemetry = True
    config.enable_progress = False
    config.cost_limit = None
    config.timeout = None
    return config


@pytest.fixture
def concrete_workflow(execution_context, sample_workflow_config):
    """Given a concrete workflow implementation."""

    class ConcreteWorkflow(BaseWorkflow):
        def __init__(self, context, config):
            super().__init__(context=context, config=config)

        def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
            """Execute the workflow."""
            return {"result": "success", "input": input_data}

        def _validate_input(self, input_data: Dict[str, Any]) -> bool:
            """Validate input data."""
            return isinstance(input_data, dict)

    return ConcreteWorkflow(context=execution_context, config=sample_workflow_config)


# ModelTier Deprecation Tests
class TestModelTierDeprecation:
    """Tests for deprecated ModelTier enum."""

    def test_given_deprecated_modeltier_when_imported_then_enum_exists(self):
        """Given deprecated ModelTier is imported,
        When accessed,
        Then it should still exist for backward compatibility."""
        from empathy_os.workflows.base import ModelTier as DeprecatedModelTier

        assert hasattr(DeprecatedModelTier, "CHEAP")
        assert hasattr(DeprecatedModelTier, "SMART")
        assert hasattr(DeprecatedModelTier, "GENIUS")

    def test_given_deprecated_modeltier_when_used_then_values_match(self):
        """Given deprecated ModelTier values,
        When compared with new ModelTier,
        Then values should be compatible."""
        from empathy_os.workflows.base import ModelTier as DeprecatedModelTier

        assert DeprecatedModelTier.CHEAP.value == "cheap"
        assert DeprecatedModelTier.SMART.value == "smart"
        assert DeprecatedModelTier.GENIUS.value == "genius"


# CachingMixin Tests
class TestCachingMixin:
    """Tests for CachingMixin functionality."""

    @pytest.fixture
    def caching_workflow(self, execution_context, sample_workflow_config):
        """Given a workflow with caching mixin."""

        class CachingWorkflow(CachingMixin, BaseWorkflow):
            def __init__(self, context, config):
                BaseWorkflow.__init__(self, context=context, config=config)

            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        return CachingWorkflow(
            context=execution_context, config=sample_workflow_config
        )

    def test_given_cachable_request_when_cache_miss_then_executes_and_caches(
        self, caching_workflow, mock_cache
    ):
        """Given a cacheable request with cache miss,
        When executed,
        Then result should be computed and cached."""
        mock_cache.get.return_value = None

        def computation():
            return {"computed": "result"}

        result = caching_workflow._get_cached_or_compute(
            cache_key="test_key", computation_func=computation
        )

        assert result == {"computed": "result"}
        mock_cache.set.assert_called_once()

    def test_given_cachable_request_when_cache_hit_then_returns_cached(
        self, caching_workflow, mock_cache
    ):
        """Given a cacheable request with cache hit,
        When executed,
        Then cached result should be returned."""
        cached_data = {"cached": "result"}
        mock_cache.get.return_value = json.dumps(cached_data).encode()

        def computation():
            raise AssertionError("Should not be called")

        result = caching_workflow._get_cached_or_compute(
            cache_key="test_key", computation_func=computation
        )

        assert result == cached_data
        mock_cache.set.assert_not_called()

    def test_given_caching_disabled_when_compute_then_always_executes(
        self, caching_workflow, mock_cache
    ):
        """Given caching is disabled,
        When compute is called,
        Then computation should always execute."""
        caching_workflow.config.enable_caching = False

        def computation():
            return {"computed": "result"}

        result = caching_workflow._get_cached_or_compute(
            cache_key="test_key", computation_func=computation
        )

        assert result == {"computed": "result"}
        mock_cache.get.assert_not_called()
        mock_cache.set.assert_not_called()

    def test_given_cache_key_generation_when_called_then_consistent_key(
        self, caching_workflow
    ):
        """Given cache key generation,
        When called with same inputs,
        Then should return consistent key."""
        key1 = caching_workflow._generate_cache_key("step1", {"param": "value"})
        key2 = caching_workflow._generate_cache_key("step1", {"param": "value"})

        assert key1 == key2
        assert isinstance(key1, str)

    def test_given_different_inputs_when_generate_key_then_different_keys(
        self, caching_workflow
    ):
        """Given different inputs,
        When generating cache keys,
        Then keys should be different."""
        key1 = caching_workflow._generate_cache_key("step1", {"param": "value1"})
        key2 = caching_workflow._generate_cache_key("step1", {"param": "value2"})

        assert key1 != key2

    def test_given_cache_error_when_get_then_falls_back_to_compute(
        self, caching_workflow, mock_cache
    ):
        """Given cache raises error on get,
        When getting cached value,
        Then should fall back to computation."""
        mock_cache.get.side_effect = Exception("Cache error")

        def computation():
            return {"computed": "result"}

        result = caching_workflow._get_cached_or_compute(
            cache_key="test_key", computation_func=computation
        )

        assert result == {"computed": "result"}


# TelemetryMixin Tests
class TestTelemetryMixin:
    """Tests for TelemetryMixin functionality."""

    @pytest.fixture
    def telemetry_workflow(self, execution_context, sample_workflow_config):
        """Given a workflow with telemetry mixin."""

        class TelemetryWorkflow(TelemetryMixin, BaseWorkflow):
            def __init__(self, context, config):
                BaseWorkflow.__init__(self, context=context, config=config)

            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        return TelemetryWorkflow(
            context=execution_context, config=sample_workflow_config
        )

    def test_given_telemetry_enabled_when_record_routing_then_records(
        self, telemetry_workflow, mock_telemetry_backend
    ):
        """Given telemetry is enabled,
        When recording task routing,
        Then should record to backend."""
        telemetry_workflow._record_task_routing(
            task_id="test_task",
            task_type="analysis",
            selected_tier=ModelTier.SMART,
            reasoning="test reasoning",
        )

        mock_telemetry_backend.record_task_routing.assert_called_once()
        call_args = mock_telemetry_backend.record_task_routing.call_args[0][0]
        assert isinstance(call_args, TaskRoutingRecord)
        assert call_args.task_id == "test_task"
        assert call_args.task_type == "analysis"
        assert call_args.selected_tier == ModelTier.SMART

    def test_given_telemetry_disabled_when_record_routing_then_no_record(
        self, telemetry_workflow, mock_telemetry_backend
    ):
        """Given telemetry is disabled,
        When recording task routing,
        Then should not record to backend."""
        telemetry_workflow.config.enable_telemetry = False

        telemetry_workflow._record_task_routing(
            task_id="test_task",
            task_type="analysis",
            selected_tier=ModelTier.SMART,
            reasoning="test reasoning",
        )

        mock_telemetry_backend.record_task_routing.assert_not_called()

    def test_given_no_telemetry_backend_when_record_then_no_error(
        self, telemetry_workflow
    ):
        """Given no telemetry backend,
        When recording task routing,
        Then should not raise error."""
        telemetry_workflow.context.telemetry_backend = None

        # Should not raise
        telemetry_workflow._record_task_routing(
            task_id="test_task",
            task_type="analysis",
            selected_tier=ModelTier.SMART,
            reasoning="test reasoning",
        )

    def test_given_telemetry_backend_error_when_record_then_logs_and_continues(
        self, telemetry_workflow, mock_telemetry_backend, caplog
    ):
        """Given telemetry backend raises error,
        When recording task routing,
        Then should log error and continue."""
        mock_telemetry_backend.record_task_routing.side_effect = Exception(
            "Backend error"
        )

        with caplog.at_level(logging.ERROR):
            telemetry_workflow._record_task_routing(
                task_id="test_task",
                task_type="analysis",
                selected_tier=ModelTier.SMART,
                reasoning="test reasoning",
            )

        assert "Failed to record task routing" in caplog.text


# ProgressTracker Tests
class TestProgressTracker:
    """Tests for ProgressTracker functionality."""

    def test_given_progress_tracker_when_created_then_initializes(self):
        """Given progress tracker creation,
        When initialized,
        Then should set up with total steps."""
        tracker = ProgressTracker(total_steps=5)

        assert tracker.total_steps == 5
        assert tracker.current_step == 0
        assert tracker.start_time is not None

    def test_given_progress_tracker_when_update_then_increments(self):
        """Given progress tracker,
        When update is called,
        Then should increment current step."""
        tracker = ProgressTracker(total_steps=5)
        tracker.update("Step 1")

        assert tracker.current_step == 1

    def test_given_callback_when_update_then_calls_callback(self):
        """Given progress callback,
        When update is called,
        Then callback should be invoked."""
        callback = Mock(spec=ProgressCallback)
        tracker = ProgressTracker(total_steps=5, callback=callback)

        tracker.update("Step 1", status="running")

        callback.assert_called_once_with(
            current=1, total=5, step_name="Step 1", status="running"
        )

    def test_given_progress_tracker_when_complete_then_marks_done(self):
        """Given progress tracker,
        When complete is called,
        Then should mark as complete."""
        tracker = ProgressTracker(total_steps=5)
        tracker.complete()

        assert tracker.current_step == tracker.total_steps

    def test_given_progress_tracker_when_get_progress_then_returns_percentage(self):
        """Given progress tracker with progress,
        When getting progress,
        Then should return correct percentage."""
        tracker = ProgressTracker(total_steps=4)
        tracker.update("Step 1")
        tracker.update("Step 2")

        progress = tracker.get_progress()
        assert progress == 0.5  # 2/4 = 0.5

    def test_given_progress_tracker_when_zero_total_then_handles_gracefully(self):
        """Given progress tracker with zero total steps,
        When getting progress,
        Then should handle gracefully."""
        tracker = ProgressTracker(total_steps=0)
        progress = tracker.get_progress()

        assert progress == 0.0

    def test_given_progress_tracker_when_elapsed_time_then_calculates(self):
        """Given progress tracker with elapsed time,
        When getting elapsed time,
        Then should calculate duration."""
        tracker = ProgressTracker(total_steps=5)
        elapsed = tracker.get_elapsed_time()

        assert elapsed >= 0.0

    def test_given_callback_error_when_update_then_continues(self, caplog):
        """Given callback that raises error,
        When update is called,
        Then should log error and continue."""

        def bad_callback(*args, **kwargs):
            raise Exception("Callback error")

        tracker = ProgressTracker(total_steps=5, callback=bad_callback)

        with caplog.at_level(logging.ERROR):
            tracker.update("Step 1")

        assert tracker.current_step == 1


# BaseWorkflow Tests
class TestBaseWorkflow:
    """Tests for BaseWorkflow base class."""

    def test_given_workflow_when_created_then_initializes(
        self, concrete_workflow, execution_context
    ):
        """Given workflow creation,
        When initialized,
        Then should set up context and config."""
        assert concrete_workflow.context == execution_context
        assert concrete_workflow.config is not None
        assert concrete_workflow.run_id is not None

    def test_given_workflow_when_execute_then_validates_input(
        self, concrete_workflow, caplog
    ):
        """Given workflow execution,
        When execute is called,
        Then should validate input."""
        with caplog.at_level(logging.INFO):
            result = concrete_workflow.execute({"test": "data"})

        assert result["result"] == "success"

    def test_given_invalid_input_when_execute_then_logs_warning(
        self, concrete_workflow, caplog
    ):
        """Given invalid input,
        When execute is called,
        Then should log validation warning."""
        with patch.object(
            concrete_workflow, "_validate_input", return_value=False
        ), caplog.at_level(logging.WARNING):
            concrete_workflow.execute({"invalid": "data"})

        assert "Input validation failed" in caplog.text

    def test_given_workflow_when_get_cost_summary_then_returns_dict(
        self, concrete_workflow
    ):
        """Given workflow with cost tracker,
        When getting cost summary,
        Then should return dictionary."""
        with patch.object(
            concrete_workflow.cost_tracker,
            "export_to_dict",
            return_value={"total_cost": 1.5},
        ):
            summary = concrete_workflow.get_cost_summary()

        assert isinstance(summary, dict)
        assert "total_cost" in summary

    def test_given_workflow_when_get_run_metadata_then_returns_info(
        self, concrete_workflow
    ):
        """Given workflow,
        When getting run metadata,
        Then should return run information."""
        metadata = concrete_workflow.get_run_metadata()

        assert "run_id" in metadata
        assert "start_time" in metadata
        assert metadata["run_id"] == str(concrete_workflow.run_id)

    def test_given_cost_limit_when_execute_then_checks_limit(self, concrete_workflow):
        """Given cost limit is set,
        When executing workflow,
        Then should check against limit."""
        concrete_workflow.config.cost_limit = 1.0

        with patch.object(
            concrete_workflow.cost_tracker, "get_total_cost", return_value=0.5
        ):
            result = concrete_workflow.execute({"test": "data"})

        assert result["result"] == "success"

    def test_given_cost_exceeded_when_check_then_raises_error(self, concrete_workflow):
        """Given cost exceeds limit,
        When checking cost limit,
        Then should raise RuntimeError."""
        concrete_workflow.config.cost_limit = 1.0

        with patch.object(
            concrete_workflow.cost_tracker, "get_total_cost", return_value=1.5
        ), pytest.raises(RuntimeError, match="Cost limit exceeded"):
            concrete_workflow._check_cost_limit()

    @patch("empathy_os.workflows.base.Path.mkdir")
    @patch("builtins.open", new_callable=mock_open, read_data="[]")
    def test_given_workflow_when_save_run_history_then_writes_file(
        self, mock_file, mock_mkdir, concrete_workflow
    ):
        """Given workflow completion,
        When saving run history,
        Then should write to history file."""
        concrete_workflow._save_run_history(
            {"status": "success", "duration": 1.5, "cost": 0.5}
        )

        mock_mkdir.assert_called_once()
        mock_file.assert_called()

    @patch("empathy_os.workflows.base.Path.mkdir")
    @patch("builtins.open", side_effect=Exception("Write error"))
    def test_given_file_error_when_save_history_then_logs_error(
        self, mock_file, mock_mkdir, concrete_workflow, caplog
    ):
        """Given file write error,
        When saving run history,
        Then should log error and continue."""
        with caplog.at_level(logging.ERROR):
            concrete_workflow._save_run_history({"status": "success"})

        assert "Failed to save workflow run history" in caplog.text

    def test_given_workflow_when_str_then_returns_name(self, concrete_workflow):
        """Given workflow instance,
        When converting to string,
        Then should return workflow name."""
        result = str(concrete_workflow)

        assert "ConcreteWorkflow" in result

    def test_given_workflow_when_repr_then_returns_details(self, concrete_workflow):
        """Given workflow instance,
        When getting repr,
        Then should return detailed representation."""
        result = repr(concrete_workflow)

        assert "ConcreteWorkflow" in result
        assert str(concrete_workflow.run_id) in result


# Integration Tests
class TestWorkflowIntegration:
    """Integration tests for workflow components."""

    @pytest.fixture
    def full_workflow(self, execution_context, sample_workflow_config):
        """Given a fully integrated workflow."""

        class FullWorkflow(CachingMixin, TelemetryMixin, BaseWorkflow):
            def __init__(self, context, config):
                BaseWorkflow.__init__(self, context=context, config=config)

            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                # Simulate caching
                result = self._get_cached_or_compute(
                    cache_key="test_key", computation_func=lambda: {"computed": True}
                )

                # Simulate telemetry
                self._record_task_routing(
                    task_id="task1",
                    task_type="analysis",
                    selected_tier=ModelTier.SMART,
                    reasoning="test",
                )

                return {"result": result}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        return FullWorkflow(context=execution_context, config=sample_workflow_config)

    def test_given_full_workflow_when_execute_then_all_mixins_work(
        self, full_workflow, mock_cache, mock_telemetry_backend
    ):
        """Given fully integrated workflow,
        When executing,
        Then all mixins should function together."""
        mock_cache.get.return_value = None

        result = full_workflow.execute({"input": "data"})

        assert "result" in result
        mock_cache.set.assert_called()
        mock_telemetry_backend.record_task_routing.assert_called()

    def test_given_cached_execution_when_run_then_uses_cache(
        self, full_workflow, mock_cache, mock_telemetry_backend
    ):
        """Given cached execution,
        When running workflow,
        Then should use cached result."""
        cached_result = {"computed": True}
        mock_cache.get.return_value = json.dumps(cached_result).encode()

        result = full_workflow.execute({"input": "data"})

        assert result["result"] == cached_result
        mock_telemetry_backend.record_task_routing.assert_called()

    def test_given_progress_tracking_when_execute_then_tracks_progress(
        self, execution_context, sample_workflow_config
    ):
        """Given progress tracking enabled,
        When executing workflow,
        Then should track progress."""
        sample_workflow_config.enable_progress = True
        callback = Mock()

        class ProgressWorkflow(BaseWorkflow):
            def __init__(self, context, config):
                super().__init__(context=context, config=config)

            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                tracker = ProgressTracker(total_steps=3, callback=callback)
                tracker.update("Step 1")
                tracker.update("Step 2")
                tracker.complete()
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = ProgressWorkflow(
            context=execution_context, config=sample_workflow_config
        )
        result = workflow.execute({"input": "data"})

        assert result["result"] == "success"
        assert callback.call_count >= 2


# Error Handling Tests
class TestWorkflowErrorHandling:
    """Tests for workflow error handling."""

    def test_given_llm_executor_error_when_execute_then_handles_gracefully(
        self, execution_context, sample_workflow_config
    ):
        """Given LLM executor raises error,
        When executing workflow,
        Then should handle gracefully."""

        class ErrorWorkflow(BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                self.context.llm_executor.execute.side_effect = Exception("LLM error")
                try:
                    self.context.llm_executor.execute({"prompt": "test"})
                except Exception as e:
                    return {"error": str(e)}
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = ErrorWorkflow(
            context=execution_context, config=sample_workflow_config
        )
        result = workflow.execute({"input": "data"})

        assert "error" in result

    def test_given_cache_error_when_execute_then_continues(
        self, execution_context, sample_workflow_config, mock_cache
    ):
        """Given cache raises error,
        When executing workflow,
        Then should continue without cache."""
        mock_cache.get.side_effect = Exception("Cache error")

        class CachedWorkflow(CachingMixin, BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                result = self._get_cached_or_compute(
                    cache_key="test", computation_func=lambda: {"computed": True}
                )
                return {"result": result}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = CachedWorkflow(
            context=execution_context, config=sample_workflow_config
        )
        result = workflow.execute({"input": "data"})

        assert result["result"] == {"computed": True}

    def test_given_telemetry_error_when_execute_then_continues(
        self, execution_context, sample_workflow_config, mock_telemetry_backend, caplog
    ):
        """Given telemetry raises error,
        When executing workflow,
        Then should continue execution."""
        mock_telemetry_backend.record_task_routing.side_effect = Exception(
            "Telemetry error"
        )

        class TelemetryWorkflow(TelemetryMixin, BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                self._record_task_routing(
                    task_id="test",
                    task_type="test",
                    selected_tier=ModelTier.SMART,
                    reasoning="test",
                )
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = TelemetryWorkflow(
            context=execution_context, config=sample_workflow_config
        )

        with caplog.at_level(logging.ERROR):
            result = workflow.execute({"input": "data"})

        assert result["result"] == "success"
        assert "Failed to record task routing" in caplog.text


# Edge Cases Tests
class TestWorkflowEdgeCases:
    """Tests for workflow edge cases."""

    def test_given_empty_input_when_execute_then_handles(self, concrete_workflow):
        """Given empty input data,
        When executing workflow,
        Then should handle gracefully."""
        result = concrete_workflow.execute({})

        assert "result" in result

    def test_given_none_config_when_create_then_uses_defaults(self, execution_context):
        """Given None config,
        When creating workflow,
        Then should use default configuration."""

        class DefaultWorkflow(BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = DefaultWorkflow(context=execution_context, config=None)

        assert workflow.config is not None

    def test_given_workflow_when_multiple_executions_then_tracks_separately(
        self, concrete_workflow
    ):
        """Given workflow,
        When executing multiple times,
        Then each execution should be tracked separately."""
        result1 = concrete_workflow.execute({"run": 1})
        result2 = concrete_workflow.execute({"run": 2})

        assert result1["input"]["run"] == 1
        assert result2["input"]["run"] == 2

    def test_given_large_input_when_cache_key_then_handles(self, concrete_workflow):
        """Given large input data,
        When generating cache key,
        Then should handle without error."""
        large_input = {"data": "x" * 10000, "nested": {"key": "value" * 1000}}

        # Should not raise
        key = concrete_workflow._generate_cache_key("step", large_input)
        assert isinstance(key, str)

    def test_given_workflow_when_concurrent_access_then_thread_safe(
        self, execution_context, sample_workflow_config
    ):
        """Given workflow with concurrent access,
        When executed from multiple contexts,
        Then should handle safely."""
        import threading

        results = []

        class ThreadSafeWorkflow(BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                return {"thread_id": threading.current_thread().ident}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return True

        workflow = ThreadSafeWorkflow(
            context=execution_context, config=sample_workflow_config
        )

        def run_workflow():
            result = workflow.execute({"test": "data"})
            results.append(result)

        threads = [threading.Thread(target=run_workflow) for _ in range(5)]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

        assert len(results) == 5


# CachedResponse Tests
class TestCachedResponse:
    """Tests for CachedResponse dataclass."""

    def test_given_cached_response_when_created_then_has_fields(self):
        """Given cached response creation,
        When initialized,
        Then should have required fields."""
        response = CachedResponse(data={"result": "test"}, timestamp=datetime.now())

        assert response.data == {"result": "test"}
        assert isinstance(response.timestamp, datetime)

    def test_given_cached_response_when_serialize_then_json_compatible(self):
        """Given cached response,
        When serializing,
        Then should be JSON compatible."""
        response = CachedResponse(
            data={"result": "test"}, timestamp=datetime.now(), metadata={"cached": True}
        )

        # Should not raise
        json_str = json.dumps(
            {
                "data": response.data,
                "timestamp": response.timestamp.isoformat(),
                "metadata": response.metadata,
            }
        )
        assert "result" in json_str


# Constants and Module Tests
class TestModuleConstants:
    """Tests for module-level constants."""

    def test_given_workflow_history_file_when_accessed_then_has_value(self):
        """Given WORKFLOW_HISTORY_FILE constant,
        When accessed,
        Then should have expected value."""
        assert WORKFLOW_HISTORY_FILE == ".empathy/workflow_runs.json"

    def test_given_module_when_imported_then_has_required_classes(self):
        """Given module import,
        When checking exports,
        Then should have required classes."""
        from empathy_os.workflows import base

        assert hasattr(base, "BaseWorkflow")
        assert hasattr(base, "CachingMixin")
        assert hasattr(base, "TelemetryMixin")
        assert hasattr(base, "ProgressTracker")


# Validation Tests
class TestWorkflowValidation:
    """Tests for workflow input validation."""

    def test_given_valid_dict_input_when_validate_then_returns_true(
        self, concrete_workflow
    ):
        """Given valid dictionary input,
        When validating,
        Then should return True."""
        assert concrete_workflow._validate_input({"key": "value"}) is True

    def test_given_non_dict_input_when_validate_then_returns_false(
        self, concrete_workflow
    ):
        """Given non-dictionary input,
        When validating,
        Then should return False."""
        assert concrete_workflow._validate_input("string") is False
        assert concrete_workflow._validate_input(123) is False
        assert concrete_workflow._validate_input(None) is False

    def test_given_custom_validation_when_override_then_uses_custom(
        self, execution_context, sample_workflow_config
    ):
        """Given custom validation logic,
        When overridden,
        Then should use custom validation."""

        class CustomValidationWorkflow(BaseWorkflow):
            def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
                return {"result": "success"}

            def _validate_input(self, input_data: Dict[str, Any]) -> bool:
                return "required_field" in input_data

        workflow = CustomValidationWorkflow(
            context=execution_context, config=sample_workflow_config
        )

        assert workflow._validate_input({"required_field": "value"}) is True
        assert workflow._validate_input({"other_field": "value"}) is False