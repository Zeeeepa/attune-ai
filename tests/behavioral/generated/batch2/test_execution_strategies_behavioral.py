"""Behavioral tests for execution_strategies.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
import json
import pytest
from dataclasses import asdict
from typing import Any
from unittest.mock import AsyncMock, MagicMock, Mock, patch

from empathy_os.orchestration.execution_strategies import (
    AgentResult,
    StrategyResult,
    ConditionType,
    Condition,
    ConditionalStrategy,
    SequentialStrategy,
    ParallelStrategy,
    DebateStrategy,
    TeachingStrategy,
    RefinementStrategy,
    AdaptiveStrategy,
    ExecutionStrategy,
)


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def mock_agent():
    """Create a mock agent for testing.
    
    Given a mock agent with execute method
    When the agent is called
    Then it returns a structured result
    """
    agent = Mock()
    agent.id = "test_agent_1"
    agent.name = "Test Agent"
    agent.execute = AsyncMock(return_value={
        "status": "success",
        "data": {"result": "test output"},
        "confidence": 0.9
    })
    return agent


@pytest.fixture
def mock_agent_2():
    """Create a second mock agent for multi-agent tests."""
    agent = Mock()
    agent.id = "test_agent_2"
    agent.name = "Test Agent 2"
    agent.execute = AsyncMock(return_value={
        "status": "success",
        "data": {"result": "test output 2"},
        "confidence": 0.85
    })
    return agent


@pytest.fixture
def mock_agent_3():
    """Create a third mock agent for multi-agent tests."""
    agent = Mock()
    agent.id = "test_agent_3"
    agent.name = "Test Agent 3"
    agent.execute = AsyncMock(return_value={
        "status": "success",
        "data": {"result": "test output 3"},
        "confidence": 0.95
    })
    return agent


@pytest.fixture
def failing_agent():
    """Create a mock agent that fails execution."""
    agent = Mock()
    agent.id = "failing_agent"
    agent.name = "Failing Agent"
    agent.execute = AsyncMock(side_effect=Exception("Agent execution failed"))
    return agent


@pytest.fixture
def context():
    """Create a test execution context."""
    return {
        "user_id": "user123",
        "request_id": "req456",
        "input": "test input data",
        "metadata": {"source": "test"}
    }


@pytest.fixture
def agent_result():
    """Create a sample AgentResult."""
    return AgentResult(
        agent_id="agent1",
        success=True,
        output={"result": "test"},
        confidence=0.9,
        duration_seconds=1.5,
        error=""
    )


@pytest.fixture
def strategy_result(agent_result):
    """Create a sample StrategyResult."""
    return StrategyResult(
        success=True,
        outputs=[agent_result],
        aggregated_output={"final": "result"},
        total_duration=2.5,
        errors=[]
    )


# =============================================================================
# AgentResult Tests
# =============================================================================


class TestAgentResult:
    """Behavioral tests for AgentResult dataclass."""

    def test_agent_result_creation(self):
        """Given agent execution data
        When creating an AgentResult
        Then it should store all attributes correctly
        """
        # Given
        agent_id = "agent1"
        success = True
        output = {"key": "value"}
        confidence = 0.95
        duration = 1.23
        error = ""

        # When
        result = AgentResult(
            agent_id=agent_id,
            success=success,
            output=output,
            confidence=confidence,
            duration_seconds=duration,
            error=error
        )

        # Then
        assert result.agent_id == agent_id
        assert result.success == success
        assert result.output == output
        assert result.confidence == confidence
        assert result.duration_seconds == duration
        assert result.error == error

    def test_agent_result_with_error(self):
        """Given a failed agent execution
        When creating an AgentResult with error
        Then it should capture the error message
        """
        # Given
        error_msg = "Connection timeout"

        # When
        result = AgentResult(
            agent_id="agent2",
            success=False,
            output={},
            error=error_msg
        )

        # Then
        assert not result.success
        assert result.error == error_msg
        assert result.confidence == 0.0

    def test_agent_result_default_values(self):
        """Given minimal agent result data
        When creating an AgentResult with defaults
        Then it should use appropriate default values
        """
        # When
        result = AgentResult(
            agent_id="agent3",
            success=True,
            output={"data": "test"}
        )

        # Then
        assert result.confidence == 0.0
        assert result.duration_seconds == 0.0
        assert result.error == ""


# =============================================================================
# StrategyResult Tests
# =============================================================================


class TestStrategyResult:
    """Behavioral tests for StrategyResult dataclass."""

    def test_strategy_result_creation(self, agent_result):
        """Given strategy execution results
        When creating a StrategyResult
        Then it should aggregate all data correctly
        """
        # Given
        outputs = [agent_result]
        aggregated = {"final": "output"}

        # When
        result = StrategyResult(
            success=True,
            outputs=outputs,
            aggregated_output=aggregated,
            total_duration=3.5
        )

        # Then
        assert result.success
        assert result.outputs == outputs
        assert result.aggregated_output == aggregated
        assert result.total_duration == 3.5
        assert result.errors == []

    def test_strategy_result_with_errors(self, agent_result):
        """Given a strategy execution with errors
        When creating a StrategyResult with error list
        Then it should store all errors
        """
        # Given
        errors = ["Error 1", "Error 2"]

        # When
        result = StrategyResult(
            success=False,
            outputs=[agent_result],
            aggregated_output={},
            errors=errors
        )

        # Then
        assert not result.success
        assert result.errors == errors

    def test_strategy_result_post_init(self):
        """Given a StrategyResult without errors
        When post_init is called
        Then it should initialize errors as empty list
        """
        # When
        result = StrategyResult(
            success=True,
            outputs=[],
            aggregated_output={},
            errors=None
        )

        # Then
        assert result.errors == []
        assert isinstance(result.errors, list)


# =============================================================================
# Condition Tests
# =============================================================================


class TestConditionType:
    """Behavioral tests for ConditionType enum."""

    def test_condition_type_values(self):
        """Given the ConditionType enum
        When accessing its values
        Then it should contain expected types
        """
        # Then
        assert hasattr(ConditionType, 'JSON_PREDICATE')
        assert hasattr(ConditionType, 'NATURAL_LANGUAGE')
        assert hasattr(ConditionType, 'COMPOSITE')


class TestCondition:
    """Behavioral tests for Condition class."""

    def test_condition_creation_json_predicate(self):
        """Given a JSON predicate
        When creating a Condition
        Then it should store the predicate correctly
        """
        # Given
        predicate = {"confidence": {"$gt": 0.8}}

        # When
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate=predicate
        )

        # Then
        assert condition.condition_type == ConditionType.JSON_PREDICATE
        assert condition.predicate == predicate

    def test_condition_evaluate_json_gt(self):
        """Given a JSON predicate with $gt operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"confidence": {"$gt": 0.7}}
        )
        context = {"confidence": 0.85}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is True

    def test_condition_evaluate_json_lt(self):
        """Given a JSON predicate with $lt operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"confidence": {"$lt": 0.5}}
        )
        context = {"confidence": 0.85}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is False

    def test_condition_evaluate_json_eq(self):
        """Given a JSON predicate with $eq operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"status": {"$eq": "active"}}
        )
        context = {"status": "active"}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is True

    def test_condition_evaluate_json_gte(self):
        """Given a JSON predicate with $gte operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"score": {"$gte": 90}}
        )
        context = {"score": 90}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is True

    def test_condition_evaluate_json_lte(self):
        """Given a JSON predicate with $lte operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"count": {"$lte": 10}}
        )
        context = {"count": 5}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is True

    def test_condition_evaluate_json_ne(self):
        """Given a JSON predicate with $ne operator
        When evaluating against context
        Then it should return correct boolean result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"status": {"$ne": "failed"}}
        )
        context = {"status": "success"}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is True

    def test_condition_evaluate_missing_field(self):
        """Given a JSON predicate referencing missing field
        When evaluating against context
        Then it should return False
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"missing_field": {"$gt": 0}}
        )
        context = {"other_field": 100}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is False

    def test_condition_evaluate_invalid_operator(self):
        """Given a JSON predicate with invalid operator
        When evaluating against context
        Then it should return False
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"field": {"$invalid": 10}}
        )
        context = {"field": 5}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is False

    def test_condition_evaluate_natural_language_unsupported(self):
        """Given a natural language condition
        When evaluating (not yet implemented)
        Then it should return False
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.NATURAL_LANGUAGE,
            predicate="confidence is high"
        )
        context = {"confidence": 0.9}

        # When
        result = condition.evaluate(context)

        # Then
        assert result is False


# =============================================================================
# SequentialStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestSequentialStrategy:
    """Behavioral tests for SequentialStrategy (Pattern 1: A â†’ B â†’ C)."""

    async def test_sequential_execution_success(self, mock_agent, mock_agent_2, context):
        """Given multiple agents in sequence
        When executing sequentially
        Then each agent should execute in order with previous outputs
        """
        # Given
        strategy = SequentialStrategy()
        agents = [mock_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        assert len(result.outputs) == 2
        assert mock_agent.execute.called
        assert mock_agent_2.execute.called
        assert result.total_duration > 0

    async def test_sequential_execution_empty_agents(self, context):
        """Given no agents
        When executing sequential strategy
        Then it should return empty success result
        """
        # Given
        strategy = SequentialStrategy()
        agents = []

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        assert len(result.outputs) == 0
        assert result.aggregated_output == {}

    async def test_sequential_execution_with_failure(self, mock_agent, failing_agent, context):
        """Given agents where one fails
        When executing sequentially
        Then it should stop and return failure result
        """
        # Given
        strategy = SequentialStrategy()
        agents = [mock_agent, failing_agent]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert not result.success
        assert len(result.errors) > 0
        assert mock_agent.execute.called

    async def test_sequential_passes_context_between_agents(self, mock_agent, mock_agent_2, context):
        """Given sequential agents
        When executing
        Then each agent should receive updated context with previous outputs
        """
        # Given
        strategy = SequentialStrategy()
        agents = [mock_agent, mock_agent_2]

        # When
        await strategy.execute(agents, context)

        # Then
        # First agent gets original context
        first_call = mock_agent.execute.call_args[0][0]
        assert "input" in first_call

        # Second agent gets context with first agent's output
        second_call = mock_agent_2.execute.call_args[0][0]
        assert "previous_output" in second_call or "input" in second_call


# =============================================================================
# ParallelStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestParallelStrategy:
    """Behavioral tests for ParallelStrategy (Pattern 2: A || B || C)."""

    async def test_parallel_execution_success(self, mock_agent, mock_agent_2, mock_agent_3, context):
        """Given multiple agents
        When executing in parallel
        Then all agents should execute concurrently
        """
        # Given
        strategy = ParallelStrategy()
        agents = [mock_agent, mock_agent_2, mock_agent_3]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        assert len(result.outputs) == 3
        assert mock_agent.execute.called
        assert mock_agent_2.execute.called
        assert mock_agent_3.execute.called

    async def test_parallel_execution_empty_agents(self, context):
        """Given no agents
        When executing parallel strategy
        Then it should return empty success result
        """
        # Given
        strategy = ParallelStrategy()
        agents = []

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        assert len(result.outputs) == 0

    async def test_parallel_execution_partial_failure(self, mock_agent, failing_agent, mock_agent_2, context):
        """Given agents where some fail
        When executing in parallel
        Then it should continue and report all results
        """
        # Given
        strategy = ParallelStrategy()
        agents = [mock_agent, failing_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        # Depending on implementation, might be partial success
        assert len(result.outputs) > 0
        assert mock_agent.execute.called
        assert mock_agent_2.execute.called

    async def test_parallel_aggregates_outputs(self, mock_agent, mock_agent_2, context):
        """Given parallel agent execution
        When aggregating results
        Then it should combine all outputs
        """
        # Given
        strategy = ParallelStrategy()
        agents = [mock_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.aggregated_output is not None
        assert len(result.outputs) == 2


# =============================================================================
# DebateStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestDebateStrategy:
    """Behavioral tests for DebateStrategy (Pattern 3: A â‡„ B â‡„ C â†’ Synthesis)."""

    async def test_debate_execution_success(self, mock_agent, mock_agent_2, context):
        """Given multiple agents for debate
        When executing debate rounds
        Then agents should exchange outputs and synthesize
        """
        # Given
        strategy = DebateStrategy(max_rounds=2)
        agents = [mock_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        # Each agent called multiple times for rounds
        assert mock_agent.execute.call_count >= 1
        assert mock_agent_2.execute.call_count >= 1

    async def test_debate_with_max_rounds(self, mock_agent, mock_agent_2, context):
        """Given debate strategy with max rounds
        When executing
        Then it should not exceed max rounds
        """
        # Given
        max_rounds = 3
        strategy = DebateStrategy(max_rounds=max_rounds)
        agents = [mock_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        # Each agent should be called at most max_rounds times
        assert mock_agent.execute.call_count <= max_rounds
        assert mock_agent_2.execute.call_count <= max_rounds

    async def test_debate_synthesis(self, mock_agent, mock_agent_2, context):
        """Given completed debate rounds
        When synthesizing results
        Then it should produce aggregated output
        """
        # Given
        strategy = DebateStrategy(max_rounds=2)
        agents = [mock_agent, mock_agent_2]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.aggregated_output is not None
        assert "synthesis" in result.aggregated_output or result.aggregated_output != {}


# =============================================================================
# TeachingStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestTeachingStrategy:
    """Behavioral tests for TeachingStrategy (Pattern 4: Junior â†’ Expert validation)."""

    async def test_teaching_execution_success(self, context):
        """Given junior and expert agents
        When executing teaching strategy
        Then junior output should be validated by expert
        """
        # Given
        junior = Mock()
        junior.id = "junior"
        junior.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"answer": "junior answer"},
            "confidence": 0.6
        })

        expert = Mock()
        expert.id = "expert"
        expert.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"validation": "approved", "feedback": "good work"},
            "confidence": 0.95
        })

        strategy = TeachingStrategy(junior_agent=junior, expert_agent=expert)

        # When
        result = await strategy.execute([], context)

        # Then
        assert result.success
        assert junior.execute.called
        assert expert.execute.called

    async def test_teaching_expert_feedback_integration(self, context):
        """Given expert feedback on junior output
        When executing teaching strategy
        Then feedback should be integrated into result
        """
        # Given
        junior = Mock()
        junior.id = "junior"
        junior.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"answer": "initial attempt"},
            "confidence": 0.5
        })

        expert = Mock()
        expert.id = "expert"
        expert.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"validation": "needs_improvement", "suggestions": ["add details"]},
            "confidence": 0.9
        })

        strategy = TeachingStrategy(junior_agent=junior, expert_agent=expert)

        # When
        result = await strategy.execute([], context)

        # Then
        assert result.aggregated_output is not None
        # Expert output should be in aggregated output
        assert len(result.outputs) >= 2


# =============================================================================
# RefinementStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestRefinementStrategy:
    """Behavioral tests for RefinementStrategy (Pattern 5: Draft â†’ Review â†’ Polish)."""

    async def test_refinement_execution_success(self, mock_agent, mock_agent_2, mock_agent_3, context):
        """Given draft, review, and polish agents
        When executing refinement strategy
        Then output should be progressively refined
        """
        # Given
        strategy = RefinementStrategy()
        agents = [mock_agent, mock_agent_2, mock_agent_3]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        assert len(result.outputs) == 3
        assert mock_agent.execute.called  # draft
        assert mock_agent_2.execute.called  # review
        assert mock_agent_3.execute.called  # polish

    async def test_refinement_passes_feedback(self, context):
        """Given refinement stages
        When executing
        Then each stage should receive previous stage feedback
        """
        # Given
        draft = Mock()
        draft.id = "draft"
        draft.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"content": "draft content"},
            "confidence": 0.6
        })

        review = Mock()
        review.id = "review"
        review.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"feedback": "needs more detail"},
            "confidence": 0.8
        })

        polish = Mock()
        polish.id = "polish"
        polish.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"content": "polished content"},
            "confidence": 0.95
        })

        strategy = RefinementStrategy()
        agents = [draft, review, polish]

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert result.success
        # Review should receive draft output
        review_context = review.execute.call_args[0][0]
        assert "previous_output" in review_context or "input" in review_context


# =============================================================================
# AdaptiveStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestAdaptiveStrategy:
    """Behavioral tests for AdaptiveStrategy (Pattern 6: Classifier â†’ Specialist)."""

    async def test_adaptive_execution_with_classifier(self, context):
        """Given classifier and specialist agents
        When executing adaptive strategy
        Then classifier should route to appropriate specialist
        """
        # Given
        classifier = Mock()
        classifier.id = "classifier"
        classifier.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"category": "technical"},
            "confidence": 0.9
        })

        specialist1 = Mock()
        specialist1.id = "specialist1"
        specialist1.category = "technical"
        specialist1.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"result": "technical response"},
            "confidence": 0.95
        })

        specialist2 = Mock()
        specialist2.id = "specialist2"
        specialist2.category = "general"
        specialist2.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"result": "general response"},
            "confidence": 0.85
        })

        strategy = AdaptiveStrategy(
            classifier_agent=classifier,
            specialist_agents=[specialist1, specialist2]
        )

        # When
        result = await strategy.execute([], context)

        # Then
        assert result.success
        assert classifier.execute.called
        # At least one specialist should be called
        assert specialist1.execute.called or specialist2.execute.called

    async def test_adaptive_no_matching_specialist(self, context):
        """Given classifier output with no matching specialist
        When executing adaptive strategy
        Then it should handle gracefully with default behavior
        """
        # Given
        classifier = Mock()
        classifier.id = "classifier"
        classifier.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"category": "unknown"},
            "confidence": 0.7
        })

        specialist = Mock()
        specialist.id = "specialist"
        specialist.category = "known"
        specialist.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"result": "response"},
            "confidence": 0.9
        })

        strategy = AdaptiveStrategy(
            classifier_agent=classifier,
            specialist_agents=[specialist]
        )

        # When
        result = await strategy.execute([], context)

        # Then
        assert classifier.execute.called
        # Should either use fallback or return classifier result


# =============================================================================
# ConditionalStrategy Tests
# =============================================================================


@pytest.mark.asyncio
class TestConditionalStrategy:
    """Behavioral tests for ConditionalStrategy (Pattern 7: if X then A else B)."""

    async def test_conditional_then_branch(self, mock_agent, mock_agent_2, context):
        """Given condition that evaluates to true
        When executing conditional strategy
        Then then-branch should execute
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"confidence": {"$gt": 0.5}}
        )
        context["confidence"] = 0.8

        strategy = ConditionalStrategy(
            condition=condition,
            then_branch=[mock_agent],
            else_branch=[mock_agent_2]
        )

        # When
        result = await strategy.execute([], context)

        # Then
        assert result.success
        assert mock_agent.execute.called
        assert not mock_agent_2.execute.called

    async def test_conditional_else_branch(self, mock_agent, mock_agent_2, context):
        """Given condition that evaluates to false
        When executing conditional strategy
        Then else-branch should execute
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"confidence": {"$gt": 0.9}}
        )
        context["confidence"] = 0.5

        strategy = ConditionalStrategy(
            condition=condition,
            then_branch=[mock_agent],
            else_branch=[mock_agent_2]
        )

        # When
        result = await strategy.execute([], context)

        # Then
        assert result.success
        assert not mock_agent.execute.called
        assert mock_agent_2.execute.called

    async def test_conditional_no_else_branch(self, mock_agent, context):
        """Given condition with no else branch
        When condition is false
        Then it should return empty result
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"confidence": {"$gt": 0.9}}
        )
        context["confidence"] = 0.5

        strategy = ConditionalStrategy(
            condition=condition,
            then_branch=[mock_agent],
            else_branch=[]
        )

        # When
        result = await strategy.execute([], context)

        # Then
        assert not mock_agent.execute.called

    async def test_conditional_nested_strategies(self, mock_agent, mock_agent_2, context):
        """Given nested conditional strategies
        When executing
        Then it should handle nested branching correctly
        """
        # Given
        inner_condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"level": {"$eq": 2}}
        )

        outer_condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"level": {"$gt": 0}}
        )

        context["level"] = 2

        inner_strategy = ConditionalStrategy(
            condition=inner_condition,
            then_branch=[mock_agent_2],
            else_branch=[]
        )

        outer_strategy = ConditionalStrategy(
            condition=outer_condition,
            then_branch=[mock_agent],
            else_branch=[]
        )

        # When
        result = await outer_strategy.execute([], context)

        # Then
        assert result.success


# =============================================================================
# ExecutionStrategy Abstract Base Tests
# =============================================================================


class TestExecutionStrategy:
    """Behavioral tests for ExecutionStrategy abstract base class."""

    def test_execution_strategy_is_abstract(self):
        """Given ExecutionStrategy base class
        When trying to instantiate directly
        Then it should not be instantiable
        """
        # Given/When/Then
        with pytest.raises(TypeError):
            ExecutionStrategy()

    def test_execution_strategy_requires_execute(self):
        """Given a subclass without execute method
        When trying to instantiate
        Then it should raise TypeError
        """
        # Given
        class IncompleteStrategy(ExecutionStrategy):
            pass

        # When/Then
        with pytest.raises(TypeError):
            IncompleteStrategy()


# =============================================================================
# Integration Tests
# =============================================================================


@pytest.mark.asyncio
class TestStrategyIntegration:
    """Integration tests for strategy combinations."""

    async def test_sequential_with_conditional(self, mock_agent, mock_agent_2, context):
        """Given sequential strategy containing conditional
        When executing
        Then it should handle nested strategies correctly
        """
        # Given
        condition = Condition(
            condition_type=ConditionType.JSON_PREDICATE,
            predicate={"proceed": {"$eq": True}}
        )
        context["proceed"] = True

        conditional = ConditionalStrategy(
            condition=condition,
            then_branch=[mock_agent],
            else_branch=[]
        )

        sequential = SequentialStrategy()

        # When
        result = await sequential.execute([mock_agent_2], context)

        # Then
        assert result.success

    async def test_parallel_with_timeout_handling(self, context):
        """Given parallel execution with slow agents
        When timeout occurs
        Then it should handle gracefully
        """
        # Given
        slow_agent = Mock()
        slow_agent.id = "slow"
        slow_agent.execute = AsyncMock(side_effect=asyncio.TimeoutError())

        fast_agent = Mock()
        fast_agent.id = "fast"
        fast_agent.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"result": "fast"},
            "confidence": 0.9
        })

        strategy = ParallelStrategy()

        # When
        result = await strategy.execute([slow_agent, fast_agent], context)

        # Then
        # Should handle timeout gracefully
        assert len(result.outputs) >= 0


# =============================================================================
# Error Handling Tests
# =============================================================================


@pytest.mark.asyncio
class TestErrorHandling:
    """Tests for error handling across strategies."""

    async def test_agent_exception_handling(self, failing_agent, context):
        """Given an agent that raises exception
        When executing any strategy
        Then it should capture and report error
        """
        # Given
        strategy = SequentialStrategy()

        # When
        result = await strategy.execute([failing_agent], context)

        # Then
        assert not result.success or len(result.errors) > 0

    async def test_invalid_agent_output_handling(self, context):
        """Given an agent returning invalid output
        When executing strategy
        Then it should handle gracefully
        """
        # Given
        invalid_agent = Mock()
        invalid_agent.id = "invalid"
        invalid_agent.execute = AsyncMock(return_value=None)

        strategy = SequentialStrategy()

        # When
        result = await strategy.execute([invalid_agent], context)

        # Then
        # Should handle None output gracefully
        assert result is not None

    async def test_empty_context_handling(self, mock_agent):
        """Given empty context
        When executing strategy
        Then it should handle gracefully
        """
        # Given
        strategy = SequentialStrategy()
        empty_context = {}

        # When
        result = await strategy.execute([mock_agent], empty_context)

        # Then
        assert result is not None


# =============================================================================
# Edge Cases
# =============================================================================


@pytest.mark.asyncio
class TestEdgeCases:
    """Tests for edge cases and boundary conditions."""

    async def test_single_agent_execution(self, mock_agent, context):
        """Given single agent in strategy
        When executing
        Then it should work correctly
        """
        # Given
        strategy = SequentialStrategy()

        # When
        result = await strategy.execute([mock_agent], context)

        # Then
        assert result.success
        assert len(result.outputs) == 1

    async def test_large_number_of_agents(self, context):
        """Given large number of agents
        When executing parallel strategy
        Then it should handle efficiently
        """
        # Given
        agents = []
        for i in range(50):
            agent = Mock()
            agent.id = f"agent_{i}"
            agent.execute = AsyncMock(return_value={
                "status": "success",
                "data": {"result": f"output_{i}"},
                "confidence": 0.8
            })
            agents.append(agent)

        strategy = ParallelStrategy()

        # When
        result = await strategy.execute(agents, context)

        # Then
        assert len(result.outputs) <= 50

    async def test_deeply_nested_output_structures(self, context):
        """Given agents producing deeply nested outputs
        When aggregating results
        Then it should handle complex structures
        """
        # Given
        agent = Mock()
        agent.id = "complex"
        agent.execute = AsyncMock(return_value={
            "status": "success",
            "data": {
                "level1": {
                    "level2": {
                        "level3": {
                            "value": "deep"
                        }
                    }
                }
            },
            "confidence": 0.9
        })

        strategy = SequentialStrategy()

        # When
        result = await strategy.execute([agent], context)

        # Then
        assert result.success
        assert result.outputs[0].output is not None

    async def test_unicode_and_special_characters(self, context):
        """Given agents producing unicode and special characters
        When processing outputs
        Then it should handle correctly
        """
        # Given
        agent = Mock()
        agent.id = "unicode"
        agent.execute = AsyncMock(return_value={
            "status": "success",
            "data": {"text": "Hello ä¸–ç•Œ ðŸŒ <>&"},
            "confidence": 0.9
        })

        strategy = SequentialStrategy()

        # When
        result = await strategy.execute([agent], context)

        # Then
        assert result.success
        assert "ä¸–ç•Œ" in str(result.outputs[0].output) or "text" in result.outputs[0].output

    async def test_concurrent_strategy_executions(self, mock_agent, context):
        """Given multiple concurrent strategy executions
        When running in parallel
        Then they should not interfere with each other
        """
        # Given
        strategy1 = SequentialStrategy()
        strategy2 = SequentialStrategy()

        # When
        results = await asyncio.gather(
            strategy1.execute([mock_agent], context),
            strategy2.execute([mock_agent], context)
        )

        # Then
        assert len(results) == 2
        assert all(r.success for r in results)