"""Behavioral tests for pr_review.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
import pytest
from unittest.mock import AsyncMock, MagicMock, Mock, patch
from empathy_os.workflows.pr_review import (
    PRReviewResult,
    PRReviewWorkflow,
)


# ============================================================================
# Fixtures
# ============================================================================


@pytest.fixture
def mock_code_review_crew():
    """Fixture providing a mocked CodeReviewCrew."""
    with patch("empathy_os.workflows.pr_review.CodeReviewCrew") as mock_crew_class:
        mock_crew = MagicMock()
        mock_crew_class.return_value = mock_crew
        
        # Mock execute method
        mock_crew.execute = AsyncMock(return_value={
            "success": True,
            "findings": [
                {
                    "severity": "high",
                    "category": "code_quality",
                    "message": "Complex function detected",
                    "file": "main.py",
                    "line": 42,
                }
            ],
            "quality_score": 85.0,
            "agents_used": ["code_reviewer", "architect"],
            "duration_seconds": 2.5,
            "cost": 0.05,
        })
        
        yield mock_crew_class


@pytest.fixture
def mock_security_audit_crew():
    """Fixture providing a mocked SecurityAuditCrew."""
    with patch("empathy_os.workflows.pr_review.SecurityAuditCrew") as mock_crew_class:
        mock_crew = MagicMock()
        mock_crew_class.return_value = mock_crew
        
        # Mock execute method
        mock_crew.execute = AsyncMock(return_value={
            "success": True,
            "findings": [
                {
                    "severity": "critical",
                    "category": "security",
                    "message": "SQL injection vulnerability",
                    "file": "db.py",
                    "line": 10,
                }
            ],
            "risk_score": 75.0,
            "agents_used": ["security_auditor"],
            "duration_seconds": 3.0,
            "cost": 0.08,
        })
        
        yield mock_crew_class


@pytest.fixture
def sample_diff():
    """Fixture providing a sample git diff."""
    return """
diff --git a/src/main.py b/src/main.py
index abc123..def456 100644
--- a/src/main.py
+++ b/src/main.py
@@ -1,3 +1,5 @@
+import os
+
 def process_data(data):
-    return data
+    query = f"SELECT * FROM users WHERE id={data}"
+    return execute_query(query)
"""


@pytest.fixture
def sample_files():
    """Fixture providing sample changed files list."""
    return ["src/main.py", "src/db.py", "tests/test_main.py"]


# ============================================================================
# PRReviewResult Tests
# ============================================================================


class TestPRReviewResult:
    """Behavioral tests for PRReviewResult dataclass."""

    def test_given_all_fields_when_created_then_stores_values(self):
        """GIVEN all required fields
        WHEN PRReviewResult is created
        THEN it should store all values correctly.
        """
        # Given
        code_review = {"quality_score": 85.0}
        security_audit = {"risk_score": 70.0}
        findings = [{"severity": "high", "message": "Issue"}]
        
        # When
        result = PRReviewResult(
            success=True,
            verdict="approve_with_suggestions",
            code_quality_score=85.0,
            security_risk_score=70.0,
            combined_score=77.5,
            code_review=code_review,
            security_audit=security_audit,
            all_findings=findings,
            code_findings=[findings[0]],
            security_findings=[],
            critical_count=0,
            high_count=1,
            blockers=[],
            warnings=["Warning 1"],
            recommendations=["Recommendation 1"],
            summary="Review complete",
            agents_used=["agent1", "agent2"],
            duration_seconds=5.5,
            cost=0.13,
            metadata={"key": "value"},
        )
        
        # Then
        assert result.success is True
        assert result.verdict == "approve_with_suggestions"
        assert result.code_quality_score == 85.0
        assert result.security_risk_score == 70.0
        assert result.combined_score == 77.5
        assert result.code_review == code_review
        assert result.security_audit == security_audit
        assert result.all_findings == findings
        assert result.critical_count == 0
        assert result.high_count == 1
        assert len(result.warnings) == 1
        assert len(result.recommendations) == 1
        assert result.duration_seconds == 5.5
        assert result.cost == 0.13
        assert result.metadata == {"key": "value"}

    def test_given_minimal_fields_when_created_then_defaults_applied(self):
        """GIVEN only required fields
        WHEN PRReviewResult is created
        THEN default values should be applied.
        """
        # When
        result = PRReviewResult(
            success=False,
            verdict="reject",
            code_quality_score=0.0,
            security_risk_score=0.0,
            combined_score=0.0,
            code_review=None,
            security_audit=None,
            all_findings=[],
            code_findings=[],
            security_findings=[],
            critical_count=0,
            high_count=0,
            blockers=[],
            warnings=[],
            recommendations=[],
            summary="",
            agents_used=[],
            duration_seconds=0.0,
        )
        
        # Then
        assert result.cost == 0.0
        assert result.metadata == {}


# ============================================================================
# PRReviewWorkflow Initialization Tests
# ============================================================================


class TestPRReviewWorkflowInit:
    """Behavioral tests for PRReviewWorkflow initialization."""

    def test_given_no_args_when_initialized_then_uses_defaults(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN no arguments
        WHEN PRReviewWorkflow is initialized
        THEN it should use default values.
        """
        # When
        workflow = PRReviewWorkflow()
        
        # Then
        assert workflow.provider == "anthropic"
        assert workflow.use_code_crew is True
        assert workflow.use_security_crew is True
        assert workflow.parallel is True

    def test_given_custom_provider_when_initialized_then_stores_provider(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN custom provider
        WHEN PRReviewWorkflow is initialized
        THEN it should store the provider.
        """
        # When
        workflow = PRReviewWorkflow(provider="openai")
        
        # Then
        assert workflow.provider == "openai"

    def test_given_hybrid_provider_when_initialized_then_maps_to_anthropic(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN hybrid provider
        WHEN PRReviewWorkflow is initialized
        THEN it should map to anthropic for crews.
        """
        # When
        workflow = PRReviewWorkflow(provider="hybrid")
        
        # Then
        assert workflow.provider == "hybrid"
        assert workflow.code_crew_config["provider"] == "anthropic"
        assert workflow.security_crew_config["provider"] == "anthropic"

    def test_given_crews_disabled_when_initialized_then_stores_flags(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN crews disabled
        WHEN PRReviewWorkflow is initialized
        THEN it should store the flags correctly.
        """
        # When
        workflow = PRReviewWorkflow(use_code_crew=False, use_security_crew=False)
        
        # Then
        assert workflow.use_code_crew is False
        assert workflow.use_security_crew is False

    def test_given_sequential_mode_when_initialized_then_parallel_false(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN sequential mode requested
        WHEN PRReviewWorkflow is initialized
        THEN parallel should be False.
        """
        # When
        workflow = PRReviewWorkflow(parallel=False)
        
        # Then
        assert workflow.parallel is False

    def test_given_custom_configs_when_initialized_then_merges_with_provider(
        self, mock_code_review_crew, mock_security_audit_crew
    ):
        """GIVEN custom crew configs
        WHEN PRReviewWorkflow is initialized
        THEN it should merge configs with provider.
        """
        # Given
        code_config = {"max_tokens": 2000}
        security_config = {"temperature": 0.5}
        
        # When
        workflow = PRReviewWorkflow(
            provider="openai",
            code_crew_config=code_config,
            security_crew_config=security_config,
        )
        
        # Then
        assert workflow.code_crew_config["provider"] == "openai"
        assert workflow.code_crew_config["max_tokens"] == 2000
        assert workflow.security_crew_config["provider"] == "openai"
        assert workflow.security_crew_config["temperature"] == 0.5


# ============================================================================
# PRReviewWorkflow Execute Tests - Both Crews Enabled
# ============================================================================


class TestPRReviewWorkflowExecuteBothCrews:
    """Behavioral tests for execute method with both crews enabled."""

    @pytest.mark.asyncio
    async def test_given_valid_input_when_executed_parallel_then_returns_success(
        self, mock_code_review_crew, mock_security_audit_crew, sample_diff, sample_files
    ):
        """GIVEN valid PR input
        WHEN execute is called with parallel mode
        THEN it should return successful result combining both crews.
        """
        # Given
        workflow = PRReviewWorkflow(parallel=True)
        
        # When
        result = await workflow.execute(
            diff=sample_diff,
            files_changed=sample_files,
            target_path="./src",
        )
        
        # Then
        assert result.success is True
        assert result.code_review is not None
        assert result.security_audit is not None
        assert result.code_quality_score == 85.0
        assert result.security_risk_score == 75.0
        assert len(result.all_findings) == 2
        assert len(result.code_findings) == 1
        assert len(result.security_findings) == 1
        assert result.critical_count == 1
        assert result.high_count == 1
        assert result.cost == 0.13  # 0.05 + 0.08
        assert "code_reviewer" in result.agents_used
        assert "security_auditor" in result.agents_used

    @pytest.mark.asyncio
    async def test_given_valid_input_when_executed_sequential_then_returns_success(
        self, mock_code_review_crew, mock_security_audit_crew, sample_diff, sample_files
    ):
        """GIVEN valid PR input
        WHEN execute is called with sequential mode
        THEN it should return successful result running crews sequentially.
        """
        # Given
        workflow = PRReviewWorkflow(parallel=False)
        
        # When
        result = await workflow.execute(
            diff=sample_diff,
            files_changed=sample_files,
            target_path="./src",
        )
        
        # Then
        assert result.success is True
        assert result.code_review is not None
        assert result.security_audit is not None
        assert len(result.all_findings) == 2

    @pytest.mark.asyncio
    async def test_given_critical_findings_when_executed_then_verdict_is_reject(
        self, mock_code_review_crew, mock_security_audit_crew, sample_diff, sample_files
    ):
        """GIVEN PR with critical security findings
        WHEN execute is called
        THEN verdict should be reject.
        """
        # Given
        workflow = PRReviewWorkflow()
        
        # When
        result = await workflow.execute(
            diff=sample_diff,
            files_changed=sample_files,
            target_path="./src",
        )
        
        # Then
        assert result.critical_count > 0
        assert result.verdict == "reject"

    @pytest.mark.asyncio
    async def test_given_high_findings_when_executed_then_verdict_is_request_changes(
        self, mock_code_review_crew, mock_security_audit_crew, sample_diff, sample_files
    ):
        """GIVEN PR with high severity findings only
        WHEN execute is called
        THEN verdict should be request_changes.
        """
        # Given
        mock_security_audit_crew.return_value.execute = AsyncMock(return_value={
            "success": True,
            "findings": [
                {
                    "severity": "high",
                    "category": "security",
                    "message": "Weak encryption",
                    "file": "crypto.py",
                    "line": 5,
                }
            ],
            "risk_score": 60.0,
            "agents_used": ["security_auditor"],
            "duration_seconds": 2.0,
            "cost": 0.05,
        })
        workflow = PRReviewWorkflow()
        
        # When
        result = await workflow.execute(
            diff=sample_diff,
            files_changed=sample_files,
            target_path="./src",
        )
        
        # Then
        assert result.critical_count == 0
        assert result.high_count > 0
        assert result.verdict == "request_changes"

    @pytest.mark.asyncio
    async def test_given_medium_findings_when_executed_then_verdict_is_approve_with_suggestions(
        self, mock_code_review_crew, mock_security_audit_crew, sample_diff, sample_files
    ):
        """GIVEN PR with medium severity findings only
        WHEN execute is called
        THEN verdict should be approve_with_suggestions.
        """
        # Given
        mock_code_review_crew.return_value.execute = AsyncMock(return_value={
            "success": True,
            "findings": [
                {
                    "severity": "medium",
                    "category": "code_quality",
                    "message": "Consider refactoring",
                    "file": "main.py",
                    "line": 42,
                }
            ],
            "quality_score": 90.0,
            "agents_used": ["code_reviewer"],
            "duration_seconds": 2.0,
            "cost": 0.04,
        })
        mock_security_audit_crew.return_value.execute = AsyncMock(return_value={
            "success": True,
            "findings": [],
            "risk_score": 95.0,
            "agents_used": ["security_auditor"],
            "duration_seconds": 1.5,
            "cost": 0.03,
        })
        workflow = PRReviewWorkflow()
        
        # When
        result = await workflow.execute(
            diff=sample_diff,
            files_changed=sample_files,
            target_path="./src