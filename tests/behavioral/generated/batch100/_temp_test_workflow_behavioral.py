"""Tests for Test Generation Workflow.

Generated by enhanced autonomous test generation system.

Copyright 2025 Smart-AI-Memory
Licensed under Fair Source License 0.9
"""

import json
from pathlib import Path
from typing import Any
from unittest.mock import AsyncMock, Mock, mock_open, patch

import pytest

from empathy_os.workflows.test_gen.workflow import TestGenerationWorkflow, main


@pytest.fixture
def mock_path_for_init(mocker):
    """Mock Path to prevent file I/O during initialization."""
    mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
    mock_path_instance = Mock()
    mock_debugging_file = Mock()
    mock_debugging_file.exists.return_value = False
    mock_path_instance.__truediv__.return_value = mock_debugging_file
    mock_path_class.return_value = mock_path_instance
    return mock_path_class


@pytest.fixture
def mock_ast_analyzer(mocker):
    """Mock ASTFunctionAnalyzer."""
    mock_class = mocker.patch(
        "empathy_os.workflows.test_gen.workflow.ASTFunctionAnalyzer"
    )
    mock_instance = Mock()
    mock_instance.analyze.return_value = ([], [])
    mock_instance.last_error = None
    mock_class.return_value = mock_instance
    return mock_instance


@pytest.fixture
def mock_test_templates(mocker):
    """Mock test template generation functions."""
    mock_func = mocker.patch(
        "empathy_os.workflows.test_gen.workflow.generate_test_for_function"
    )
    mock_class = mocker.patch(
        "empathy_os.workflows.test_gen.workflow.generate_test_for_class"
    )
    mock_func.return_value = "def test_function():\n    pass"
    mock_class.return_value = "def test_class():\n    pass"
    return {"function": mock_func, "class": mock_class}


@pytest.fixture
def workflow(mock_path_for_init):
    """Create workflow instance with mocked dependencies."""
    return TestGenerationWorkflow(
        patterns_dir="./test_patterns",
        min_tests_for_review=10,
        write_tests=False,
        output_dir="tests/generated",
    )


class TestTestGenerationWorkflowInit:
    """Test workflow initialization."""

    def test_initializes_with_default_parameters(self, mock_path_for_init):
        """Given default parameters, when initializing, then sets correct defaults."""
        # When
        workflow = TestGenerationWorkflow()

        # Then
        assert workflow.patterns_dir == "./patterns"
        assert workflow.min_tests_for_review == 10
        assert workflow.write_tests is False
        assert workflow.output_dir == "tests/generated"
        assert workflow.enable_auth_strategy is True
        assert workflow._test_count == 0
        assert workflow._bug_hotspots == []
        assert workflow._auth_mode_used is None

    def test_initializes_with_custom_parameters(self, mock_path_for_init):
        """Given custom parameters, when initializing, then sets custom values."""
        # When
        workflow = TestGenerationWorkflow(
            patterns_dir="./custom_patterns",
            min_tests_for_review=20,
            write_tests=True,
            output_dir="custom/tests",
            enable_auth_strategy=False,
        )

        # Then
        assert workflow.patterns_dir == "./custom_patterns"
        assert workflow.min_tests_for_review == 20
        assert workflow.write_tests is True
        assert workflow.output_dir == "custom/tests"
        assert workflow.enable_auth_strategy is False

    def test_loads_bug_hotspots_on_init(self, mocker):
        """Given debugging.json exists, when initializing, then loads bug hotspots."""
        # Given
        debug_data = {
            "patterns": [
                {"files_affected": ["src/module.py", "src/utils.py"]},
                {"files_affected": ["src/main.py"]},
            ]
        }

        mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_path_instance = Mock()
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_path_instance.__truediv__.return_value = mock_file
        mock_path_class.return_value = mock_path_instance

        mocker.patch("builtins.open", mock_open(read_data=json.dumps(debug_data)))

        # When
        workflow = TestGenerationWorkflow(patterns_dir="./patterns")

        # Then
        assert len(workflow._bug_hotspots) == 3
        assert "src/module.py" in workflow._bug_hotspots


class TestLoadBugHotspots:
    """Test bug hotspot loading."""

    def test_loads_hotspots_from_valid_json(self, workflow, mocker):
        """Given valid debugging.json, when loading, then extracts file paths."""
        # Given
        debug_data = {
            "patterns": [
                {"files_affected": ["file1.py", "file2.py"]},
                {"files_affected": ["file3.py"]},
            ]
        }

        mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_path_instance = Mock()
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_path_instance.__truediv__.return_value = mock_file
        mock_path_class.return_value = mock_path_instance

        mocker.patch("builtins.open", mock_open(read_data=json.dumps(debug_data)))

        # When
        workflow._load_bug_hotspots()

        # Then
        assert len(workflow._bug_hotspots) == 3
        assert "file1.py" in workflow._bug_hotspots

    def test_handles_missing_file_gracefully(self, workflow, mocker):
        """Given missing debugging.json, when loading, then sets empty hotspots."""
        # Given
        mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_path_instance = Mock()
        mock_file = Mock()
        mock_file.exists.return_value = False
        mock_path_instance.__truediv__.return_value = mock_file
        mock_path_class.return_value = mock_path_instance

        # When
        workflow._load_bug_hotspots()

        # Then
        assert workflow._bug_hotspots == []

    def test_handles_invalid_json_gracefully(self, workflow, mocker):
        """Given invalid JSON, when loading, then sets empty hotspots."""
        # Given
        mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_path_instance = Mock()
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_path_instance.__truediv__.return_value = mock_file
        mock_path_class.return_value = mock_path_instance

        mocker.patch("builtins.open", mock_open(read_data="invalid json"))

        # When
        workflow._load_bug_hotspots()

        # Then
        assert workflow._bug_hotspots == []

    def test_handles_none_values_in_files_affected(self, workflow, mocker):
        """Given None values in files_affected, when loading, then skips them."""
        # Given
        debug_data = {
            "patterns": [
                {"files_affected": ["file1.py", None, "file2.py"]},
            ]
        }

        mock_path_class = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_path_instance = Mock()
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_path_instance.__truediv__.return_value = mock_file
        mock_path_class.return_value = mock_path_instance

        mocker.patch("builtins.open", mock_open(read_data=json.dumps(debug_data)))

        # When
        workflow._load_bug_hotspots()

        # Then
        assert len(workflow._bug_hotspots) == 2
        assert "file1.py" in workflow._bug_hotspots
        assert "file2.py" in workflow._bug_hotspots


class TestShouldSkipStage:
    """Test stage skipping logic."""

    def test_downgrades_review_when_few_tests(self, workflow):
        """Given few tests, when checking review stage, then downgrades to CAPABLE."""
        # Given
        from empathy_os.workflows.base import ModelTier

        workflow._test_count = 5
        workflow.min_tests_for_review = 10

        # When
        should_skip, reason = workflow.should_skip_stage("review", {})

        # Then
        assert should_skip is False
        assert reason is None
        assert workflow.tier_map["review"] == ModelTier.CAPABLE

    def test_keeps_review_premium_when_enough_tests(self, workflow):
        """Given enough tests, when checking review stage, then keeps PREMIUM."""
        # Given
        from empathy_os.workflows.base import ModelTier

        workflow._test_count = 15
        workflow.min_tests_for_review = 10
        workflow.tier_map["review"] = ModelTier.PREMIUM

        # When
        should_skip, reason = workflow.should_skip_stage("review", {})

        # Then
        assert should_skip is False
        assert workflow.tier_map["review"] == ModelTier.PREMIUM

    def test_does_not_skip_other_stages(self, workflow):
        """Given other stages, when checking, then does not skip."""
        # When
        skip_identify, _ = workflow.should_skip_stage("identify", {})
        skip_analyze, _ = workflow.should_skip_stage("analyze", {})
        skip_generate, _ = workflow.should_skip_stage("generate", {})

        # Then
        assert skip_identify is False
        assert skip_analyze is False
        assert skip_generate is False


class TestRunStage:
    """Test stage routing."""

    @pytest.mark.asyncio
    async def test_routes_to_identify_stage(self, workflow, mocker):
        """Given identify stage, when running, then calls _identify."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_identify = mocker.patch.object(
            workflow, "_identify", return_value=({}, 10, 20)
        )

        # When
        result = await workflow.run_stage("identify", ModelTier.CHEAP, {})

        # Then
        mock_identify.assert_called_once()
        assert result == ({}, 10, 20)

    @pytest.mark.asyncio
    async def test_routes_to_analyze_stage(self, workflow, mocker):
        """Given analyze stage, when running, then calls _analyze."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_analyze = mocker.patch.object(
            workflow, "_analyze", return_value=({}, 10, 20)
        )

        # When
        result = await workflow.run_stage("analyze", ModelTier.CAPABLE, {})

        # Then
        mock_analyze.assert_called_once()

    @pytest.mark.asyncio
    async def test_routes_to_generate_stage(self, workflow, mocker):
        """Given generate stage, when running, then calls _generate."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_generate = mocker.patch.object(
            workflow, "_generate", return_value=({}, 10, 20)
        )

        # When
        result = await workflow.run_stage("generate", ModelTier.CAPABLE, {})

        # Then
        mock_generate.assert_called_once()

    @pytest.mark.asyncio
    async def test_routes_to_review_stage(self, workflow, mocker):
        """Given review stage, when running, then calls _review."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_review = mocker.patch.object(
            workflow, "_review", return_value=({}, 10, 20)
        )

        # When
        result = await workflow.run_stage("review", ModelTier.PREMIUM, {})

        # Then
        mock_review.assert_called_once()

    @pytest.mark.asyncio
    async def test_raises_error_for_unknown_stage(self, workflow):
        """Given unknown stage, when running, then raises ValueError."""
        # Given
        from empathy_os.workflows.base import ModelTier

        # When/Then
        with pytest.raises(ValueError, match="Unknown stage: unknown"):
            await workflow.run_stage("unknown", ModelTier.CHEAP, {})


class TestIdentifyStage:
    """Test file identification stage."""

    @pytest.mark.asyncio
    async def test_identifies_python_files(self, workflow, mocker):
        """Given Python files, when identifying, then returns candidates."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_file = Mock()
        mock_file.stat.return_value.st_size = 1024
        mock_file.read_text.return_value = "def test():\n    pass\n"
        mock_file.__str__.return_value = "src/module.py"

        mock_target.rglob.return_value = [mock_file]
        mock_path.return_value = mock_target

        input_data = {"path": ".", "file_types": [".py"]}

        # When
        result, input_tokens, output_tokens = await workflow._identify(
            input_data, ModelTier.CHEAP
        )

        # Then
        assert "candidates" in result
        assert result["total_candidates"] >= 0
        assert "scan_summary" in result

    @pytest.mark.asyncio
    async def test_skips_test_files(self, workflow, mocker):
        """Given test files, when identifying, then excludes them from candidates."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_file = Mock()
        mock_file.stat.return_value.st_size = 1024
        mock_file.__str__.return_value = "tests/test_module.py"

        mock_target.rglob.return_value = [mock_file]
        mock_path.return_value = mock_target

        input_data = {"path": ".", "file_types": [".py"]}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert result["existing_test_files"] == 1
        assert len(result["candidates"]) == 0

    @pytest.mark.asyncio
    async def test_applies_skip_patterns(self, workflow, mocker):
        """Given skip patterns, when identifying, then excludes matching files."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_file = Mock()
        mock_file.__str__.return_value = "venv/lib/module.py"
        mock_file.stat.return_value.st_size = 1024

        mock_target.rglob.return_value = [mock_file]
        mock_path.return_value = mock_target

        input_data = {
            "path": ".",
            "file_types": [".py"],
            "skip_patterns": ["venv", "node_modules"],
        }

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert result["scan_summary"]["files_excluded_by_pattern"] == 1

    @pytest.mark.asyncio
    async def test_respects_file_size_limit(self, workflow, mocker):
        """Given large files, when identifying, then skips files over limit."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_file = Mock()
        mock_file.stat.return_value.st_size = 1024 * 300  # 300KB
        mock_file.__str__.return_value = "src/large.py"

        mock_target.rglob.return_value = [mock_file]
        mock_path.return_value = mock_target

        input_data = {"path": ".", "file_types": [".py"], "max_file_size_kb": 200}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert result["scan_summary"]["files_too_large"] == 1

    @pytest.mark.asyncio
    async def test_respects_scan_limit(self, workflow, mocker):
        """Given many files, when identifying, then stops at scan limit."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_files = []
        for i in range(15):
            mock_file = Mock()
            mock_file.stat.return_value.st_size = 1024
            mock_file.read_text.return_value = f"# File {i}\n"
            mock_file.__str__.return_value = f"src/module{i}.py"
            mock_files.append(mock_file)

        mock_target.rglob.return_value = mock_files
        mock_path.return_value = mock_target

        input_data = {"path": ".", "file_types": [".py"], "max_files_to_scan": 10}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert result["scan_summary"]["files_scanned"] == 10
        assert "max_files_to_scan" in result["scan_summary"]["early_exit_reason"]

    @pytest.mark.asyncio
    async def test_calculates_priority_correctly(self, workflow, mocker):
        """Given file characteristics, when identifying, then calculates priority."""
        # Given
        from empathy_os.workflows.base import ModelTier

        workflow._bug_hotspots = ["src/buggy.py"]

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True

        mock_file = Mock()
        mock_file.stat.return_value.st_size = 1024
        mock_file.read_text.return_value = "# Code\n" * 150
        mock_file.__str__.return_value = "src/buggy.py"

        mock_target.rglob.return_value = [mock_file]
        mock_path.return_value = mock_target

        mock_test_file = mocker.patch.object(workflow, "_find_test_file")
        mock_test_file.return_value = Mock(exists=Mock(return_value=False))

        input_data = {"path": ".", "file_types": [".py"]}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert len(result["candidates"]) > 0
        candidate = result["candidates"][0]
        assert candidate["is_hotspot"] is True
        assert candidate["has_tests"] is False
        assert candidate["priority"] > 0

    @pytest.mark.asyncio
    async def test_integrates_auth_strategy(self, workflow, mocker):
        """Given auth strategy enabled, when identifying, then calculates auth mode."""
        # Given
        from empathy_os.workflows.base import ModelTier

        workflow.enable_auth_strategy = True

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True
        mock_target.is_file.return_value = False
        mock_target.is_dir.return_value = True
        mock_target.rglob.return_value = []

        mock_path.return_value = mock_target

        mock_count_loc = mocker.patch(
            "empathy_os.workflows.test_gen.workflow.count_lines_of_code"
        )
        mock_count_loc.return_value = 1000

        mock_strategy = Mock()
        mock_mode = Mock(value="pay_as_you_go")
        mock_strategy.get_recommended_mode.return_value = mock_mode
        mock_strategy.estimate_cost.return_value = {
            "monetary_cost": 0.05,
            "quota_cost": 100,
        }

        mocker.patch(
            "empathy_os.workflows.test_gen.workflow.get_auth_strategy",
            return_value=mock_strategy,
        )
        mocker.patch(
            "empathy_os.workflows.test_gen.workflow.get_module_size_category",
            return_value="medium",
        )

        input_data = {"path": "src/module.py", "file_types": [".py"]}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert workflow._auth_mode_used == "pay_as_you_go"

    @pytest.mark.asyncio
    async def test_handles_auth_strategy_error(self, workflow, mocker):
        """Given auth strategy error, when identifying, then continues gracefully."""
        # Given
        from empathy_os.workflows.base import ModelTier

        workflow.enable_auth_strategy = True

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_target = Mock()
        mock_target.exists.return_value = True
        mock_target.is_file.return_value = True
        mock_target.rglob.return_value = []

        mock_path.return_value = mock_target

        mocker.patch(
            "empathy_os.workflows.test_gen.workflow.count_lines_of_code",
            side_effect=Exception("Count error"),
        )

        input_data = {"path": ".", "file_types": [".py"]}

        # When
        result, _, _ = await workflow._identify(input_data, ModelTier.CHEAP)

        # Then
        assert "candidates" in result


class TestFindTestFile:
    """Test test file location logic."""

    def test_finds_test_file_in_same_directory(self, workflow, mocker):
        """Given test in same dir, when finding, then returns correct path."""
        # Given
        source_file = Path("src/module.py")
        expected_test = Path("src/test_module.py")

        mock_path = mocker.patch("pathlib.Path.exists")
        mock_path.return_value = True

        # When
        result = workflow._find_test_file(source_file)

        # Then
        assert result == expected_test

    def test_returns_expected_location_when_not_found(self, workflow, mocker):
        """Given no test file, when finding, then returns expected location."""
        # Given
        source_file = Path("src/module.py")

        mock_path = mocker.patch("pathlib.Path.exists")
        mock_path.return_value = False

        # When
        result = workflow._find_test_file(source_file)

        # Then
        assert result == Path("src/test_module.py")


class TestAnalyzeStage:
    """Test code analysis stage."""

    @pytest.mark.asyncio
    async def test_analyzes_candidate_files(self, workflow, mock_ast_analyzer, mocker):
        """Given candidates, when analyzing, then extracts functions and classes."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_file.read_text.return_value = "def foo(): pass"
        mock_path.return_value = mock_file

        mock_ast_analyzer.analyze.return_value = (
            [
                Mock(
                    name="test_func",
                    params=[("x", "int", None)],
                    is_async=False,
                    return_type="str",
                    raises=set(),
                    has_side_effects=False,
                    complexity=1,
                    docstring="Test",
                )
            ],
            [],
        )

        input_data = {
            "candidates": [{"file": "src/module.py", "priority": 50}],
            "config": {},
        }

        # When
        result, _, _ = await workflow._analyze(input_data, ModelTier.CAPABLE)

        # Then
        assert "analysis" in result
        assert len(result["analysis"]) == 1
        assert result["total_functions"] == 1

    @pytest.mark.asyncio
    async def test_respects_max_files_to_analyze(self, workflow, mock_ast_analyzer, mocker):
        """Given many candidates, when analyzing, then respects limit."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_file.read_text.return_value = "# code"
        mock_path.return_value = mock_file

        mock_ast_analyzer.analyze.return_value = ([], [])

        candidates = [{"file": f"file{i}.py", "priority": 10} for i in range(30)]
        input_data = {
            "candidates": candidates,
            "config": {"max_files_to_analyze": 5},
        }

        # When
        result, _, _ = await workflow._analyze(input_data, ModelTier.CAPABLE)

        # Then
        assert len(result["analysis"]) <= 5

    @pytest.mark.asyncio
    async def test_tracks_parse_errors(self, workflow, mock_ast_analyzer, mocker):
        """Given parse errors, when analyzing, then tracks them."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_file = Mock()
        mock_file.exists.return_value = True
        mock_file.read_text.return_value = "invalid python"
        mock_path.return_value = mock_file

        mock_ast_analyzer.analyze.return_value = ([], [])
        mock_ast_analyzer.last_error = "SyntaxError at line 1"

        input_data = {
            "candidates": [{"file": "src/bad.py", "priority": 10}],
            "config": {},
        }

        # When
        result, _, _ = await workflow._analyze(input_data, ModelTier.CAPABLE)

        # Then
        assert "parse_errors" in result
        assert len(result["parse_errors"]) > 0

    @pytest.mark.asyncio
    async def test_handles_missing_files(self, workflow, mock_ast_analyzer, mocker):
        """Given missing files, when analyzing, then skips them."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_file = Mock()
        mock_file.exists.return_value = False
        mock_path.return_value = mock_file

        input_data = {
            "candidates": [{"file": "missing.py", "priority": 10}],
            "config": {},
        }

        # When
        result, _, _ = await workflow._analyze(input_data, ModelTier.CAPABLE)

        # Then
        assert len(result["analysis"]) == 0


class TestExtractFunctions:
    """Test function extraction."""

    def test_extracts_function_signatures(self, workflow, mock_ast_analyzer):
        """Given Python code, when extracting, then returns function details."""
        # Given
        mock_ast_analyzer.analyze.return_value = (
            [
                Mock(
                    name="calculate",
                    params=[("x", "int", None), ("y", "int", "0")],
                    is_async=False,
                    return_type="int",
                    raises={"ValueError"},
                    has_side_effects=False,
                    complexity=2,
                    docstring="Calculate sum",
                )
            ],
            [],
        )

        content = "def calculate(x: int, y: int = 0) -> int:\n    return x + y"

        # When
        functions, error = workflow._extract_functions(content)

        # Then
        assert len(functions) == 1
        assert functions[0]["name"] == "calculate"
        assert len(functions[0]["params"]) == 2
        assert functions[0]["return_type"] == "int"
        assert "ValueError" in functions[0]["raises"]

    def test_respects_max_functions_limit(self, workflow, mock_ast_analyzer):
        """Given many functions, when extracting, then respects limit."""
        # Given
        mock_funcs = [
            Mock(
                name=f"func{i}",
                params=[],
                is_async=False,
                return_type=None,
                raises=set(),
                has_side_effects=False,
                complexity=1,
                docstring=None,
            )
            for i in range(50)
        ]
        mock_ast_analyzer.analyze.return_value = (mock_funcs, [])

        content = "# many functions"

        # When
        functions, _ = workflow._extract_functions(content, max_functions=10)

        # Then
        assert len(functions) == 10

    def test_filters_private_functions(self, workflow, mock_ast_analyzer):
        """Given private functions, when extracting, then excludes them."""
        # Given
        mock_ast_analyzer.analyze.return_value = (
            [
                Mock(
                    name="_private",
                    params=[],
                    is_async=False,
                    return_type=None,
                    raises=set(),
                    has_side_effects=False,
                    complexity=1,
                    docstring=None,
                ),
                Mock(
                    name="__dunder__",
                    params=[],
                    is_async=False,
                    return_type=None,
                    raises=set(),
                    has_side_effects=False,
                    complexity=1,
                    docstring=None,
                ),
            ],
            [],
        )

        content = "def _private(): pass\ndef __dunder__(): pass"

        # When
        functions, _ = workflow._extract_functions(content)

        # Then
        assert len(functions) == 1
        assert functions[0]["name"] == "__dunder__"

    def test_returns_error_on_parse_failure(self, workflow, mock_ast_analyzer):
        """Given parse error, when extracting, then returns error message."""
        # Given
        mock_ast_analyzer.analyze.return_value = ([], [])
        mock_ast_analyzer.last_error = "SyntaxError: invalid syntax"

        content = "invalid python code {"

        # When
        functions, error = workflow._extract_functions(content)

        # Then
        assert error == "SyntaxError: invalid syntax"


class TestExtractClasses:
    """Test class extraction."""

    def test_extracts_class_details(self, workflow, mock_ast_analyzer):
        """Given Python code, when extracting, then returns class details."""
        # Given
        mock_method = Mock(
            name="process",
            params=[("self", None, None), ("data", "str", None)],
            is_async=False,
            raises=set(),
        )

        mock_ast_analyzer.analyze.return_value = (
            [],
            [
                Mock(
                    name="DataProcessor",
                    init_params=[("self", None, None), ("config", "dict", None)],
                    methods=[mock_method],
                    base_classes=["BaseProcessor"],
                    docstring="Process data",
                    is_dataclass=False,
                    is_enum=False,
                    required_init_params=["config"],
                )
            ],
        )

        content = "class DataProcessor:\n    pass"

        # When
        classes, _ = workflow._extract_classes(content)

        # Then
        assert len(classes) == 1
        assert classes[0]["name"] == "DataProcessor"
        assert len(classes[0]["methods"]) == 1
        assert "BaseProcessor" in classes[0]["base_classes"]

    def test_respects_max_classes_limit(self, workflow, mock_ast_analyzer):
        """Given many classes, when extracting, then respects limit."""
        # Given
        mock_classes = [
            Mock(
                name=f"Class{i}",
                init_params=[],
                methods=[],
                base_classes=[],
                docstring=None,
                is_dataclass=False,
                is_enum=False,
                required_init_params=[],
            )
            for i in range(20)
        ]
        mock_ast_analyzer.analyze.return_value = ([], mock_classes)

        content = "# many classes"

        # When
        classes, _ = workflow._extract_classes(content, max_classes=5)

        # Then
        assert len(classes) == 5

    def test_skips_enum_classes(self, workflow, mock_ast_analyzer):
        """Given enum classes, when extracting, then excludes them."""
        # Given
        mock_ast_analyzer.analyze.return_value = (
            [],
            [
                Mock(
                    name="Color",
                    init_params=[],
                    methods=[],
                    base_classes=["Enum"],
                    docstring=None,
                    is_dataclass=False,
                    is_enum=True,
                    required_init_params=[],
                )
            ],
        )

        content = "class Color(Enum): pass"

        # When
        classes, _ = workflow._extract_classes(content)

        # Then
        assert len(classes) == 0

    def test_includes_init_method(self, workflow, mock_ast_analyzer):
        """Given __init__ method, when extracting, then includes it."""
        # Given
        mock_init = Mock(
            name="__init__",
            params=[("self", None, None)],
            is_async=False,
            raises=set(),
        )

        mock_ast_analyzer.analyze.return_value = (
            [],
            [
                Mock(
                    name="MyClass",
                    init_params=[],
                    methods=[mock_init],
                    base_classes=[],
                    docstring=None,
                    is_dataclass=False,
                    is_enum=False,
                    required_init_params=[],
                )
            ],
        )

        content = "class MyClass:\n    def __init__(self): pass"

        # When
        classes, _ = workflow._extract_classes(content)

        # Then
        assert len(classes[0]["methods"]) == 1
        assert classes[0]["methods"][0]["name"] == "__init__"


class TestGenerateSuggestions:
    """Test suggestion generation."""

    def test_generates_function_suggestions(self, workflow):
        """Given functions, when generating suggestions, then creates test ideas."""
        # Given
        functions = [
            {
                "name": "calculate",
                "params": [("x", "int", None)],
                "is_async": False,
            }
        ]
        classes = []

        # When
        suggestions = workflow._generate_suggestions(functions, classes)

        # Then
        assert len(suggestions) > 0
        assert any("calculate" in s for s in suggestions)

    def test_generates_async_suggestions(self, workflow):
        """Given async functions, when generating suggestions, then includes async tests."""
        # Given
        functions = [
            {
                "name": "fetch_data",
                "params": [],
                "is_async": True,
            }
        ]
        classes = []

        # When
        suggestions = workflow._generate_suggestions(functions, classes)

        # Then
        assert any("async" in s.lower() for s in suggestions)

    def test_generates_class_suggestions(self, workflow):
        """Given classes, when generating suggestions, then creates class test ideas."""
        # Given
        functions = []
        classes = [
            {
                "name": "DataProcessor",
                "init_params": [],
                "methods": [],
            }
        ]

        # When
        suggestions = workflow._generate_suggestions(functions, classes)

        # Then
        assert len(suggestions) > 0
        assert any("DataProcessor" in s for s in suggestions)

    def test_limits_suggestions(self, workflow):
        """Given many items, when generating suggestions, then limits output."""
        # Given
        functions = [
            {"name": f"func{i}", "params": [], "is_async": False} for i in range(10)
        ]
        classes = [
            {"name": f"Class{i}", "init_params": [], "methods": []} for i in range(5)
        ]

        # When
        suggestions = workflow._generate_suggestions(functions, classes)

        # Then
        # Should only process first 5 functions and 3 classes
        assert len(suggestions) <= 20


class TestGenerateStage:
    """Test test generation stage."""

    @pytest.mark.asyncio
    async def test_generates_function_tests(
        self, workflow, mock_test_templates, mocker
    ):
        """Given functions, when generating, then creates test code."""
        # Given
        from empathy_os.workflows.base import ModelTier

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": [
                        {
                            "name": "calculate",
                            "params": [],
                            "is_async": False,
                        }
                    ],
                    "classes": [],
                }
            ],
            "config": {},
        }

        # When
        result, _, _ = await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        assert "generated_tests" in result
        assert result["total_tests_generated"] == 1
        mock_test_templates["function"].assert_called_once()

    @pytest.mark.asyncio
    async def test_generates_class_tests(self, workflow, mock_test_templates, mocker):
        """Given classes, when generating, then creates test code."""
        # Given
        from empathy_os.workflows.base import ModelTier

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": [],
                    "classes": [
                        {
                            "name": "DataProcessor",
                            "init_params": [],
                            "methods": [],
                        }
                    ],
                }
            ],
            "config": {},
        }

        # When
        result, _, _ = await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        assert result["total_tests_generated"] == 1
        mock_test_templates["class"].assert_called_once()

    @pytest.mark.asyncio
    async def test_writes_tests_when_enabled(self, workflow, mock_test_templates, mocker):
        """Given write_tests enabled, when generating, then writes files."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_output = Mock()
        mock_file = Mock()
        mock_output.__truediv__.return_value = mock_file
        mock_path.return_value = mock_output

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": [{"name": "test_func", "params": [], "is_async": False}],
                    "classes": [],
                }
            ],
            "config": {},
            "write_tests": True,
            "output_dir": "tests/output",
        }

        # When
        result, _, _ = await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        assert result["tests_written"] is True
        assert len(result["written_files"]) > 0
        mock_file.write_text.assert_called_once()

    @pytest.mark.asyncio
    async def test_skips_writing_when_disabled(
        self, workflow, mock_test_templates, mocker
    ):
        """Given write_tests disabled, when generating, then does not write files."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": [{"name": "test_func", "params": [], "is_async": False}],
                    "classes": [],
                }
            ],
            "config": {},
            "write_tests": False,
        }

        # When
        result, _, _ = await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        assert result["tests_written"] is False
        assert len(result["written_files"]) == 0

    @pytest.mark.asyncio
    async def test_respects_generation_limits(
        self, workflow, mock_test_templates, mocker
    ):
        """Given generation limits, when generating, then respects them."""
        # Given
        from empathy_os.workflows.base import ModelTier

        functions = [{"name": f"func{i}", "params": [], "is_async": False} for i in range(20)]
        classes = [{"name": f"Class{i}", "init_params": [], "methods": []} for i in range(10)]

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": functions,
                    "classes": classes,
                }
            ],
            "config": {
                "max_files_to_generate": 1,
                "max_functions_to_generate": 5,
                "max_classes_to_generate": 2,
            },
        }

        # When
        result, _, _ = await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        # Should generate max 5 function tests + 2 class tests = 7
        assert result["total_tests_generated"] == 7

    @pytest.mark.asyncio
    async def test_combines_imports_in_output(
        self, workflow, mock_test_templates, mocker
    ):
        """Given multiple tests, when writing, then deduplicates imports."""
        # Given
        from empathy_os.workflows.base import ModelTier

        mock_test_templates["function"].side_effect = [
            "import pytest\n\ndef test_one(): pass",
            "import pytest\nimport json\n\ndef test_two(): pass",
        ]

        mock_path = mocker.patch("empathy_os.workflows.test_gen.workflow.Path")
        mock_output = Mock()
        mock_file = Mock()
        mock_output.__truediv__.return_value = mock_file
        mock_path.return_value = mock_output

        input_data = {
            "analysis": [
                {
                    "file": "src/module.py",
                    "priority": 50,
                    "functions": [
                        {"name": "func1", "params": [], "is_async": False},
                        {"name": "func2", "params": [], "is_async": False},
                    ],
                    "classes": [],
                }
            ],
            "config": {},
            "write_tests": True,
        }

        # When
        await workflow._generate(input_data, ModelTier.CAPABLE)

        # Then
        written_content = mock_file.write_text.call_args[0][0]
        # Should have deduplicated imports
        assert written_content.count("import pytest") == 1


class TestMainFunction:
    """Test CLI entry point."""

    @pytest.mark.asyncio
    async def test_main_executes_workflow(self, mocker, mock_path_for_init):
        """Given main called, when executing, then runs workflow and prints results."""
        # Given
        mock_execute = mocker.patch.object(
            TestGenerationWorkflow,
            "execute",
            return_value=Mock(
                provider="test",
                success=True,
                final_output={"total_tests": 5},
                cost_report=Mock(
                    total_cost=0.10, savings=0.05, savings_percent=50.0
                ),
            ),
        )

        mock_print = mocker.patch("builtins.print")

        # When
        await main.__wrapped__()

        # Then
        mock_execute.assert_called_once()
        assert mock_print.call_count > 0