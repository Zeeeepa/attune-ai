"""Behavioral tests for hybrid.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import hashlib
import time
from pathlib import Path
from typing import Any
from unittest.mock import Mock, MagicMock, patch, call

import numpy as np
import pytest

from empathy_os.cache.hybrid import HybridCache, cosine_similarity
from empathy_os.cache.base import CacheEntry, CacheStats


@pytest.fixture
def mock_sentence_transformer():
    """Mock SentenceTransformer for testing."""
    with patch("empathy_os.cache.hybrid.SentenceTransformer") as mock_st:
        model = MagicMock()
        # Return fixed embeddings for predictable testing
        model.encode.return_value = np.array([0.1, 0.2, 0.3, 0.4])
        mock_st.return_value = model
        yield mock_st


@pytest.fixture
def mock_cache_storage():
    """Mock CacheStorage for testing."""
    with patch("empathy_os.cache.hybrid.CacheStorage") as mock_storage:
        storage = MagicMock()
        storage.load_cache.return_value = {}
        storage.get_cache_size_mb.return_value = 0.0
        mock_storage.return_value = storage
        yield storage


@pytest.fixture
def temp_cache_dir(tmp_path):
    """Create temporary cache directory."""
    cache_dir = tmp_path / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


@pytest.fixture
def hybrid_cache(mock_sentence_transformer, mock_cache_storage, temp_cache_dir):
    """Create HybridCache instance with mocked dependencies."""
    cache = HybridCache(
        max_size_mb=100,
        default_ttl=3600,
        max_memory_mb=50,
        similarity_threshold=0.95,
        model_name="all-MiniLM-L6-v2",
        device="cpu",
        cache_dir=temp_cache_dir,
    )
    return cache


class TestCosineSimilarity:
    """Tests for cosine_similarity function."""

    def test_identical_vectors_returns_one(self):
        """
        Given: Two identical vectors
        When: Calculating cosine similarity
        Then: Should return 1.0
        """
        # Given
        vector = np.array([1.0, 2.0, 3.0])

        # When
        similarity = cosine_similarity(vector, vector)

        # Then
        assert similarity == pytest.approx(1.0)

    def test_orthogonal_vectors_returns_zero(self):
        """
        Given: Two orthogonal vectors
        When: Calculating cosine similarity
        Then: Should return 0.0
        """
        # Given
        a = np.array([1.0, 0.0, 0.0])
        b = np.array([0.0, 1.0, 0.0])

        # When
        similarity = cosine_similarity(a, b)

        # Then
        assert similarity == pytest.approx(0.0)

    def test_opposite_vectors_returns_negative_one(self):
        """
        Given: Two opposite vectors
        When: Calculating cosine similarity
        Then: Should return -1.0
        """
        # Given
        a = np.array([1.0, 2.0, 3.0])
        b = np.array([-1.0, -2.0, -3.0])

        # When
        similarity = cosine_similarity(a, b)

        # Then
        assert similarity == pytest.approx(-1.0)

    def test_similar_vectors_returns_high_similarity(self):
        """
        Given: Two similar but not identical vectors
        When: Calculating cosine similarity
        Then: Should return high similarity score
        """
        # Given
        a = np.array([1.0, 2.0, 3.0])
        b = np.array([1.1, 2.1, 2.9])

        # When
        similarity = cosine_similarity(a, b)

        # Then
        assert 0.98 < similarity < 1.0

    def test_zero_magnitude_vector_handles_correctly(self):
        """
        Given: Vector with zero magnitude
        When: Calculating cosine similarity
        Then: Should handle division by zero gracefully
        """
        # Given
        a = np.array([1.0, 2.0, 3.0])
        b = np.array([0.0, 0.0, 0.0])

        # When/Then - should not raise error
        try:
            similarity = cosine_similarity(a, b)
            assert np.isnan(similarity) or np.isinf(similarity)
        except ZeroDivisionError:
            pytest.fail("Should handle zero magnitude gracefully")


class TestHybridCacheInitialization:
    """Tests for HybridCache initialization."""

    def test_initialization_with_defaults(self, mock_sentence_transformer, mock_cache_storage, temp_cache_dir):
        """
        Given: Default initialization parameters
        When: Creating HybridCache instance
        Then: Should initialize with correct defaults
        """
        # When
        cache = HybridCache(cache_dir=temp_cache_dir)

        # Then
        assert cache.max_size_mb == 500
        assert cache.default_ttl == 86400
        assert cache.max_memory_mb == 100
        assert cache.similarity_threshold == 0.95
        assert cache.model_name == "all-MiniLM-L6-v2"
        assert cache.device == "cpu"

    def test_initialization_with_custom_parameters(self, mock_sentence_transformer, mock_cache_storage, temp_cache_dir):
        """
        Given: Custom initialization parameters
        When: Creating HybridCache instance
        Then: Should initialize with custom values
        """
        # When
        cache = HybridCache(
            max_size_mb=200,
            default_ttl=7200,
            max_memory_mb=75,
            similarity_threshold=0.90,
            model_name="custom-model",
            device="cuda",
            cache_dir=temp_cache_dir,
        )

        # Then
        assert cache.max_size_mb == 200
        assert cache.default_ttl == 7200
        assert cache.max_memory_mb == 75
        assert cache.similarity_threshold == 0.90
        assert cache.model_name == "custom-model"
        assert cache.device == "cuda"

    def test_initialization_loads_model(self, mock_sentence_transformer, mock_cache_storage, temp_cache_dir):
        """
        Given: HybridCache initialization
        When: Creating instance
        Then: Should load sentence transformer model
        """
        # When
        cache = HybridCache(cache_dir=temp_cache_dir, model_name="test-model")

        # Then
        mock_sentence_transformer.assert_called_once_with("test-model", device="cpu")

    def test_initialization_loads_storage(self, mock_sentence_transformer, mock_cache_storage, temp_cache_dir):
        """
        Given: HybridCache initialization
        When: Creating instance
        Then: Should initialize cache storage
        """
        # When
        cache = HybridCache(cache_dir=temp_cache_dir)

        # Then
        assert mock_cache_storage.called


class TestHybridCacheHashGeneration:
    """Tests for hash generation."""

    def test_make_key_generates_consistent_hash(self, hybrid_cache):
        """
        Given: Same input parameters
        When: Generating cache keys multiple times
        Then: Should return identical hash keys
        """
        # Given
        mode = "code-review"
        task = "scan"
        prompt = "Add auth middleware"
        model = "sonnet"

        # When
        key1 = hybrid_cache._make_key(mode, task, prompt, model)
        key2 = hybrid_cache._make_key(mode, task, prompt, model)

        # Then
        assert key1 == key2
        assert isinstance(key1, str)
        assert len(key1) == 64  # SHA-256 hex digest

    def test_make_key_different_for_different_inputs(self, hybrid_cache):
        """
        Given: Different input parameters
        When: Generating cache keys
        Then: Should return different hash keys
        """
        # Given
        key1 = hybrid_cache._make_key("mode1", "task1", "prompt1", "model1")
        key2 = hybrid_cache._make_key("mode2", "task2", "prompt2", "model2")

        # Then
        assert key1 != key2

    def test_make_key_handles_special_characters(self, hybrid_cache):
        """
        Given: Input with special characters
        When: Generating cache key
        Then: Should handle gracefully
        """
        # Given
        prompt = "Test\nwith\tspecial\rcharsâ„¢"

        # When
        key = hybrid_cache._make_key("mode", "task", prompt, "model")

        # Then
        assert isinstance(key, str)
        assert len(key) == 64


class TestHybridCacheEmbeddings:
    """Tests for embedding generation."""

    def test_get_embedding_calls_model(self, hybrid_cache):
        """
        Given: Text input
        When: Getting embedding
        Then: Should call model encoder
        """
        # Given
        text = "Test prompt"

        # When
        embedding = hybrid_cache._get_embedding(text)

        # Then
        hybrid_cache.model.encode.assert_called()
        assert isinstance(embedding, np.ndarray)

    def test_get_embedding_returns_numpy_array(self, hybrid_cache):
        """
        Given: Text input
        When: Getting embedding
        Then: Should return numpy array
        """
        # Given
        text = "Test prompt"

        # When
        embedding = hybrid_cache._get_embedding(text)

        # Then
        assert isinstance(embedding, np.ndarray)

    def test_get_embedding_handles_empty_string(self, hybrid_cache):
        """
        Given: Empty string input
        When: Getting embedding
        Then: Should handle gracefully
        """
        # Given
        text = ""

        # When
        embedding = hybrid_cache._get_embedding(text)

        # Then
        assert isinstance(embedding, np.ndarray)


class TestHybridCacheGet:
    """Tests for cache retrieval."""

    def test_get_returns_none_for_empty_cache(self, hybrid_cache):
        """
        Given: Empty cache
        When: Getting non-existent key
        Then: Should return None
        """
        # When
        result = hybrid_cache.get("mode", "task", "prompt", "model")

        # Then
        assert result is None

    def test_get_returns_exact_match_from_hash_cache(self, hybrid_cache):
        """
        Given: Entry in hash cache
        When: Getting with exact match
        Then: Should return cached response
        """
        # Given
        mode, task, prompt, model = "mode", "task", "prompt", "model"
        expected_response = {"result": "test"}
        hybrid_cache.put(mode, task, prompt, model, expected_response)

        # When
        result = hybrid_cache.get(mode, task, prompt, model)

        # Then
        assert result == expected_response

    def test_get_increments_hash_hits_for_exact_match(self, hybrid_cache):
        """
        Given: Entry in hash cache
        When: Getting with exact match
        Then: Should increment hash hits counter
        """
        # Given
        mode, task, prompt, model = "mode", "task", "prompt", "model"
        hybrid_cache.put(mode, task, prompt, model, {"result": "test"})
        initial_hash_hits = hybrid_cache.stats.hash_hits

        # When
        hybrid_cache.get(mode, task, prompt, model)

        # Then
        assert hybrid_cache.stats.hash_hits == initial_hash_hits + 1

    def test_get_finds_semantic_match_above_threshold(self, hybrid_cache):
        """
        Given: Similar but not exact entry in cache
        When: Getting with high similarity
        Then: Should return semantic match
        """
        # Given
        mode, task, model = "mode", "task", "model"
        original_prompt = "Add auth middleware"
        similar_prompt = "Add authentication middleware"
        expected_response = {"result": "test"}

        # Mock high similarity
        with patch.object(hybrid_cache, "_get_embedding") as mock_embed:
            embed1 = np.array([1.0, 0.0, 0.0, 0.0])
            embed2 = np.array([0.96, 0.0, 0.0, 0.0])  # 96% similar
            mock_embed.side_effect = [embed1, embed2]

            hybrid_cache.put(mode, task, original_prompt, model, expected_response)
            mock_embed.side_effect = [embed2]

            # When
            result = hybrid_cache.get(mode, task, similar_prompt, model)

            # Then
            assert result == expected_response

    def test_get_increments_semantic_hits_for_similarity_match(self, hybrid_cache):
        """
        Given: Similar entry in cache
        When: Getting with semantic match
        Then: Should increment semantic hits counter
        """
        # Given
        mode, task, model = "mode", "task", "model"
        hybrid_cache.similarity_threshold = 0.90

        with patch.object(hybrid_cache, "_get_embedding") as mock_embed:
            embed1 = np.array([1.0, 0.0, 0.0, 0.0])
            embed2 = np.array([0.95, 0.0, 0.0, 0.0])
            mock_embed.side_effect = [embed1, embed2]

            hybrid_cache.put(mode, task, "prompt1", model, {"result": "test"})
            initial_semantic_hits = hybrid_cache.stats.semantic_hits

            mock_embed.side_effect = [embed2]

            # When
            hybrid_cache.get(mode, task, "prompt2", model)

            # Then
            assert hybrid_cache.stats.semantic_hits == initial_semantic_hits + 1

    def test_get_returns_none_for_similarity_below_threshold(self, hybrid_cache):
        """
        Given: Entry in cache with low similarity
        When: Getting with similarity below threshold
        Then: Should return None
        """
        # Given
        mode, task, model = "mode", "task", "model"

        with patch.object(hybrid_cache, "_get_embedding") as mock_embed:
            embed1 = np.array([1.0, 0.0, 0.0, 0.0])
            embed2 = np.array([0.5, 0.5, 0.0, 0.0])  # Low similarity
            mock_embed.side_effect = [embed1, embed2]

            hybrid_cache.put(mode, task, "prompt1", model, {"result": "test"})

            mock_embed.side_effect = [embed2]

            # When
            result = hybrid_cache.get(mode, task, "different_prompt", model)

            # Then
            assert result is None

    def test_get_increments_misses_for_cache_miss(self, hybrid_cache):
        """
        Given: Empty cache
        When: Getting non-existent key
        Then: Should increment misses counter
        """
        # Given
        initial_misses = hybrid_cache.stats.misses

        # When
        hybrid_cache.get("mode", "task", "prompt