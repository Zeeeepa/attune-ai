"""Behavioral tests for llm_analyzer.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

import json
import pytest
from unittest.mock import Mock, patch, MagicMock
from dataclasses import dataclass

from empathy_os.socratic.llm_analyzer import (
    GOAL_ANALYSIS_PROMPT,
    QUESTION_REFINEMENT_PROMPT,
)
from empathy_os.socratic.forms import FieldOption, FieldType, FieldValidation, Form, FormField
from empathy_os.socratic.session import SocraticSession


# =============================================================================
# FIXTURES
# =============================================================================


@pytest.fixture
def sample_goal():
    """Given a sample user goal statement."""
    return "I want to improve code quality in my Python project"


@pytest.fixture
def sample_goal_analysis():
    """Given a sample goal analysis response."""
    return {
        "intent": "Improve code quality through automated reviews",
        "domain": "code_review",
        "confidence": 0.85,
        "ambiguities": [
            "What specific aspects of code quality?",
            "What is the project size?"
        ],
        "assumptions": [
            "User wants automated checking",
            "Python 3.x project"
        ],
        "constraints": ["Python language"],
        "keywords": ["code quality", "Python", "project"],
        "suggested_agents": ["code_quality_reviewer", "style_enforcer"],
        "suggested_questions": [
            {
                "id": "quality_aspects",
                "question": "Which quality aspects matter most?",
                "type": "multi_select",
                "options": ["style", "complexity", "security", "performance"]
            }
        ]
    }


@pytest.fixture
def sample_session():
    """Given a mock SocraticSession."""
    session = Mock(spec=SocraticSession)
    session.goal = "Improve code quality"
    session.answers = {}
    session.requirements = {}
    session.ambiguities = ["unclear scope"]
    return session


@pytest.fixture
def mock_llm_client():
    """Given a mock LLM client."""
    with patch('empathy_os.socratic.llm_analyzer.get_llm_client') as mock:
        client = MagicMock()
        mock.return_value = client
        yield client


# =============================================================================
# PROMPT TESTS
# =============================================================================


class TestPromptConstants:
    """Test suite for prompt constants."""

    def test_given_goal_analysis_prompt_when_accessed_then_contains_required_sections(self):
        """
        Given the GOAL_ANALYSIS_PROMPT constant
        When we examine its content
        Then it should contain all required sections for goal analysis
        """
        assert "{goal}" in GOAL_ANALYSIS_PROMPT
        assert "intent" in GOAL_ANALYSIS_PROMPT
        assert "domain" in GOAL_ANALYSIS_PROMPT
        assert "confidence" in GOAL_ANALYSIS_PROMPT
        assert "ambiguities" in GOAL_ANALYSIS_PROMPT
        assert "suggested_agents" in GOAL_ANALYSIS_PROMPT
        assert "suggested_questions" in GOAL_ANALYSIS_PROMPT

    def test_given_question_refinement_prompt_when_accessed_then_contains_required_placeholders(self):
        """
        Given the QUESTION_REFINEMENT_PROMPT constant
        When we examine its content
        Then it should contain all required placeholders
        """
        assert "{goal}" in QUESTION_REFINEMENT_PROMPT
        assert "{previous_answers}" in QUESTION_REFINEMENT_PROMPT
        assert "{requirements}" in QUESTION_REFINEMENT_PROMPT
        assert "{ambiguities}" in QUESTION_REFINEMENT_PROMPT


# =============================================================================
# MODULE IMPORT TESTS
# =============================================================================


class TestModuleImports:
    """Test suite for module-level imports and structure."""

    def test_given_module_when_imported_then_required_classes_available(self):
        """
        Given the llm_analyzer module
        When we import it
        Then all required classes and functions should be available
        """
        from empathy_os.socratic import llm_analyzer
        
        # Check that prompts are accessible
        assert hasattr(llm_analyzer, 'GOAL_ANALYSIS_PROMPT')
        assert hasattr(llm_analyzer, 'QUESTION_REFINEMENT_PROMPT')
        
        # Check logger is configured
        assert hasattr(llm_analyzer, 'logger')

    def test_given_module_when_imported_then_dependencies_available(self):
        """
        Given the llm_analyzer module
        When we check its dependencies
        Then forms and session modules should be properly imported
        """
        from empathy_os.socratic import llm_analyzer
        
        # Verify dependencies are accessible
        assert llm_analyzer.Form is not None
        assert llm_analyzer.FormField is not None
        assert llm_analyzer.SocraticSession is not None


# =============================================================================
# LLM ANALYZER CLASS TESTS
# =============================================================================


@pytest.fixture
def llm_analyzer_class():
    """Given an LLM analyzer class (if it exists in the module)."""
    try:
        from empathy_os.socratic.llm_analyzer import LLMAnalyzer
        return LLMAnalyzer
    except ImportError:
        return None


class TestLLMAnalyzerClass:
    """Test suite for LLM analyzer class if present."""

    def test_given_llm_analyzer_when_initialized_then_has_required_attributes(self, llm_analyzer_class):
        """
        Given an LLMAnalyzer class
        When we initialize it
        Then it should have required attributes
        """
        if llm_analyzer_class is None:
            pytest.skip("LLMAnalyzer class not found in module")
        
        analyzer = llm_analyzer_class()
        # Add specific attribute checks based on actual implementation
        assert analyzer is not None

    def test_given_llm_analyzer_when_analyze_goal_called_then_returns_structured_data(
        self, llm_analyzer_class, sample_goal, mock_llm_client, sample_goal_analysis
    ):
        """
        Given an LLMAnalyzer instance
        When analyze_goal is called with a goal
        Then it should return structured analysis data
        """
        if llm_analyzer_class is None:
            pytest.skip("LLMAnalyzer class not found in module")
        
        # Setup mock response
        mock_llm_client.complete.return_value = json.dumps(sample_goal_analysis)
        
        analyzer = llm_analyzer_class(client=mock_llm_client)
        result = analyzer.analyze_goal(sample_goal)
        
        assert "intent" in result
        assert "domain" in result
        assert "confidence" in result
        assert isinstance(result["confidence"], (int, float))
        assert 0.0 <= result["confidence"] <= 1.0


# =============================================================================
# GOAL ANALYSIS FUNCTION TESTS
# =============================================================================


@pytest.fixture
def analyze_goal_function():
    """Given an analyze_goal function (if it exists)."""
    try:
        from empathy_os.socratic.llm_analyzer import analyze_goal
        return analyze_goal
    except ImportError:
        return None


class TestAnalyzeGoalFunction:
    """Test suite for analyze_goal function."""

    def test_given_valid_goal_when_analyzed_then_returns_complete_structure(
        self, analyze_goal_function, sample_goal, mock_llm_client, sample_goal_analysis
    ):
        """
        Given a valid goal statement
        When analyze_goal is called
        Then it should return a complete analysis structure
        """
        if analyze_goal_function is None:
            pytest.skip("analyze_goal function not found")
        
        mock_llm_client.complete.return_value = json.dumps(sample_goal_analysis)
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            result = analyze_goal_function(sample_goal)
        
        assert result["intent"] == sample_goal_analysis["intent"]
        assert result["domain"] == sample_goal_analysis["domain"]
        assert result["confidence"] == sample_goal_analysis["confidence"]
        assert len(result["ambiguities"]) > 0
        assert len(result["suggested_agents"]) > 0

    def test_given_empty_goal_when_analyzed_then_handles_gracefully(
        self, analyze_goal_function, mock_llm_client
    ):
        """
        Given an empty goal string
        When analyze_goal is called
        Then it should handle the edge case gracefully
        """
        if analyze_goal_function is None:
            pytest.skip("analyze_goal function not found")
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            mock_llm_client.complete.return_value = json.dumps({
                "intent": "Unclear",
                "domain": "general",
                "confidence": 0.1,
                "ambiguities": ["Goal statement is empty"],
                "assumptions": [],
                "constraints": [],
                "keywords": [],
                "suggested_agents": [],
                "suggested_questions": []
            })
            
            result = analyze_goal_function("")
            
            assert result["confidence"] < 0.5
            assert len(result["ambiguities"]) > 0

    def test_given_llm_returns_invalid_json_when_analyzed_then_raises_error(
        self, analyze_goal_function, sample_goal, mock_llm_client
    ):
        """
        Given an LLM that returns invalid JSON
        When analyze_goal is called
        Then it should raise an appropriate error
        """
        if analyze_goal_function is None:
            pytest.skip("analyze_goal function not found")
        
        mock_llm_client.complete.return_value = "Invalid JSON {{"
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            with pytest.raises((json.JSONDecodeError, ValueError)):
                analyze_goal_function(sample_goal)

    def test_given_goal_with_specific_domain_when_analyzed_then_identifies_correct_domain(
        self, analyze_goal_function, mock_llm_client
    ):
        """
        Given a goal with specific domain keywords
        When analyze_goal is called
        Then it should identify the correct domain
        """
        if analyze_goal_function is None:
            pytest.skip("analyze_goal function not found")
        
        security_goal = "I need to find security vulnerabilities in my code"
        
        mock_llm_client.complete.return_value = json.dumps({
            "intent": "Find security vulnerabilities",
            "domain": "security",
            "confidence": 0.9,
            "ambiguities": [],
            "assumptions": [],
            "constraints": [],
            "keywords": ["security", "vulnerabilities"],
            "suggested_agents": ["security_reviewer"],
            "suggested_questions": []
        })
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            result = analyze_goal_function(security_goal)
        
        assert result["domain"] == "security"
        assert "security_reviewer" in result["suggested_agents"]


# =============================================================================
# QUESTION GENERATION TESTS
# =============================================================================


@pytest.fixture
def generate_questions_function():
    """Given a generate_questions function (if it exists)."""
    try:
        from empathy_os.socratic.llm_analyzer import generate_questions
        return generate_questions
    except ImportError:
        return None


class TestGenerateQuestionsFunction:
    """Test suite for question generation functionality."""

    def test_given_session_when_generating_questions_then_returns_valid_questions(
        self, generate_questions_function, sample_session, mock_llm_client
    ):
        """
        Given a SocraticSession
        When generate_questions is called
        Then it should return valid question structures
        """
        if generate_questions_function is None:
            pytest.skip("generate_questions function not found")
        
        mock_response = {
            "questions": [
                {
                    "id": "q1",
                    "question": "What is your primary concern?",
                    "type": "single_select",
                    "options": ["performance", "security", "maintainability"]
                }
            ]
        }
        
        mock_llm_client.complete.return_value = json.dumps(mock_response)
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            result = generate_questions_function(sample_session)
        
        assert len(result["questions"]) > 0
        question = result["questions"][0]
        assert "id" in question
        assert "question" in question
        assert "type" in question

    def test_given_session_with_answers_when_generating_questions_then_refines_based_on_answers(
        self, generate_questions_function, sample_session, mock_llm_client
    ):
        """
        Given a session with previous answers
        When generate_questions is called
        Then it should generate refined follow-up questions
        """
        if generate_questions_function is None:
            pytest.skip("generate_questions function not found")
        
        sample_session.answers = {"q1": "security"}
        
        mock_response = {
            "questions": [
                {
                    "id": "q2",
                    "question": "What security aspects are most important?",
                    "type": "multi_select",
                    "options": ["authentication", "authorization", "encryption"]
                }
            ]
        }
        
        mock_llm_client.complete.return_value = json.dumps(mock_response)
        
        with patch('empathy_os.socratic.llm_analyzer.get_llm_client', return_value=mock_llm_client):
            result = generate_questions_function(sample_session)
        
        # Should generate follow-up questions
        assert len(result["questions"]) > 0
        assert result["questions"][0]["id"] != "q1"


# =============================================================================
# FORM GENERATION TESTS
# =============================================================================


@pytest.fixture
def create_form_from_questions_function():
    """Given a function to create forms from questions."""
    try:
        from empathy_os.socratic.llm_analyzer import create_form_from_questions
        return create_form_from_questions
    except ImportError:
        return None


class TestFormGenerationFunctions:
    """Test suite for form generation from LLM questions."""

    def test_given_questions_when_creating_form_then_returns_valid_form(
        self, create_form_from_questions_function
    ):
        """
        Given a list of questions
        When create_form_from_questions is called
        Then it should return a valid Form object
        """
        if create_form_from_questions_function is None:
            pytest.skip("create_form_from_questions function not found")
        
        questions = [
            {
                "id": "q1",
                "question": "Select options",
                "type": "multi_select",
                "options": ["opt1", "opt2"]
            }
        ]
        
        form = create_form_from_questions_function(questions)
        
        assert isinstance(form, Form)
        assert len(form.fields) > 0
        assert form.fields[0].id == "q1"

    def test_given_text_question_when_creating_form_then_creates_text_field(
        self