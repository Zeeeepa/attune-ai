"""Behavioral tests for security_audit.

Generated by LLM-enhanced test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
from unittest.mock import AsyncMock, patch

import pytest

from attune.workflows.base import ModelTier
from attune.workflows.security_audit import SecurityAuditWorkflow


@pytest.fixture
def mock_workflow():
    """Create a mock SecurityAuditWorkflow instance for testing."""
    return SecurityAuditWorkflow()


def test_security_audit_workflow_initialization(mock_workflow):
    """Verify SecurityAuditWorkflow can be initialized correctly."""
    assert mock_workflow is not None
    assert hasattr(mock_workflow, "_team_decisions")
    assert hasattr(mock_workflow, "_crew")


@pytest.mark.parametrize(
    "stage_name,input_data,expected_skip",
    [
        ("remediate", {"findings": []}, True),
        ("triage", {"path": "."}, False),
        ("analyze", {"findings": []}, False),
    ],
)
def test_should_skip_stage_with_directory_rules(
    mock_workflow, stage_name, input_data, expected_skip
):
    """Test stage skipping logic works as expected."""
    # should_skip_stage returns a tuple (should_skip, reason)
    # remediate is skipped when _has_critical is False (default)
    result, reason = mock_workflow.should_skip_stage(stage_name, input_data)
    assert result == expected_skip
    if expected_skip:
        assert reason is not None
    else:
        assert reason is None


def test_is_detection_code_identifies_detection_patterns():
    """Verify detection code identification works correctly."""
    workflow = SecurityAuditWorkflow()

    # _is_detection_code takes (line_content, match_text) - two arguments
    detection_samples = [
        ('print("eval(" in code)', "eval("),
        ("def test_regex(): re.compile(pattern)", "eval("),
        ('check = ".search(" in text', ".search("),
    ]
    non_detection_samples = [
        ("result = eval(dangerous_code)", "eval(dangerous_code)"),
        ("execute_command(input)", "execute_command(input)"),
        ("normal_function()", "normal_function()"),
    ]

    for line_content, match_text in detection_samples:
        assert workflow._is_detection_code(
            line_content, match_text
        ), f"Failed to detect: {line_content}"

    for line_content, match_text in non_detection_samples:
        assert not workflow._is_detection_code(
            line_content, match_text
        ), f"Incorrectly detected: {line_content}"


def test_is_fake_credential_identifies_test_credentials():
    """Verify fake credential detection works correctly."""
    workflow = SecurityAuditWorkflow()
    fake_credentials = ['"EXAMPLE_KEY"', '"your-key-here"', '"mock_secret"', '"TEST_PASSWORD"']
    real_credentials = ['"actual_aws_access_key"', '"production_db_password"', '"real_secret_key"']

    for cred in fake_credentials:
        assert workflow._is_fake_credential(cred), f"Failed to detect fake: {cred}"

    for cred in real_credentials:
        assert not workflow._is_fake_credential(cred), f"Incorrectly detected real: {cred}"


@pytest.mark.parametrize("stage", ["triage", "analyze", "assess", "remediate"])
@pytest.mark.asyncio
async def test_run_stage_handles_different_workflow_stages(mock_workflow, stage):
    """Test run_stage handles different workflow stages."""
    mock_context = {"path": "."}
    tier = ModelTier.CHEAP
    mock_return = ({"result": "ok"}, 10, 10)

    with patch.object(
        mock_workflow, f"_{stage}", new_callable=AsyncMock, return_value=mock_return
    ) as mock_stage:
        result = await mock_workflow.run_stage(stage, tier, mock_context)
        mock_stage.assert_called_once_with(mock_context, tier)
        assert result == mock_return


def test_initialize_crew_sets_up_workflow_crew(mock_workflow):
    """Verify crew initialization sets correct crew configuration."""
    # _initialize_crew is async, but we can test the initial state
    assert mock_workflow._crew is None
    assert mock_workflow._crew_available is False


def test_load_team_decisions_parses_security_configuration(tmp_path):
    """Test loading team security decisions from configuration."""
    # Create the expected directory structure
    security_dir = tmp_path / "security"
    security_dir.mkdir()

    mock_config = {
        "decisions": [
            {"finding_hash": "test_hash_1", "decision": "false_positive", "reason": "Test pattern"},
            {"finding_hash": "test_hash_2", "decision": "accepted", "reason": "Accepted risk"},
        ]
    }

    decisions_file = security_dir / "team_decisions.json"
    decisions_file.write_text(json.dumps(mock_config))

    # Create workflow with patterns_dir pointing to tmp_path
    workflow = SecurityAuditWorkflow(patterns_dir=str(tmp_path))

    assert "test_hash_1" in workflow._team_decisions
    assert workflow._team_decisions["test_hash_1"]["decision"] == "false_positive"
    assert "test_hash_2" in workflow._team_decisions
    assert workflow._team_decisions["test_hash_2"]["decision"] == "accepted"


@pytest.mark.parametrize(
    "input_data,expected_substring",
    [
        ({"type": "sql_injection"}, "SQL injection"),
        ({"type": "xss"}, "XSS"),
        ({"type": "unknown_type"}, "Review for security implications"),
    ],
)
def test_analyze_finding_handles_different_severity_levels(
    mock_workflow, input_data, expected_substring
):
    """Test analysis of security findings returns appropriate analysis strings."""
    # _analyze_finding returns a string, not a boolean
    result = mock_workflow._analyze_finding(input_data)
    assert isinstance(result, str)
    assert expected_substring in result


@pytest.mark.asyncio
async def test_triage_identifies_potential_vulnerabilities(mock_workflow, tmp_path):
    """Verify triage stage identifies potential security issues."""
    # Create a test file with a known vulnerability pattern
    test_file = tmp_path / "vulnerable.py"
    test_file.write_text("result = eval(user_input)\n")

    input_data = {"path": str(tmp_path), "file_types": [".py"]}
    tier = ModelTier.CHEAP

    result, input_tokens, output_tokens = await mock_workflow._triage(input_data, tier)

    assert result is not None
    assert "findings" in result
    assert "files_scanned" in result
    assert result["files_scanned"] >= 1
