"""Behavioral tests for manage_documentation.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import asyncio
import warnings
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, Mock, patch

import pytest

from empathy_os.workflows.manage_documentation import (
    Agent,
    ManageDocumentationCrew,
    ManageDocumentationCrewResult,
)


class TestManageDocumentationCrewResult:
    """Test suite for ManageDocumentationCrewResult dataclass."""

    def test_default_initialization(self):
        """Given default parameters
        When creating a ManageDocumentationCrewResult
        Then all fields should have expected default values
        """
        # Given/When
        result = ManageDocumentationCrewResult(success=True)

        # Then
        assert result.success is True
        assert result.findings == []
        assert result.recommendations == []
        assert result.files_analyzed == 0
        assert result.docs_needing_update == 0
        assert result.new_docs_needed == 0
        assert result.confidence == 0.0
        assert result.cost == 0.0
        assert result.duration_ms == 0
        assert result.formatted_report == ""

    def test_full_initialization(self):
        """Given all parameters specified
        When creating a ManageDocumentationCrewResult
        Then all fields should match provided values
        """
        # Given
        findings = [{"file": "test.py", "issue": "missing docstring"}]
        recommendations = ["Add docstrings to all functions"]

        # When
        result = ManageDocumentationCrewResult(
            success=True,
            findings=findings,
            recommendations=recommendations,
            files_analyzed=5,
            docs_needing_update=2,
            new_docs_needed=1,
            confidence=0.95,
            cost=0.05,
            duration_ms=1500,
            formatted_report="Test report",
        )

        # Then
        assert result.success is True
        assert result.findings == findings
        assert result.recommendations == recommendations
        assert result.files_analyzed == 5
        assert result.docs_needing_update == 2
        assert result.new_docs_needed == 1
        assert result.confidence == 0.95
        assert result.cost == 0.05
        assert result.duration_ms == 1500
        assert result.formatted_report == "Test report"

    def test_to_dict_conversion(self):
        """Given a ManageDocumentationCrewResult instance
        When converting to dictionary
        Then should contain all fields with correct values
        """
        # Given
        result = ManageDocumentationCrewResult(
            success=True,
            findings=[{"test": "data"}],
            recommendations=["rec1", "rec2"],
            files_analyzed=10,
            docs_needing_update=3,
            new_docs_needed=2,
            confidence=0.88,
            cost=0.12,
            duration_ms=2000,
            formatted_report="Report text",
        )

        # When
        result_dict = result.to_dict()

        # Then
        assert isinstance(result_dict, dict)
        assert result_dict["success"] is True
        assert result_dict["findings"] == [{"test": "data"}]
        assert result_dict["recommendations"] == ["rec1", "rec2"]
        assert result_dict["files_analyzed"] == 10
        assert result_dict["docs_needing_update"] == 3
        assert result_dict["new_docs_needed"] == 2
        assert result_dict["confidence"] == 0.88
        assert result_dict["cost"] == 0.12
        assert result_dict["duration_ms"] == 2000
        assert result_dict["formatted_report"] == "Report text"

    def test_to_dict_with_defaults(self):
        """Given a ManageDocumentationCrewResult with minimal parameters
        When converting to dictionary
        Then should include all default values
        """
        # Given
        result = ManageDocumentationCrewResult(success=False)

        # When
        result_dict = result.to_dict()

        # Then
        assert result_dict["success"] is False
        assert result_dict["findings"] == []
        assert result_dict["recommendations"] == []
        assert result_dict["files_analyzed"] == 0


class TestAgent:
    """Test suite for Agent dataclass."""

    def test_default_initialization(self):
        """Given minimal agent parameters
        When creating an Agent
        Then should have correct defaults
        """
        # Given/When
        agent = Agent(
            role="Tester",
            goal="Test things",
            backstory="I test"
        )

        # Then
        assert agent.role == "Tester"
        assert agent.goal == "Test things"
        assert agent.backstory == "I test"
        assert agent.expertise_level == "expert"
        assert agent.use_xml_structure is True

    def test_full_initialization(self):
        """Given all agent parameters
        When creating an Agent
        Then all fields should match provided values
        """
        # Given/When
        agent = Agent(
            role="Developer",
            goal="Write code",
            backstory="Experienced developer",
            expertise_level="senior",
            use_xml_structure=False,
        )

        # Then
        assert agent.role == "Developer"
        assert agent.goal == "Write code"
        assert agent.backstory == "Experienced developer"
        assert agent.expertise_level == "senior"
        assert agent.use_xml_structure is False

    def test_get_system_prompt_with_xml(self):
        """Given an agent with XML structure enabled
        When getting system prompt
        Then should return XML-formatted prompt
        """
        # Given
        agent = Agent(
            role="Documentation Analyst",
            goal="Analyze documentation",
            backstory="Expert in technical writing",
            use_xml_structure=True,
        )

        # When
        prompt = agent.get_system_prompt()

        # Then
        assert isinstance(prompt, str)
        assert "<agent>" in prompt
        assert "<role>Documentation Analyst</role>" in prompt
        assert "<goal>Analyze documentation</goal>" in prompt
        assert "<backstory>Expert in technical writing</backstory>" in prompt
        assert "<expertise_level>expert</expertise_level>" in prompt
        assert "</agent>" in prompt

    def test_get_system_prompt_without_xml(self):
        """Given an agent with XML structure disabled
        When getting system prompt
        Then should return legacy format prompt
        """
        # Given
        agent = Agent(
            role="Code Reviewer",
            goal="Review code",
            backstory="Senior reviewer",
            use_xml_structure=False,
        )

        # When
        prompt = agent.get_system_prompt()

        # Then
        assert isinstance(prompt, str)
        assert "You are a Code Reviewer" in prompt
        assert "Review code" in prompt
        assert "Senior reviewer" in prompt
        assert "<agent>" not in prompt
        assert "<role>" not in prompt


class TestManageDocumentationCrew:
    """Test suite for ManageDocumentationCrew class."""

    def test_initialization_default(self):
        """Given no parameters
        When creating ManageDocumentationCrew
        Then should initialize with default values
        """
        # Given/When
        crew = ManageDocumentationCrew()

        # Then
        assert crew.project_root == Path.cwd()
        assert crew.verbose is False
        assert crew.use_xml_prompts is True
        assert crew.max_workers == 4
        assert len(crew.agents) == 4
        assert crew._executor is None

    def test_initialization_custom_project_root(self):
        """Given a custom project root path
        When creating ManageDocumentationCrew
        Then should use the provided path
        """
        # Given
        custom_path = Path("/custom/path")

        # When
        crew = ManageDocumentationCrew(project_root=custom_path)

        # Then
        assert crew.project_root == custom_path

    def test_initialization_with_string_path(self):
        """Given a string path for project root
        When creating ManageDocumentationCrew
        Then should convert to Path object
        """
        # Given
        path_str = "/tmp/test"

        # When
        crew = ManageDocumentationCrew(project_root=path_str)

        # Then
        assert isinstance(crew.project_root, Path)
        assert crew.project_root == Path(path_str)

    def test_initialization_custom_parameters(self):
        """Given custom parameters
        When creating ManageDocumentationCrew
        Then should respect all custom values
        """
        # Given/When
        crew = ManageDocumentationCrew(
            verbose=True,
            use_xml_prompts=False,
            max_workers=8,
        )

        # Then
        assert crew.verbose is True
        assert crew.use_xml_prompts is False
        assert crew.max_workers == 8

    def test_agents_initialization(self):
        """Given crew initialization
        When accessing agents
        Then should have 4 agents with correct roles
        """
        # Given
        crew = ManageDocumentationCrew()

        # When
        agents = crew.agents

        # Then
        assert len(agents) == 4
        roles = [agent.role for agent in agents]
        assert "Documentation Analyzer" in roles
        assert "Code Reviewer" in roles
        assert "Documentation Writer" in roles
        assert "Quality Assurance Specialist" in roles

    def test_agents_use_xml_structure_setting(self):
        """Given use_xml_prompts setting
        When creating agents
        Then agents should have matching xml structure flag
        """
        # Given/When
        crew_with_xml = ManageDocumentationCrew(use_xml_prompts=True)
        crew_without_xml = ManageDocumentationCrew(use_xml_prompts=False)

        # Then
        assert all(agent.use_xml_structure for agent in crew_with_xml.agents)
        assert not any(agent.use_xml_structure for agent in crew_without_xml.agents)

    @pytest.mark.asyncio
    async def test_execute_task_with_mock_executor(self):
        """Given a crew with mocked executor
        When executing a task
        Then should call executor and return result
        """
        # Given
        crew = ManageDocumentationCrew()
        mock_executor = AsyncMock()
        mock_response = MagicMock()
        mock_response.content = "Test response"
        mock_response.metadata = {"cost": 0.01, "duration_ms": 100}
        mock_executor.execute.return_value = mock_response
        crew._executor = mock_executor

        agent = crew.agents[0]
        task = "Analyze documentation"

        # When
        result = await crew._execute_task(agent, task, {})

        # Then
        assert result["agent_role"] == agent.role
        assert result["task"] == task
        assert result["result"] == "Test response"
        assert result["cost"] == 0.01
        assert result["duration_ms"] == 100
        mock_executor.execute.assert_called_once()

    @pytest.mark.asyncio
    async def test_execute_task_without_executor(self):
        """Given a crew without executor
        When executing a task
        Then should return simulated result
        """
        # Given
        crew = ManageDocumentationCrew()
        crew._executor = None
        agent = crew.agents[0]
        task = "Review code"

        # When
        result = await crew._execute_task(agent, task, {})

        # Then
        assert result["agent_role"] == agent.role
        assert result["task"] == task
        assert "result" in result
        assert result["cost"] == 0.0
        assert result["duration_ms"] > 0

    @pytest.mark.asyncio
    async def test_execute_task_with_context(self):
        """Given a crew and context data
        When executing a task with context
        Then should pass context to executor
        """
        # Given
        crew = ManageDocumentationCrew()
        mock_executor = AsyncMock()
        mock_response = MagicMock()
        mock_response.content = "Response with context"
        mock_response.metadata = {"cost": 0.02, "duration_ms": 150}
        mock_executor.execute.return_value = mock_response
        crew._executor = mock_executor

        agent = crew.agents[0]
        task = "Analyze with context"
        context = {"previous_results": ["result1", "result2"]}

        # When
        result = await crew._execute_task(agent, task, context)

        # Then
        assert result["result"] == "Response with context"
        call_args = mock_executor.execute.call_args
        assert "context" in call_args[1] or len(call_args[0]) > 2

    @pytest.mark.asyncio
    async def test_execute_task_handles_exception(self):
        """Given an executor that raises an exception
        When executing a task
        Then should handle error gracefully
        """
        # Given
        crew = ManageDocumentationCrew()
        mock_executor = AsyncMock()
        mock_executor.execute.side_effect = Exception("Test error")
        crew._executor = mock_executor

        agent = crew.agents[0]
        task = "Failing task"

        # When/Then
        with pytest.raises(Exception) as exc_info:
            await crew._execute_task(agent, task, {})
        assert "Test error" in str(exc_info.value)

    @pytest.mark.asyncio
    async def test_run_success(self):
        """Given valid input
        When running the crew
        Then should return success result with all data
        """
        # Given
        crew = ManageDocumentationCrew()
        
        # Mock _execute_task to return predictable results
        async def mock_execute_task(agent, task, context):
            return {
                "agent_role": agent.role,
                "task": task,
                "result": f"Completed: {task}",
                "cost": 0.01,
                "duration_ms": 100,
            }
        
        crew._execute_task = mock_execute_task

        # When
        result = await crew.run()

        # Then
        assert isinstance(result, ManageDocumentationCrewResult)
        assert result.success is True
        assert result.files_analyzed >= 0
        assert result.cost > 0
        assert result.duration_ms > 0
        assert len(result.formatted_report) > 0

    @pytest.mark.asyncio
    async def test_run_with_custom_target_files(self):
        """Given custom target files
        When running the crew
        Then should analyze specified files
        """
        # Given
        crew = ManageDocumentationCrew()
        target_files = ["file1.py", "file2.py"]
        
        async def mock_execute_task(agent, task, context):
            return {
                "agent_role": agent.role,
                "task": task,
                "result": "Task completed",
                "cost": 0.01,
                "duration_ms": 100,
            }
        
        crew._execute_task = mock_execute_task

        # When
        result = await crew.run(target_files=target_files)

        # Then
        assert result.success is True
        assert result.files_analyzed == len(target_files)

    @pytest.mark.asyncio
    async def test_run_with_custom_output_dir(self):
        """Given custom output directory
        When running the crew
        Then should use specified output directory
        """
        # Given
        crew = ManageDocumentationCrew()
        output_dir = Path("/tmp/custom_output")
        
        async def mock_execute_task(