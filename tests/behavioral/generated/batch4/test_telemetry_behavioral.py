"""Behavioral tests for telemetry.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest

from empathy_os.models.telemetry import (
    LLMCallRecord,
    TelemetryBackend,
    TelemetryStore,
    WorkflowRunRecord,
    WorkflowStageRecord,
)


# Fixtures


@pytest.fixture
def temp_telemetry_dir():
    """Given a temporary directory for telemetry files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def sample_llm_call_record():
    """Given a sample LLM call record."""
    return LLMCallRecord(
        call_id="call_123",
        timestamp="2025-01-01T12:00:00Z",
        workflow_name="test_workflow",
        step_name="test_step",
        user_id="user_456",
        session_id="session_789",
        task_type="code_generation",
        provider="anthropic",
        tier="capable",
        model_id="claude-3-5-sonnet-20241022",
        input_tokens=100,
        output_tokens=200,
        estimated_cost=0.015,
        actual_cost=0.015,
        latency_ms=1500,
        fallback_used=False,
        fallback_chain=[],
        original_provider=None,
        original_model=None,
        retry_count=0,
        circuit_breaker_state="closed",
        success=True,
        error_type=None,
        error_message=None,
        metadata={"custom_field": "value"},
    )


@pytest.fixture
def sample_workflow_stage_record():
    """Given a sample workflow stage record."""
    return WorkflowStageRecord(
        stage_name="planning",
        tier="capable",
        model_id="claude-3-5-sonnet-20241022",
        input_tokens=50,
        output_tokens=100,
        cost=0.008,
        latency_ms=800,
        success=True,
        skipped=False,
        skip_reason=None,
        error=None,
    )


@pytest.fixture
def sample_workflow_run_record(sample_workflow_stage_record):
    """Given a sample workflow run record."""
    return WorkflowRunRecord(
        run_id="run_123",
        workflow_name="test_workflow",
        timestamp="2025-01-01T12:00:00Z",
        user_id="user_456",
        session_id="session_789",
        stages=[sample_workflow_stage_record],
        total_input_tokens=50,
        total_output_tokens=100,
        total_cost=0.008,
        total_latency_ms=800,
        success=True,
        error=None,
        metadata={"workflow_meta": "data"},
    )


@pytest.fixture
def telemetry_store(temp_telemetry_dir):
    """Given a telemetry store with a temporary directory."""
    return TelemetryStore(storage_dir=temp_telemetry_dir)


# LLMCallRecord Tests


class TestLLMCallRecord:
    """Tests for LLMCallRecord dataclass."""

    def test_to_dict_converts_record_to_dictionary(self, sample_llm_call_record):
        """Given an LLM call record.
        When converting to dictionary.
        Then all fields are present in the dictionary.
        """
        # When
        result = sample_llm_call_record.to_dict()

        # Then
        assert isinstance(result, dict)
        assert result["call_id"] == "call_123"
        assert result["timestamp"] == "2025-01-01T12:00:00Z"
        assert result["workflow_name"] == "test_workflow"
        assert result["step_name"] == "test_step"
        assert result["provider"] == "anthropic"
        assert result["model_id"] == "claude-3-5-sonnet-20241022"
        assert result["input_tokens"] == 100
        assert result["output_tokens"] == 200
        assert result["estimated_cost"] == 0.015
        assert result["success"] is True
        assert result["metadata"]["custom_field"] == "value"

    def test_from_dict_creates_record_from_dictionary(self):
        """Given a dictionary with LLM call data.
        When creating a record from the dictionary.
        Then the record is created with correct values.
        """
        # Given
        data = {
            "call_id": "call_456",
            "timestamp": "2025-01-02T10:00:00Z",
            "workflow_name": "another_workflow",
            "step_name": "another_step",
            "user_id": "user_789",
            "session_id": "session_012",
            "task_type": "summarization",
            "provider": "openai",
            "tier": "simple",
            "model_id": "gpt-4o-mini",
            "input_tokens": 50,
            "output_tokens": 75,
            "estimated_cost": 0.005,
            "actual_cost": 0.005,
            "latency_ms": 500,
            "fallback_used": False,
            "fallback_chain": [],
            "original_provider": None,
            "original_model": None,
            "retry_count": 0,
            "circuit_breaker_state": None,
            "success": True,
            "error_type": None,
            "error_message": None,
            "metadata": {},
        }

        # When
        record = LLMCallRecord.from_dict(data)

        # Then
        assert record.call_id == "call_456"
        assert record.timestamp == "2025-01-02T10:00:00Z"
        assert record.workflow_name == "another_workflow"
        assert record.provider == "openai"
        assert record.input_tokens == 50
        assert record.output_tokens == 75

    def test_default_values_for_optional_fields(self):
        """Given minimal required fields for LLM call record.
        When creating the record.
        Then optional fields have correct default values.
        """
        # Given / When
        record = LLMCallRecord(
            call_id="call_minimal",
            timestamp="2025-01-01T00:00:00Z",
        )

        # Then
        assert record.workflow_name is None
        assert record.step_name is None
        assert record.user_id is None
        assert record.session_id is None
        assert record.task_type == "unknown"
        assert record.provider == "anthropic"
        assert record.tier == "capable"
        assert record.model_id == ""
        assert record.input_tokens == 0
        assert record.output_tokens == 0
        assert record.estimated_cost == 0.0
        assert record.actual_cost is None
        assert record.latency_ms == 0
        assert record.fallback_used is False
        assert record.fallback_chain == []
        assert record.original_provider is None
        assert record.original_model is None
        assert record.retry_count == 0
        assert record.circuit_breaker_state is None
        assert record.success is True
        assert record.error_type is None
        assert record.error_message is None
        assert record.metadata == {}

    def test_fallback_tracking_fields(self):
        """Given an LLM call record with fallback usage.
        When creating the record with fallback information.
        Then fallback fields are properly set.
        """
        # Given / When
        record = LLMCallRecord(
            call_id="call_fallback",
            timestamp="2025-01-01T12:00:00Z",
            fallback_used=True,
            fallback_chain=["openai", "anthropic"],
            original_provider="openai",
            original_model="gpt-4o",
            retry_count=2,
            circuit_breaker_state="half-open",
        )

        # Then
        assert record.fallback_used is True
        assert record.fallback_chain == ["openai", "anthropic"]
        assert record.original_provider == "openai"
        assert record.original_model == "gpt-4o"
        assert record.retry_count == 2
        assert record.circuit_breaker_state == "half-open"

    def test_error_tracking_fields(self):
        """Given an LLM call record with error information.
        When creating the record with error details.
        Then error fields are properly set.
        """
        # Given / When
        record = LLMCallRecord(
            call_id="call_error",
            timestamp="2025-01-01T12:00:00Z",
            success=False,
            error_type="RateLimitError",
            error_message="API rate limit exceeded",
        )

        # Then
        assert record.success is False
        assert record.error_type == "RateLimitError"
        assert record.error_message == "API rate limit exceeded"


# WorkflowStageRecord Tests


class TestWorkflowStageRecord:
    """Tests for WorkflowStageRecord dataclass."""

    def test_create_successful_stage_record(self):
        """Given workflow stage data.
        When creating a successful stage record.
        Then the record has correct values.
        """
        # Given / When
        record = WorkflowStageRecord(
            stage_name="execution",
            tier="simple",
            model_id="gpt-4o-mini",
            input_tokens=30,
            output_tokens=60,
            cost=0.003,
            latency_ms=400,
            success=True,
        )

        # Then
        assert record.stage_name == "execution"
        assert record.tier == "simple"
        assert record.model_id == "gpt-4o-mini"
        assert record.input_tokens == 30
        assert record.output_tokens == 60
        assert record.cost == 0.003
        assert record.latency_ms == 400
        assert record.success is True
        assert record.skipped is False
        assert record.skip_reason is None
        assert record.error is None

    def test_create_skipped_stage_record(self):
        """Given a skipped workflow stage.
        When creating a stage record with skip information.
        Then skip fields are properly set.
        """
        # Given / When
        record = WorkflowStageRecord(
            stage_name="optional_stage",
            tier="capable",
            model_id="claude-3-5-sonnet-20241022",
            skipped=True,
            skip_reason="precondition_not_met",
        )

        # Then
        assert record.stage_name == "optional_stage"
        assert record.skipped is True
        assert record.skip_reason == "precondition_not_met"
        assert record.input_tokens == 0
        assert record.output_tokens == 0
        assert record.cost == 0.0

    def test_create_failed_stage_record(self):
        """Given a failed workflow stage.
        When creating a stage record with error information.
        Then error fields are properly set.
        """
        # Given / When
        record = WorkflowStageRecord(
            stage_name="validation",
            tier="capable",
            model_id="claude-3-5-sonnet-20241022",
            success=False,
            error="Validation failed: invalid input format",
        )

        # Then
        assert record.stage_name == "validation"
        assert record.success is False
        assert record.error == "Validation failed: invalid input format"


# WorkflowRunRecord Tests


class TestWorkflowRunRecord:
    """Tests for WorkflowRunRecord dataclass."""

    def test_to_dict_converts_workflow_record_to_dictionary(
        self, sample_workflow_run_record
    ):
        """Given a workflow run record.
        When converting to dictionary.
        Then all fields including nested stages are present.
        """
        # When
        result = sample_workflow_run_record.to_dict()

        # Then
        assert isinstance(result, dict)
        assert result["run_id"] == "run_123"
        assert result["workflow_name"] == "test_workflow"
        assert result["timestamp"] == "2025-01-01T12:00:00Z"
        assert result["total_input_tokens"] == 50
        assert result["total_output_tokens"] == 100
        assert result["total_cost"] == 0.008
        assert result["success"] is True
        assert len(result["stages"]) == 1
        assert result["stages"][0]["stage_name"] == "planning"

    def test_from_dict_creates_workflow_record_from_dictionary(self):
        """Given a dictionary with workflow run data.
        When creating a workflow record from the dictionary.
        Then the record is created with correct nested stages.
        """
        # Given
        data = {
            "run_id": "run_456",
            "workflow_name": "complex_workflow",
            "timestamp": "2025-01-03T14:00:00Z",
            "user_id": "user_123",
            "session_id": "session_456",
            "stages": [
                {
                    "stage_name": "stage1",
                    "tier": "simple",
                    "model_id": "gpt-4o-mini",
                    "input_tokens": 20,
                    "output_tokens": 40,
                    "cost": 0.002,
                    "latency_ms": 300,
                    "success": True,
                    "skipped": False,
                    "skip_reason": None,
                    "error": None,
                }
            ],
            "total_input_tokens": 20,
            "total_output_tokens": 40,
            "total_cost": 0.002,
            "total_latency_ms": 300,
            "success": True,
            "error": None,
            "metadata": {},
        }

        # When
        record = WorkflowRunRecord.from_dict(data)

        # Then
        assert record.run_id == "run_456"
        assert record.workflow_name == "complex_workflow"
        assert len(record.stages) == 1
        assert isinstance(record.stages[0], WorkflowStageRecord)
        assert record.stages[0].stage_name == "stage1"
        assert record.total_input_tokens == 20
        assert record.total_output_tokens == 40

    def test_workflow_record_with_multiple_stages(self):
        """Given a workflow with multiple stages.
        When creating a workflow run record.
        Then all stages are properly stored.
        """
        # Given
        stage1 = WorkflowStageRecord(
            stage_name="planning",
            tier="capable",
            model_id="claude-3-5-sonnet-20241022",
            input_tokens=50,
            output_tokens=100,
            cost=0.008,
            latency_ms=800,
        )
        stage2 = WorkflowStageRecord(
            stage_name="execution",
            tier="simple",
            model_id="gpt-4o-mini",
            input_tokens=30,
            output_tokens=60,
            cost=0.003,
            latency_ms=400,
        )

        # When
        record = WorkflowRunRecord(
            run_id="run_multi",
            workflow_name="multi_stage_workflow",
            timestamp="2025-01-01T12:00:00Z",
            stages=[stage1, stage2],
            total_input_tokens=80,
            total_output_tokens=160,
            total_cost=0.011,
            total_latency_ms=1200,
            success=True,
        )

        # Then
        assert len(record.stages) == 2
        assert record.stages[0].stage_name == "planning"
        assert record.stages[1].stage_name == "execution"
        assert record.total_input_tokens == 80
        assert record.total_output_tokens == 160

    def test_failed_workflow_run_record(self):
        """Given a failed workflow run.
        When creating the record with error information.
        Then error fields are properly set.
        """
        # Given / When
        record = WorkflowRunRecord(
            run_id="run_failed",
            workflow_name="failed_workflow",
            timestamp="2025-01-01T12:00:00Z",
            stages=[],
            total_input_tokens=0,
            total_output_tokens=0,
            total_cost=0.0,
            total_latency_ms=0,
            success=False,
            error="Workflow execution failed: timeout",
        )

        # Then
        assert record.success is False
        assert record.error == "Workflow execution failed: timeout"


# TelemetryStore Tests


class TestTelemetryStore:
    """Tests for TelemetryStore file-based backend."""

    def test_init_creates_storage_directory(self, temp_telemetry_dir):
        """Given a non-existent directory path.
        When initializing TelemetryStore.
        Then the directory is created.
        """
        # Given
        new_dir = temp_telemetry_dir / "new_telemetry"
        assert not new_dir.exists()

        # When
        store = TelemetryStore(storage_dir=new_dir)

        # Then
        assert new_dir.exists()
        assert new_dir.is_dir()

    def test_record_llm_call_writes_to_jsonl_file(
        self, telemetry_store, sample_llm_call_record, temp_telemetry_dir
    ):
        """Given a telemetry store and an LLM call record.
        When recording the LLM call.
        Then the record is appended to the JSONL file.
        """
        # When
        telemetry_store.record_llm_call(sample_llm_call_record)

        # Then
        calls_file = temp_telemetry_dir / "llm_calls.jsonl"
        assert calls_file.exists()

        with open(calls_file, "r") as f:
            lines = f.readlines()
            assert len(lines) == 1
            data = json.loads(lines[0])
            assert data["call_id"] == "call_123"
            assert data["workflow_name"] == "test_workflow"

    def test_record_workflow_run_writes_to_jsonl_file(
        self, telemetry_store, sample_workflow_run_record, temp_telemetry_dir
    ):
        """Given a telemetry store and a workflow run record.
        When recording the workflow run.
        Then the record is appended to the JSONL file.
        """
        # When
        telemetry_store.record_workflow_run(sample_workflow_run_record)

        # Then
        runs_file = temp_telemetry_dir / "workflow_runs.jsonl"
        assert runs_file.exists()

        with open(runs_file, "r") as f:
            lines = f.readlines()
            assert len(lines) == 1
            data = json.loads(lines[0])
            assert data["run_id"] == "run_123"
            assert data["workflow_name"] == "test_workflow"

    def test_multiple_llm_call_records_appended(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store.
        When recording multiple LLM calls.
        Then all records are appended to the file.
        """
        # Given
        record1 = LLMCallRecord(
            call_id="call_1", timestamp="2025-01-01T12:00:00Z"
        )
        record2 = LLMCallRecord(
            call_id="call_2", timestamp="2025-01-01T12:01:00Z"
        )
        record3 = LLMCallRecord(
            call_id="call_3", timestamp="2025-01-01T12:02:00Z"
        )

        # When
        telemetry_store.record_llm_call(record1)
        telemetry_store.record_llm_call(record2)
        telemetry_store.record_llm_call(record3)

        # Then
        calls_file = temp_telemetry_dir / "llm_calls.jsonl"
        with open(calls_file, "r") as f:
            lines = f.readlines()
            assert len(lines) == 3
            assert json.loads(lines[0])["call_id"] == "call_1"
            assert json.loads(lines[1])["call_id"] == "call_2"
            assert json.loads(lines[2])["call_id"] == "call_3"

    def test_get_llm_calls_returns_empty_list_when_no_file(self, telemetry_store):
        """Given a telemetry store with no recorded calls.
        When getting LLM calls.
        Then an empty list is returned.
        """
        # When
        calls = telemetry_store.get_llm_calls()

        # Then
        assert calls == []

    def test_get_llm_calls_returns_all_records(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store with recorded LLM calls.
        When getting LLM calls.
        Then all records are returned as LLMCallRecord objects.
        """
        # Given
        record1 = LLMCallRecord(
            call_id="call_1", timestamp="2025-01-01T12:00:00Z"
        )
        record2 = LLMCallRecord(
            call_id="call_2", timestamp="2025-01-01T12:01:00Z"
        )
        telemetry_store.record_llm_call(record1)
        telemetry_store.record_llm_call(record2)

        # When
        calls = telemetry_store.get_llm_calls()

        # Then
        assert len(calls) == 2
        assert isinstance(calls[0], LLMCallRecord)
        assert isinstance(calls[1], LLMCallRecord)
        assert calls[0].call_id == "call_1"
        assert calls[1].call_id == "call_2"

    def test_get_llm_calls_with_limit(self, telemetry_store):
        """Given a telemetry store with multiple recorded LLM calls.
        When getting LLM calls with a limit.
        Then only the specified number of records is returned.
        """
        # Given
        for i in range(10):
            record = LLMCallRecord(
                call_id=f"call_{i}", timestamp=f"2025-01-01T12:0{i}:00Z"
            )
            telemetry_store.record_llm_call(record)

        # When
        calls = telemetry_store.get_llm_calls(limit=5)

        # Then
        assert len(calls) == 5

    def test_get_workflow_runs_returns_empty_list_when_no_file(
        self, telemetry_store
    ):
        """Given a telemetry store with no recorded workflow runs.
        When getting workflow runs.
        Then an empty list is returned.
        """
        # When
        runs = telemetry_store.get_workflow_runs()

        # Then
        assert runs == []

    def test_get_workflow_runs_returns_all_records(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store with recorded workflow runs.
        When getting workflow runs.
        Then all records are returned as WorkflowRunRecord objects.
        """
        # Given
        record1 = WorkflowRunRecord(
            run_id="run_1",
            workflow_name="workflow_1",
            timestamp="2025-01-01T12:00:00Z",
            stages=[],
            total_input_tokens=0,
            total_output_tokens=0,
            total_cost=0.0,
            total_latency_ms=0,
            success=True,
        )
        record2 = WorkflowRunRecord(
            run_id="run_2",
            workflow_name="workflow_2",
            timestamp="2025-01-01T12:01:00Z",
            stages=[],
            total_input_tokens=0,
            total_output_tokens=0,
            total_cost=0.0,
            total_latency_ms=0,
            success=True,
        )
        telemetry_store.record_workflow_run(record1)
        telemetry_store.record_workflow_run(record2)

        # When
        runs = telemetry_store.get_workflow_runs()

        # Then
        assert len(runs) == 2
        assert isinstance(runs[0], WorkflowRunRecord)
        assert isinstance(runs[1], WorkflowRunRecord)
        assert runs[0].run_id == "run_1"
        assert runs[1].run_id == "run_2"

    def test_get_workflow_runs_with_limit(self, telemetry_store):
        """Given a telemetry store with multiple recorded workflow runs.
        When getting workflow runs with a limit.
        Then only the specified number of records is returned.
        """
        # Given
        for i in range(10):
            record = WorkflowRunRecord(
                run_id=f"run_{i}",
                workflow_name=f"workflow_{i}",
                timestamp=f"2025-01-01T12:0{i}:00Z",
                stages=[],
                total_input_tokens=0,
                total_output_tokens=0,
                total_cost=0.0,
                total_latency_ms=0,
                success=True,
            )
            telemetry_store.record_workflow_run(record)

        # When
        runs = telemetry_store.get_workflow_runs(limit=3)

        # Then
        assert len(runs) == 3

    def test_get_llm_calls_handles_malformed_json_gracefully(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store with a malformed JSONL file.
        When getting LLM calls.
        Then malformed lines are skipped and valid records are returned.
        """
        # Given
        calls_file = temp_telemetry_dir / "llm_calls.jsonl"
        with open(calls_file, "w") as f:
            f.write('{"call_id": "call_1", "timestamp": "2025-01-01T12:00:00Z"}\n')
            f.write("{invalid json}\n")
            f.write('{"call_id": "call_2", "timestamp": "2025-01-01T12:01:00Z"}\n')

        # When
        calls = telemetry_store.get_llm_calls()

        # Then
        assert len(calls) == 2
        assert calls[0].call_id == "call_1"
        assert calls[1].call_id == "call_2"

    def test_get_workflow_runs_handles_malformed_json_gracefully(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store with a malformed JSONL file.
        When getting workflow runs.
        Then malformed lines are skipped and valid records are returned.
        """
        # Given
        runs_file = temp_telemetry_dir / "workflow_runs.jsonl"
        with open(runs_file, "w") as f:
            f.write(
                '{"run_id": "run_1", "workflow_name": "workflow_1", '
                '"timestamp": "2025-01-01T12:00:00Z", "stages": [], '
                '"total_input_tokens": 0, "total_output_tokens": 0, '
                '"total_cost": 0.0, "total_latency_ms": 0, "success": true}\n'
            )
            f.write("{invalid json}\n")
            f.write(
                '{"run_id": "run_2", "workflow_name": "workflow_2", '
                '"timestamp": "2025-01-01T12:01:00Z", "stages": [], '
                '"total_input_tokens": 0, "total_output_tokens": 0, '
                '"total_cost": 0.0, "total_latency_ms": 0, "success": true}\n'
            )

        # When
        runs = telemetry_store.get_workflow_runs()

        # Then
        assert len(runs) == 2
        assert runs[0].run_id == "run_1"
        assert runs[1].run_id == "run_2"

    def test_record_llm_call_handles_write_errors_gracefully(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store with a read-only directory.
        When recording an LLM call.
        Then the error is handled gracefully without raising exception.
        """
        # Given
        record = LLMCallRecord(
            call_id="call_error", timestamp="2025-01-01T12:00:00Z"
        )

        with patch("builtins.open", side_effect=IOError("Permission denied")):
            # When / Then (should not raise)
            try:
                telemetry_store.record_llm_call(record)
            except IOError:
                pytest.fail("Should handle IOError gracefully")

    def test_record_workflow_run_handles_write_errors_gracefully(
        self, telemetry_store
    ):
        """Given a telemetry store with a read-only directory.
        When recording a workflow run.
        Then the error is handled gracefully without raising exception.
        """
        # Given
        record = WorkflowRunRecord(
            run_id="run_error",
            workflow_name="workflow_error",
            timestamp="2025-01-01T12:00:00Z",
            stages=[],
            total_input_tokens=0,
            total_output_tokens=0,
            total_cost=0.0,
            total_latency_ms=0,
            success=True,
        )

        with patch("builtins.open", side_effect=IOError("Permission denied")):
            # When / Then (should not raise)
            try:
                telemetry_store.record_workflow_run(record)
            except IOError:
                pytest.fail("Should handle IOError gracefully")


# Protocol Tests


class TestTelemetryBackendProtocol:
    """Tests for TelemetryBackend protocol compliance."""

    def test_telemetry_store_implements_backend_protocol(self):
        """Given TelemetryStore class.
        When checking protocol compliance.
        Then it implements TelemetryBackend protocol.
        """
        # Given / When / Then
        assert isinstance(TelemetryStore, type)
        # Protocol checking is done at type-checking time, not runtime

    def test_custom_backend_can_implement_protocol(
        self, sample_llm_call_record, sample_workflow_run_record
    ):
        """Given a custom backend implementation.
        When implementing the TelemetryBackend protocol.
        Then it can be used as a telemetry backend.
        """

        # Given
        class CustomBackend:
            def __init__(self):
                self.llm_calls = []
                self.workflow_runs = []

            def record_llm_call(self, record: LLMCallRecord) -> None:
                self.llm_calls.append(record)

            def record_workflow_run(self, record: WorkflowRunRecord) -> None:
                self.workflow_runs.append(record)

            def get_llm_calls(self, limit: int | None = None) -> list[LLMCallRecord]:
                return self.llm_calls[:limit] if limit else self.llm_calls

            def get_workflow_runs(
                self, limit: int | None = None
            ) -> list[WorkflowRunRecord]:
                return self.workflow_runs[:limit] if limit else self.workflow_runs

        # When
        backend = CustomBackend()
        backend.record_llm_call(sample_llm_call_record)
        backend.record_workflow_run(sample_workflow_run_record)

        # Then
        assert len(backend.get_llm_calls()) == 1
        assert len(backend.get_workflow_runs()) == 1
        assert backend.get_llm_calls()[0].call_id == "call_123"
        assert backend.get_workflow_runs()[0].run_id == "run_123"


# Edge Cases and Integration Tests


class TestEdgeCases:
    """Tests for edge cases and integration scenarios."""

    def test_empty_metadata_serialization(self):
        """Given an LLM call record with empty metadata.
        When converting to dict and back.
        Then the record is preserved correctly.
        """
        # Given
        record = LLMCallRecord(
            call_id="call_empty_meta",
            timestamp="2025-01-01T12:00:00Z",
            metadata={},
        )

        # When
        dict_data = record.to_dict()
        restored = LLMCallRecord.from_dict(dict_data)

        # Then
        assert restored.metadata == {}

    def test_nested_metadata_serialization(self):
        """Given an LLM call record with nested metadata.
        When converting to dict and back.
        Then nested structures are preserved.
        """
        # Given
        record = LLMCallRecord(
            call_id="call_nested_meta",
            timestamp="2025-01-01T12:00:00Z",
            metadata={
                "nested": {"key1": "value1", "key2": [1, 2, 3]},
                "list": [{"a": 1}, {"b": 2}],
            },
        )

        # When
        dict_data = record.to_dict()
        restored = LLMCallRecord.from_dict(dict_data)

        # Then
        assert restored.metadata["nested"]["key1"] == "value1"
        assert restored.metadata["nested"]["key2"] == [1, 2, 3]
        assert restored.metadata["list"][0]["a"] == 1

    def test_workflow_with_all_stages_skipped(self):
        """Given a workflow where all stages are skipped.
        When creating the workflow record.
        Then the record correctly reflects all skipped stages.
        """
        # Given
        stage1 = WorkflowStageRecord(
            stage_name="stage1",
            tier="capable",
            model_id="claude-3-5-sonnet-20241022",
            skipped=True,
            skip_reason="condition_not_met",
        )
        stage2 = WorkflowStageRecord(
            stage_name="stage2",
            tier="simple",
            model_id="gpt-4o-mini",
            skipped=True,
            skip_reason="previous_stage_skipped",
        )

        # When
        record = WorkflowRunRecord(
            run_id="run_all_skipped",
            workflow_name="conditional_workflow",
            timestamp="2025-01-01T12:00:00Z",
            stages=[stage1, stage2],
            total_input_tokens=0,
            total_output_tokens=0,
            total_cost=0.0,
            total_latency_ms=0,
            success=True,
        )

        # Then
        assert all(stage.skipped for stage in record.stages)
        assert record.total_input_tokens == 0
        assert record.total_cost == 0.0

    def test_concurrent_writes_to_telemetry_store(
        self, telemetry_store, temp_telemetry_dir
    ):
        """Given a telemetry store.
        When recording calls concurrently.
        Then all records are persisted (basic test without actual threading).
        """
        # Given
        records = [
            LLMCallRecord(call_id=f"call_{i}", timestamp=f"2025-01-01T12:0{i}:00Z")
            for i in range(5)
        ]

        # When
        for record in records:
            telemetry_store.record_llm_call(record)

        # Then
        calls = telemetry_store.get_llm_calls()
        assert len(calls) == 5

    def test_large_token_counts(self):
        """Given an LLM call with very large token counts.
        When creating the record.
        Then the values are stored correctly.
        """
        # Given / When
        record = LLMCallRecord(
            call_id="call_large_tokens",
            timestamp="2025-01-01T12:00:00Z",
            input_tokens=1000000,
            output_tokens=500000,
            estimated_cost=150.0,
        )

        # Then
        assert record.input_tokens == 1000000
        assert record.output_tokens == 500000
        assert record.estimated_cost == 150.0

    def test_zero_cost_and_tokens(self):
        """Given an LLM call with zero cost and tokens (e.g., cached response).
        When creating the record.
        Then zero values are preserved.
        """
        # Given / When
        record = LLMCallRecord(
            call_id="call_cached",
            timestamp="2025-01-01T12:00:00Z",
            input_tokens=0,
            output_tokens=0,
            estimated_cost=0.0,
            latency_ms=50,
        )

        # Then
        assert record.input_tokens == 0
        assert record.output_tokens == 0
        assert record.estimated_cost == 0.0
        assert record.latency_ms == 50

    def test_workflow_stage_record_serialization_roundtrip(self):
        """Given a workflow stage record.
        When converting through dict and back.
        Then all data is preserved.
        """
        # Given
        original = WorkflowStageRecord(
            stage_name="test_stage",
            tier="capable",
            model_id="claude-3-5-sonnet-20241022",
            input_tokens=100,
            output_tokens=200,
            cost=0.015,
            latency_ms=1500,
            success=True,
            skipped=False,
            skip_reason=None,
            error=None,
        )

        # When
        as_dict = {
            "stage_name": original.stage_name,
            "tier": original.tier,
            "model_id": original.model_id,
            "input_tokens": original.input_tokens,
            "output_tokens": original.output_tokens,
            "cost": original.cost,
            "latency_ms": original.latency_ms,
            "success": original.success,
            "skipped": original.skipped,
            "skip_reason": original.skip_reason,
            "error": original.error,
        }
        restored = WorkflowStageRecord(**as_dict)

        # Then
        assert restored.stage_name == original.stage_name
        assert restored.tier == original.tier
        assert restored.model_id == original.model_id
        assert restored.input_tokens == original.input_tokens
        assert restored.output_tokens == original.output_tokens
        assert restored.cost == original.cost
        assert restored.latency_ms == original.latency_ms

    def test_special_characters_in_strings(self):
        """Given records with special characters in string fields.
        When serializing and deserializing.
        Then special characters are preserved.
        """
        # Given
        record = LLMCallRecord(
            call_id='call_"special"',
            timestamp="2025-01-01T12:00:00Z",
            workflow_name="workflow\nwith\nnewlines",
            error_message='Error: "Something went wrong" (code: 500)',
            metadata={"key": "value with 'quotes' and \"double quotes\""},
        )

        # When
        dict_data = record.to_dict()
        json_str = json.dumps(dict_data)
        restored_dict = json.loads(json_str)
        restored = LLMCallRecord.from_dict(restored_dict)

        # Then
        assert restored.call_id == 'call_"special"'
        assert restored.workflow_name == "workflow\nwith\nnewlines"
        assert 'Error: "Something went wrong"' in restored.error_message

    def test_none_values_serialization(self):
        """Given records with None values.
        When serializing and deserializing.
        Then None values are preserved.
        """
        # Given
        record = LLMCallRecord(
            call_id="call_none_values",
            timestamp="2025-01-01T12:00:00Z",
            workflow_name=None,
            step_name=None,
            user_id=None,
            actual_cost=None,
            error_type=None,
        )

        # When
        dict_data = record.to_dict()
        restored = LLMCallRecord.from_dict(dict_data)

        # Then
        assert restored.workflow_name is None
        assert restored.step_name is None
        assert restored.user_id is None
        assert restored.actual_cost is None
        assert restored.error_type is None