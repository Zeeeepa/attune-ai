"""Behavioral tests for cli_meta_workflows.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import MagicMock, Mock, patch, call

import pytest
from typer.testing import CliRunner

from empathy_os.meta_workflows.cli_meta_workflows import (
    meta_workflow_app,
    list_templates,
    console,
)


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def cli_runner():
    """Provide a CLI runner for testing Typer commands."""
    return CliRunner()


@pytest.fixture
def mock_template_registry():
    """Mock TemplateRegistry for testing."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.TemplateRegistry") as mock:
        registry_instance = Mock()
        mock.return_value = registry_instance
        yield registry_instance


@pytest.fixture
def mock_meta_workflow():
    """Mock MetaWorkflow for testing."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.MetaWorkflow") as mock:
        workflow_instance = Mock()
        mock.return_value = workflow_instance
        yield workflow_instance


@pytest.fixture
def mock_pattern_learner():
    """Mock PatternLearner for testing."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.PatternLearner") as mock:
        learner_instance = Mock()
        mock.return_value = learner_instance
        yield learner_instance


@pytest.fixture
def mock_intent_detector():
    """Mock IntentDetector for testing."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.IntentDetector") as mock:
        detector_instance = Mock()
        mock.return_value = detector_instance
        yield detector_instance


@pytest.fixture
def mock_console():
    """Mock Rich Console for testing output."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.console") as mock:
        yield mock


@pytest.fixture
def mock_list_execution_results():
    """Mock list_execution_results function."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.list_execution_results") as mock:
        yield mock


@pytest.fixture
def mock_load_execution_result():
    """Mock load_execution_result function."""
    with patch("empathy_os.meta_workflows.cli_meta_workflows.load_execution_result") as mock:
        yield mock


@pytest.fixture
def sample_template():
    """Provide sample template data."""
    return {
        "template_id": "test_template_123",
        "name": "Test Template",
        "description": "A test template for unit tests",
        "version": "1.0.0",
        "cost_range": {"min": 0.05, "max": 0.15},
        "questions": ["Question 1?", "Question 2?"],
        "agent_generation_rules": ["Rule 1", "Rule 2"],
        "created_at": "2026-01-17T10:00:00",
    }


@pytest.fixture
def sample_execution_result():
    """Provide sample execution result data."""
    return {
        "run_id": "run_456",
        "template_id": "test_template_123",
        "template_name": "Test Template",
        "start_time": "2026-01-17T10:00:00",
        "end_time": "2026-01-17T10:05:00",
        "status": "completed",
        "total_cost": 0.08,
        "output": "Test output",
        "metadata": {"key": "value"},
    }


# =============================================================================
# Tests for list_templates command
# =============================================================================


def test_list_templates_with_no_templates(cli_runner, mock_template_registry, mock_console):
    """
    GIVEN a template registry with no templates
    WHEN list-templates command is executed
    THEN it should display a message indicating no templates found
    """
    # Given
    mock_template_registry.list_templates.return_value = []

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # Then
    assert result.exit_code == 0
    mock_console.print.assert_any_call("[yellow]No templates found.[/yellow]")


def test_list_templates_with_builtin_templates(cli_runner, mock_template_registry, mock_console, sample_template):
    """
    GIVEN a template registry with builtin templates
    WHEN list-templates command is executed
    THEN it should display all templates with builtin marker
    """
    # Given
    mock_template_registry.list_templates.return_value = ["builtin_template_1", "builtin_template_2"]
    mock_template_registry.is_builtin.return_value = True
    mock_template_registry.load_template.return_value = sample_template

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # Then
    assert result.exit_code == 0
    assert mock_template_registry.list_templates.called


def test_list_templates_with_mixed_templates(cli_runner, mock_template_registry, mock_console, sample_template):
    """
    GIVEN a template registry with both builtin and user templates
    WHEN list-templates command is executed
    THEN it should display templates with correct counts
    """
    # Given
    mock_template_registry.list_templates.return_value = ["builtin_1", "user_1", "user_2"]
    mock_template_registry.is_builtin.side_effect = lambda x: x.startswith("builtin")
    mock_template_registry.load_template.return_value = sample_template

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # Then
    assert result.exit_code == 0
    assert mock_template_registry.list_templates.called


def test_list_templates_with_custom_storage_dir(cli_runner, mock_template_registry):
    """
    GIVEN a custom storage directory path
    WHEN list-templates command is executed with --storage-dir option
    THEN it should use the custom directory
    """
    # Given
    custom_dir = "/custom/templates"
    mock_template_registry.list_templates.return_value = []

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-templates", "--storage-dir", custom_dir])

    # Then
    assert result.exit_code == 0


def test_list_templates_handles_exception(cli_runner, mock_template_registry, mock_console):
    """
    GIVEN a template registry that raises an exception
    WHEN list-templates command is executed
    THEN it should handle the error gracefully
    """
    # Given
    mock_template_registry.list_templates.side_effect = Exception("Registry error")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # Then
    assert result.exit_code != 0 or mock_console.print.called


# =============================================================================
# Tests for run command
# =============================================================================


def test_run_workflow_with_valid_template_id(cli_runner, mock_meta_workflow, mock_template_registry, sample_template):
    """
    GIVEN a valid template ID
    WHEN run command is executed
    THEN it should execute the workflow successfully
    """
    # Given
    template_id = "test_template_123"
    mock_template_registry.load_template.return_value = sample_template
    mock_meta_workflow.generate_workflow.return_value = {"result": "success"}
    mock_meta_workflow.execute_workflow.return_value = {"output": "completed"}

    # When
    result = cli_runner.invoke(meta_workflow_app, ["run", template_id])

    # Then
    assert result.exit_code == 0


def test_run_workflow_with_nonexistent_template(cli_runner, mock_template_registry):
    """
    GIVEN a nonexistent template ID
    WHEN run command is executed
    THEN it should display an error message
    """
    # Given
    template_id = "nonexistent_template"
    mock_template_registry.load_template.side_effect = FileNotFoundError("Template not found")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["run", template_id])

    # Then
    assert result.exit_code != 0


def test_run_workflow_with_custom_input(cli_runner, mock_meta_workflow, mock_template_registry, sample_template):
    """
    GIVEN a template ID and custom input
    WHEN run command is executed with --input option
    THEN it should use the custom input
    """
    # Given
    template_id = "test_template_123"
    custom_input = "Custom user input"
    mock_template_registry.load_template.return_value = sample_template
    mock_meta_workflow.generate_workflow.return_value = {"result": "success"}
    mock_meta_workflow.execute_workflow.return_value = {"output": "completed"}

    # When
    result = cli_runner.invoke(meta_workflow_app, ["run", template_id, "--input", custom_input])

    # Then
    assert result.exit_code == 0


def test_run_workflow_with_skip_confirmation(cli_runner, mock_meta_workflow, mock_template_registry, sample_template):
    """
    GIVEN a template ID with skip confirmation flag
    WHEN run command is executed
    THEN it should skip user confirmation
    """
    # Given
    template_id = "test_template_123"
    mock_template_registry.load_template.return_value = sample_template
    mock_meta_workflow.generate_workflow.return_value = {"result": "success"}
    mock_meta_workflow.execute_workflow.return_value = {"output": "completed"}

    # When
    result = cli_runner.invoke(meta_workflow_app, ["run", template_id, "--yes"])

    # Then
    assert result.exit_code == 0


def test_run_workflow_execution_failure(cli_runner, mock_meta_workflow, mock_template_registry, sample_template):
    """
    GIVEN a workflow that fails during execution
    WHEN run command is executed
    THEN it should handle the failure gracefully
    """
    # Given
    template_id = "test_template_123"
    mock_template_registry.load_template.return_value = sample_template
    mock_meta_workflow.generate_workflow.return_value = {"result": "success"}
    mock_meta_workflow.execute_workflow.side_effect = RuntimeError("Execution failed")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["run", template_id, "--yes"])

    # Then
    assert result.exit_code != 0


# =============================================================================
# Tests for analytics command
# =============================================================================


def test_analytics_all_templates(cli_runner, mock_pattern_learner, mock_console):
    """
    GIVEN pattern learner with analytics data
    WHEN analytics command is executed without template ID
    THEN it should display analytics for all templates
    """
    # Given
    mock_pattern_learner.get_analytics.return_value = {
        "total_runs": 10,
        "avg_cost": 0.10,
        "success_rate": 0.95,
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["analytics"])

    # Then
    assert result.exit_code == 0


def test_analytics_specific_template(cli_runner, mock_pattern_learner, mock_console):
    """
    GIVEN a specific template ID
    WHEN analytics command is executed with template ID
    THEN it should display analytics for that template only
    """
    # Given
    template_id = "test_template_123"
    mock_pattern_learner.get_template_analytics.return_value = {
        "template_id": template_id,
        "runs": 5,
        "avg_cost": 0.08,
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["analytics", template_id])

    # Then
    assert result.exit_code == 0


def test_analytics_with_time_range(cli_runner, mock_pattern_learner):
    """
    GIVEN a time range filter
    WHEN analytics command is executed with --days option
    THEN it should filter analytics by time range
    """
    # Given
    days = 7
    mock_pattern_learner.get_analytics.return_value = {
        "total_runs": 3,
        "avg_cost": 0.09,
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["analytics", "--days", str(days)])

    # Then
    assert result.exit_code == 0


def test_analytics_no_data(cli_runner, mock_pattern_learner, mock_console):
    """
    GIVEN pattern learner with no analytics data
    WHEN analytics command is executed
    THEN it should display a message indicating no data
    """
    # Given
    mock_pattern_learner.get_analytics.return_value = {}

    # When
    result = cli_runner.invoke(meta_workflow_app, ["analytics"])

    # Then
    assert result.exit_code == 0


# =============================================================================
# Tests for list-runs command
# =============================================================================


def test_list_runs_with_results(cli_runner, mock_list_execution_results, mock_console, sample_execution_result):
    """
    GIVEN execution results exist
    WHEN list-runs command is executed
    THEN it should display all execution runs
    """
    # Given
    mock_list_execution_results.return_value = [sample_execution_result]

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-runs"])

    # Then
    assert result.exit_code == 0
    mock_list_execution_results.assert_called_once()


def test_list_runs_with_no_results(cli_runner, mock_list_execution_results, mock_console):
    """
    GIVEN no execution results exist
    WHEN list-runs command is executed
    THEN it should display a message indicating no runs found
    """
    # Given
    mock_list_execution_results.return_value = []

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-runs"])

    # Then
    assert result.exit_code == 0


def test_list_runs_with_template_filter(cli_runner, mock_list_execution_results, sample_execution_result):
    """
    GIVEN a template ID filter
    WHEN list-runs command is executed with --template option
    THEN it should filter runs by template
    """
    # Given
    template_id = "test_template_123"
    filtered_result = sample_execution_result.copy()
    filtered_result["template_id"] = template_id
    mock_list_execution_results.return_value = [filtered_result]

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-runs", "--template", template_id])

    # Then
    assert result.exit_code == 0


def test_list_runs_with_limit(cli_runner, mock_list_execution_results, sample_execution_result):
    """
    GIVEN a limit parameter
    WHEN list-runs command is executed with --limit option
    THEN it should limit the number of results
    """
    # Given
    limit = 5
    mock_list_execution_results.return_value = [sample_execution_result] * 3

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-runs", "--limit", str(limit)])

    # Then
    assert result.exit_code == 0


def test_list_runs_handles_exception(cli_runner, mock_list_execution_results, mock_console):
    """
    GIVEN list_execution_results raises an exception
    WHEN list-runs command is executed
    THEN it should handle the error gracefully
    """
    # Given
    mock_list_execution_results.side_effect = Exception("Storage error")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["list-runs"])

    # Then
    assert result.exit_code != 0 or mock_console.print.called


# =============================================================================
# Tests for show command
# =============================================================================


def test_show_run_with_valid_run_id(cli_runner, mock_load_execution_result, mock_console, sample_execution_result):
    """
    GIVEN a valid run ID
    WHEN show command is executed
    THEN it should display detailed run information
    """
    # Given
    run_id = "run_456"
    mock_load_execution_result.return_value = sample_execution_result

    # When
    result = cli_runner.invoke(meta_workflow_app, ["show", run_id])

    # Then
    assert result.exit_code == 0
    mock_load_execution_result.assert_called_once_with(run_id)


def test_show_run_with_nonexistent_run_id(cli_runner, mock_load_execution_result, mock_console):
    """
    GIVEN a nonexistent run ID
    WHEN show command is executed
    THEN it should display an error message
    """
    # Given
    run_id = "nonexistent_run"
    mock_load_execution_result.return_value = None

    # When
    result = cli_runner.invoke(meta_workflow_app, ["show", run_id])

    # Then
    assert result.exit_code == 0


def test_show_run_with_detailed_output(cli_runner, mock_load_execution_result, sample_execution_result):
    """
    GIVEN a run with detailed metadata
    WHEN show command is executed with --verbose option
    THEN it should display all details including metadata
    """
    # Given
    run_id = "run_456"
    detailed_result = sample_execution_result.copy()
    detailed_result["metadata"] = {
        "agents": ["Agent 1", "Agent 2"],
        "tasks": ["Task 1", "Task 2"],
    }
    mock_load_execution_result.return_value = detailed_result

    # When
    result = cli_runner.invoke(meta_workflow_app, ["show", run_id, "--verbose"])

    # Then
    assert result.exit_code == 0


def test_show_run_handles_exception(cli_runner, mock_load_execution_result, mock_console):
    """
    GIVEN load_execution_result raises an exception
    WHEN show command is executed
    THEN it should handle the error gracefully
    """
    # Given
    run_id = "run_456"
    mock_load_execution_result.side_effect = Exception("Load error")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["show", run_id])

    # Then
    assert result.exit_code != 0 or mock_console.print.called


# =============================================================================
# Tests for detect-intent command
# =============================================================================


def test_detect_intent_with_valid_input(cli_runner, mock_intent_detector, mock_console):
    """
    GIVEN a user input string
    WHEN detect-intent command is executed
    THEN it should detect and display the intent
    """
    # Given
    user_input = "Create a research workflow"
    mock_intent_detector.detect_intent.return_value = {
        "intent": "create_workflow",
        "confidence": 0.95,
        "suggested_template": "research_template",
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["detect-intent", user_input])

    # Then
    assert result.exit_code == 0


def test_detect_intent_with_low_confidence(cli_runner, mock_intent_detector, mock_console):
    """
    GIVEN a user input with low confidence detection
    WHEN detect-intent command is executed
    THEN it should display a warning about low confidence
    """
    # Given
    user_input = "Do something vague"
    mock_intent_detector.detect_intent.return_value = {
        "intent": "unknown",
        "confidence": 0.30,
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["detect-intent", user_input])

    # Then
    assert result.exit_code == 0


def test_detect_intent_with_suggested_templates(cli_runner, mock_intent_detector, mock_console):
    """
    GIVEN intent detection with suggested templates
    WHEN detect-intent command is executed
    THEN it should display suggested templates
    """
    # Given
    user_input = "Research task"
    mock_intent_detector.detect_intent.return_value = {
        "intent": "research",
        "confidence": 0.88,
        "suggested_templates": ["research_template_1", "research_template_2"],
    }

    # When
    result = cli_runner.invoke(meta_workflow_app, ["detect-intent", user_input])

    # Then
    assert result.exit_code == 0


def test_detect_intent_handles_exception(cli_runner, mock_intent_detector, mock_console):
    """
    GIVEN intent detector raises an exception
    WHEN detect-intent command is executed
    THEN it should handle the error gracefully
    """
    # Given
    user_input = "Test input"
    mock_intent_detector.detect_intent.side_effect = Exception("Detection error")

    # When
    result = cli_runner.invoke(meta_workflow_app, ["detect-intent", user_input])

    # Then
    assert result.exit_code != 0 or mock_console.print.called


# =============================================================================
# Tests for edge cases and error handling
# =============================================================================


def test_cli_app_without_arguments(cli_runner):
    """
    GIVEN the CLI app
    WHEN invoked without any arguments
    THEN it should display help message
    """
    # When
    result = cli_runner.invoke(meta_workflow_app)

    # Then
    assert result.exit_code == 0
    assert "meta-workflow" in result.stdout.lower() or "help" in result.stdout.lower()


def test_cli_app_with_help_flag(cli_runner):
    """
    GIVEN the CLI app
    WHEN invoked with --help flag
    THEN it should display help information
    """
    # When
    result = cli_runner.invoke(meta_workflow_app, ["--help"])

    # Then
    assert result.exit_code == 0
    assert "help" in result.stdout.lower() or "usage" in result.stdout.lower()


def test_invalid_command(cli_runner):
    """
    GIVEN an invalid command name
    WHEN the command is executed
    THEN it should display an error message
    """
    # When
    result = cli_runner.invoke(meta_workflow_app, ["invalid-command"])

    # Then
    assert result.exit_code != 0


def test_validate_file_path_integration(cli_runner, tmp_path):
    """
    GIVEN a file path validation requirement
    WHEN commands require file paths
    THEN they should validate paths correctly
    """
    # Given
    valid_path = tmp_path / "test.json"
    valid_path.touch()

    # When - this would be used internally by commands
    # Just verify the import is available
    from empathy_os.config import _validate_file_path

    # Then
    assert callable(_validate_file_path)


def test_console_output_methods(mock_console):
    """
    GIVEN the console instance
    WHEN various output methods are called
    THEN they should work correctly
    """
    # Given/When
    mock_console.print("[bold]Test[/bold]")
    mock_console.print("[yellow]Warning[/yellow]")

    # Then
    assert mock_console.print.call_count == 2


def test_datetime_handling_in_commands():
    """
    GIVEN datetime objects in command processing
    WHEN formatting dates for display
    THEN they should handle various datetime formats
    """
    # Given
    now = datetime.now()
    past = now - timedelta(days=7)

    # When/Then - verify datetime can be used
    assert isinstance(now, datetime)
    assert isinstance(past, datetime)
    assert past < now


def test_path_handling_in_commands(tmp_path):
    """
    GIVEN Path objects in command processing
    WHEN handling file paths
    THEN they should work with pathlib.Path
    """
    # Given
    test_path = tmp_path / "test_dir"

    # When
    test_path.mkdir(exist_ok=True)

    # Then
    assert test_path.exists()
    assert isinstance(test_path, Path)


# =============================================================================
# Integration-style tests
# =============================================================================


def test_full_workflow_lifecycle(
    cli_runner,
    mock_template_registry,
    mock_meta_workflow,
    mock_list_execution_results,
    mock_load_execution_result,
    sample_template,
    sample_execution_result,
):
    """
    GIVEN a complete workflow lifecycle
    WHEN listing templates, running a workflow, and viewing results
    THEN all commands should work together correctly
    """
    # Given
    template_id = "test_template_123"
    run_id = "run_456"

    mock_template_registry.list_templates.return_value = [template_id]
    mock_template_registry.load_template.return_value = sample_template
    mock_template_registry.is_builtin.return_value = False
    mock_meta_workflow.generate_workflow.return_value = {"result": "success"}
    mock_meta_workflow.execute_workflow.return_value = sample_execution_result
    mock_list_execution_results.return_value = [sample_execution_result]
    mock_load_execution_result.return_value = sample_execution_result

    # When - list templates
    result1 = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # When - run workflow
    result2 = cli_runner.invoke(meta_workflow_app, ["run", template_id, "--yes"])

    # When - list runs
    result3 = cli_runner.invoke(meta_workflow_app, ["list-runs"])

    # When - show specific run
    result4 = cli_runner.invoke(meta_workflow_app, ["show", run_id])

    # Then
    assert result1.exit_code == 0
    assert result2.exit_code == 0
    assert result3.exit_code == 0
    assert result4.exit_code == 0


def test_error_recovery_sequence(cli_runner, mock_template_registry, mock_meta_workflow):
    """
    GIVEN a sequence of operations where some fail
    WHEN executing multiple commands
    THEN the system should recover gracefully from errors
    """
    # Given
    mock_template_registry.list_templates.side_effect = [
        Exception("First error"),
        ["template_1"],
    ]

    # When - first attempt fails
    result1 = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # When - second attempt succeeds
    result2 = cli_runner.invoke(meta_workflow_app, ["list-templates"])

    # Then - system recovers
    assert result1.exit_code != 0 or result2.exit_code == 0