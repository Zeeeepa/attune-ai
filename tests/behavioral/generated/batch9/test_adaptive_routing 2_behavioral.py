"""Behavioral tests for adaptive_routing 2.

Generated by enhanced autonomous test generation system.

Copyright 2026 Smart-AI-Memory
Licensed under Apache 2.0
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any
from unittest.mock import MagicMock, Mock, patch

import pytest

from empathy_os.models.adaptive_routing_2 import (
    AdaptiveModelRouter,
    ModelPerformance,
    _get_registry,
)


# Test Fixtures


@pytest.fixture
def mock_registry():
    """Create a mock ModelRegistry for testing."""
    registry = MagicMock()
    registry.get_model.return_value = Mock(
        model_id="claude-3-5-haiku-20241022",
        tier="CHEAP",
        cost_per_1k_tokens=0.001,
    )
    registry.get_models_by_tier.return_value = [
        Mock(
            model_id="claude-3-5-haiku-20241022",
            tier="CHEAP",
            cost_per_1k_tokens=0.001,
        ),
        Mock(
            model_id="claude-3-5-sonnet-20241022",
            tier="CAPABLE",
            cost_per_1k_tokens=0.003,
        ),
    ]
    return registry


@pytest.fixture
def mock_usage_tracker():
    """Create a mock UsageTracker for testing."""
    tracker = MagicMock()
    tracker.get_telemetry_for_workflow.return_value = [
        {
            "model_id": "claude-3-5-haiku-20241022",
            "workflow": "code-review",
            "stage": "analysis",
            "success": True,
            "latency_ms": 150.0,
            "cost": 0.002,
            "timestamp": "2025-01-01T00:00:00Z",
        },
        {
            "model_id": "claude-3-5-haiku-20241022",
            "workflow": "code-review",
            "stage": "analysis",
            "success": True,
            "latency_ms": 160.0,
            "cost": 0.0021,
            "timestamp": "2025-01-01T00:01:00Z",
        },
        {
            "model_id": "claude-3-5-sonnet-20241022",
            "workflow": "code-review",
            "stage": "analysis",
            "success": True,
            "latency_ms": 200.0,
            "cost": 0.005,
            "timestamp": "2025-01-01T00:02:00Z",
        },
    ]
    return tracker


@pytest.fixture
def router(mock_usage_tracker):
    """Create an AdaptiveModelRouter instance for testing."""
    return AdaptiveModelRouter(mock_usage_tracker)


# ModelPerformance Tests


class TestModelPerformance:
    """Test suite for ModelPerformance dataclass."""

    def test_given_high_success_low_cost_when_calculating_quality_score_then_returns_high_score(
        self,
    ):
        """
        Given a model with high success rate and low cost
        When calculating quality score
        Then returns a high quality score
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="CHEAP",
            success_rate=0.95,
            avg_latency_ms=100.0,
            avg_cost=0.001,
            sample_size=100,
        )

        # When
        score = perf.quality_score

        # Then
        assert score > 90  # 95 - 0.01 = ~94.99
        assert isinstance(score, float)

    def test_given_low_success_high_cost_when_calculating_quality_score_then_returns_low_score(
        self,
    ):
        """
        Given a model with low success rate and high cost
        When calculating quality score
        Then returns a low quality score
        """
        # Given
        perf = ModelPerformance(
            model_id="expensive-model",
            tier="PREMIUM",
            success_rate=0.50,
            avg_latency_ms=500.0,
            avg_cost=0.10,
            sample_size=50,
        )

        # When
        score = perf.quality_score

        # Then
        assert score < 51  # 50 - 1.0 = 49.0
        assert score == pytest.approx(49.0)

    def test_given_perfect_success_zero_cost_when_calculating_quality_score_then_returns_100(
        self,
    ):
        """
        Given a model with perfect success rate and zero cost
        When calculating quality score
        Then returns 100.0
        """
        # Given
        perf = ModelPerformance(
            model_id="perfect-model",
            tier="CHEAP",
            success_rate=1.0,
            avg_latency_ms=50.0,
            avg_cost=0.0,
            sample_size=200,
        )

        # When
        score = perf.quality_score

        # Then
        assert score == 100.0

    def test_given_model_performance_when_accessing_attributes_then_returns_correct_values(
        self,
    ):
        """
        Given a ModelPerformance instance
        When accessing its attributes
        Then returns the correct values
        """
        # Given
        perf = ModelPerformance(
            model_id="test-model",
            tier="CAPABLE",
            success_rate=0.85,
            avg_latency_ms=200.0,
            avg_cost=0.005,
            sample_size=75,
            recent_failures=3,
        )

        # When/Then
        assert perf.model_id == "test-model"
        assert perf.tier == "CAPABLE"
        assert perf.success_rate == 0.85
        assert perf.avg_latency_ms == 200.0
        assert perf.avg_cost == 0.005
        assert perf.sample_size == 75
        assert perf.recent_failures == 3


# Registry Lazy Loading Tests


class TestRegistryLoading:
    """Test suite for lazy registry loading."""

    def test_given_no_registry_loaded_when_getting_registry_then_loads_registry(self):
        """
        Given no registry has been loaded
        When calling _get_registry()
        Then loads and returns the registry
        """
        # Given
        global _model_registry
        import empathy_os.models.adaptive_routing_2 as routing_module

        original_registry = routing_module._model_registry
        routing_module._model_registry = None

        # When
        with patch("empathy_os.models.adaptive_routing_2.MODEL_REGISTRY") as mock_reg:
            registry = _get_registry()

            # Then
            assert registry is mock_reg
            assert routing_module._model_registry is mock_reg

        # Cleanup
        routing_module._model_registry = original_registry

    def test_given_registry_already_loaded_when_getting_registry_then_returns_cached_instance(
        self,
    ):
        """
        Given registry has already been loaded
        When calling _get_registry() again
        Then returns cached instance
        """
        # Given
        import empathy_os.models.adaptive_routing_2 as routing_module

        mock_reg = MagicMock()
        routing_module._model_registry = mock_reg

        # When
        registry = _get_registry()

        # Then
        assert registry is mock_reg


# AdaptiveModelRouter Tests


class TestAdaptiveModelRouterInitialization:
    """Test suite for AdaptiveModelRouter initialization."""

    def test_given_usage_tracker_when_initializing_router_then_stores_tracker(
        self, mock_usage_tracker
    ):
        """
        Given a UsageTracker instance
        When initializing AdaptiveModelRouter
        Then stores the tracker
        """
        # Given/When
        router = AdaptiveModelRouter(mock_usage_tracker)

        # Then
        assert router.usage_tracker is mock_usage_tracker

    def test_given_no_tracker_when_initializing_router_then_raises_error(self):
        """
        Given no usage tracker
        When initializing AdaptiveModelRouter
        Then raises TypeError
        """
        # Given/When/Then
        with pytest.raises(TypeError):
            AdaptiveModelRouter()


class TestAdaptiveModelRouterGetBestModel:
    """Test suite for get_best_model method."""

    def test_given_workflow_and_stage_when_getting_best_model_then_returns_model_with_best_quality_score(
        self, router, mock_usage_tracker
    ):
        """
        Given a workflow and stage with telemetry data
        When getting best model
        Then returns the model with the highest quality score
        """
        # Given
        mock_usage_tracker.get_telemetry_for_workflow.return_value = [
            {
                "model_id": "model-a",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 100.0,
                "cost": 0.001,
            },
            {
                "model_id": "model-a",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 110.0,
                "cost": 0.001,
            },
            {
                "model_id": "model-b",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 200.0,
                "cost": 0.005,
            },
        ]

        # When
        with patch.object(router, "_analyze_performance") as mock_analyze:
            mock_analyze.return_value = [
                ModelPerformance(
                    model_id="model-a",
                    tier="CHEAP",
                    success_rate=1.0,
                    avg_latency_ms=105.0,
                    avg_cost=0.001,
                    sample_size=2,
                ),
                ModelPerformance(
                    model_id="model-b",
                    tier="CAPABLE",
                    success_rate=1.0,
                    avg_latency_ms=200.0,
                    avg_cost=0.005,
                    sample_size=1,
                ),
            ]
            best_model = router.get_best_model(
                workflow="test-workflow", stage="test-stage"
            )

        # Then
        assert best_model == "model-a"  # Better quality score due to lower cost

    def test_given_max_cost_constraint_when_getting_best_model_then_filters_expensive_models(
        self, router, mock_usage_tracker
    ):
        """
        Given a max_cost constraint
        When getting best model
        Then filters out models exceeding the cost limit
        """
        # Given
        mock_usage_tracker.get_telemetry_for_workflow.return_value = [
            {
                "model_id": "cheap-model",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 100.0,
                "cost": 0.001,
            },
            {
                "model_id": "expensive-model",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 200.0,
                "cost": 0.10,
            },
        ]

        # When
        with patch.object(router, "_analyze_performance") as mock_analyze:
            mock_analyze.return_value = [
                ModelPerformance(
                    model_id="cheap-model",
                    tier="CHEAP",
                    success_rate=1.0,
                    avg_latency_ms=100.0,
                    avg_cost=0.001,
                    sample_size=1,
                ),
                ModelPerformance(
                    model_id="expensive-model",
                    tier="PREMIUM",
                    success_rate=1.0,
                    avg_latency_ms=200.0,
                    avg_cost=0.10,
                    sample_size=1,
                ),
            ]
            best_model = router.get_best_model(
                workflow="test-workflow", stage="test-stage", max_cost=0.01
            )

        # Then
        assert best_model == "cheap-model"

    def test_given_max_latency_constraint_when_getting_best_model_then_filters_slow_models(
        self, router, mock_usage_tracker
    ):
        """
        Given a max_latency constraint
        When getting best model
        Then filters out models exceeding the latency limit
        """
        # Given
        mock_usage_tracker.get_telemetry_for_workflow.return_value = [
            {
                "model_id": "fast-model",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 50.0,
                "cost": 0.001,
            },
            {
                "model_id": "slow-model",
                "workflow": "test-workflow",
                "stage": "test-stage",
                "success": True,
                "latency_ms": 500.0,
                "cost": 0.001,
            },
        ]

        # When
        with patch.object(router, "_analyze_performance") as mock_analyze:
            mock_analyze.return_value = [
                ModelPerformance(
                    model_id="fast-model",
                    tier="CHEAP",
                    success_rate=1.0,
                    avg_latency_ms=50.0,
                    avg_cost=0.001,
                    sample_size=1,
                ),
                ModelPerformance(
                    model_id="slow-model",
                    tier="CHEAP",
                    success_rate=1.0,
                    avg_latency_ms=500.0,
                    avg_cost=0.001,
                    sample_size=1,
                ),
            ]
            best_model = router.get_best_model(
                workflow="test-workflow", stage="test-stage", max_latency_ms=100.0
            )

        # Then
        assert best_model == "fast-model"

    def test_given_no_telemetry_data_when_getting_best_model_then_returns_none(
        self, router, mock_usage_tracker
    ):
        """
        Given no telemetry data available
        When getting best model
        Then returns None
        """
        # Given
        mock_usage_tracker.get_telemetry_for_workflow.return_value = []

        # When
        with patch.object(router, "_analyze_performance") as mock_analyze:
            mock_analyze.return_value = []
            best_model = router.get_best_model(
                workflow="unknown-workflow", stage="unknown-stage"
            )

        # Then
        assert best_model is None

    def test_given_all_models_filtered_when_getting_best_model_then_returns_none(
        self, router, mock_usage_tracker
    ):
        """
        Given all models are filtered by constraints
        When getting best model
        Then returns None
        """
        # Given
        mock_usage_tracker.get_telemetry_for_workflow.return_value = [
            {
                "model_id": "expensive-model",
                "workflow": "test-workflow",
                "stage": "test-stage",